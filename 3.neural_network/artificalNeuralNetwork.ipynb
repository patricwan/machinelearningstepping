{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人工神经网络\n",
    "这个ipython notebook是手写的多层神经网络(都是全连接层)，然后在CIFAR-10数据集上做实验<br>\n",
    "[@寒小阳](http://blog.csdn.net/han_xiaoyang)<br>\n",
    "2016年5月"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# 初始设定，可以略过\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" 返回相对误差 \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们手写的神经网络非常简单，模型其实就是最后的权重，我们存在一个python dict里面，按层存了W和偏移项b<br>\n",
    "先练练手，我们初始化一个给定初始权重的神经网络，以及一部分数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机初始化一个试验模型(其实就是存在dic中的权重)和数据集\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "  model = {}\n",
    "  model['W1'] = np.linspace(-0.2, 0.6, num=input_size*hidden_size).reshape(input_size, hidden_size)\n",
    "  model['b1'] = np.linspace(-0.3, 0.7, num=hidden_size)\n",
    "  model['W2'] = np.linspace(-0.4, 0.1, num=hidden_size*num_classes).reshape(hidden_size, num_classes)\n",
    "  model['b2'] = np.linspace(-0.5, 0.9, num=num_classes)\n",
    "  return model\n",
    "\n",
    "def init_toy_data():\n",
    "  X = np.linspace(-0.2, 0.5, num=num_inputs*input_size).reshape(num_inputs, input_size)\n",
    "  y = np.array([0, 1, 2, 2, 1])\n",
    "  return X, y\n",
    "\n",
    "model = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前向计算: 获取得分\n",
    "这个部分有点像我们前面写的linear SVM和Softmax分类器：其实做的事情都一样，我们根据数据和权重去计算每个类的得分，损失函数值，以及参数上的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 result shape: (5, 10)\n",
      "Layer 2 result shape: (5, 3)\n",
      "[[-0.5328368   0.20031504  0.93346689]\n",
      " [-0.59412164  0.15498488  0.9040914 ]\n",
      " [-0.67658362  0.08978957  0.85616275]\n",
      " [-0.77092643  0.01339997  0.79772637]\n",
      " [-0.89110401 -0.08754544  0.71601312]]\n",
      "前向运算得到的得分和实际的得分差别:\n",
      "3.848682303062012e-08\n"
     ]
    }
   ],
   "source": [
    "from nn.classifiers.neural_net import two_layer_net\n",
    "\n",
    "scores = two_layer_net(X, model, verbose=True)\n",
    "print(scores)\n",
    "correct_scores = [[-0.5328368, 0.20031504, 0.93346689],\n",
    " [-0.59412164, 0.15498488, 0.9040914 ],\n",
    " [-0.67658362, 0.08978957, 0.85616275],\n",
    " [-0.77092643, 0.01339997, 0.79772637],\n",
    " [-0.89110401, -0.08754544, 0.71601312]]\n",
    "\n",
    "# 我们前向运算计算得到的得分和实际的得分应该差别很小才对\n",
    "print('前向运算得到的得分和实际的得分差别:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前向运算：计算损失\n",
    "这里的loss包括数据损失和正则化损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们计算到的损失和真实的损失值之间差别:\n",
      "4.6769255135359344e-12\n"
     ]
    }
   ],
   "source": [
    "reg = 0.1\n",
    "loss, _ = two_layer_net(X, model, y, reg)\n",
    "correct_loss = 1.38191946092\n",
    "\n",
    "# 应该差值是很小的\n",
    "print('我们计算到的损失和真实的损失值之间差别:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播部分\n",
    "咱们得计算loss在`W1`, `b1`, `W2`和`b2`上的梯度，就是反向传播的实现，不过注意梯度计算的时候要进行梯度检验哦:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 最大相对误差: 8.023743e-10\n",
      "b2 最大相对误差: 8.190173e-11\n",
      "W1 最大相对误差: 4.426512e-09\n",
      "b1 最大相对误差: 5.435430e-08\n"
     ]
    }
   ],
   "source": [
    "from nn.gradient_check import eval_numerical_gradient\n",
    "\n",
    "# 使用数值梯度去检查反向传播的计算\n",
    "\n",
    "loss, grads = two_layer_net(X, model, y, reg)\n",
    "\n",
    "# 各参数应该比 1e-8 要小才保险\n",
    "for param_name in grads:\n",
    "  param_grad_num = eval_numerical_gradient(lambda W: two_layer_net(X, model, y, reg)[0], model[param_name], verbose=False)\n",
    "  print('%s 最大相对误差: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练神经网络\n",
    "用定步长SGD和SGD with Momentum完成最小化损失函数。<br>\n",
    "具体的实现在`classifier_trainer.py`文件的`ClassifierTrainer`类里。<br>\n",
    "先试试定步长的SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting iteration  0\n",
      "Final loss with vanilla SGD: 0.940686\n"
     ]
    }
   ],
   "source": [
    "from nn.classifier_trainer import ClassifierTrainer\n",
    "\n",
    "model = init_toy_model()\n",
    "trainer = ClassifierTrainer()\n",
    "# 这个地方是自己手造的数据，量不大，所以其实sample_batches就设为False了，直接全量梯度下降\n",
    "best_model, loss_history, _, _ = trainer.train(X, y, X, y,\n",
    "                                             model, two_layer_net,\n",
    "                                             reg=0.001,\n",
    "                                             learning_rate=1e-1, momentum=0.0, learning_rate_decay=1,\n",
    "                                             update='sgd', sample_batches=False,\n",
    "                                             num_epochs=100,\n",
    "                                             verbose=False)\n",
    "print('Final loss with vanilla SGD: %f' % (loss_history[-1], ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是使用**momentum update**的步长更新策略的SGD, 你会看到最后的loss值会比上面要小一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting iteration  0\n",
      "Final loss with momentum SGD: 0.494394. We get: 0.494394\n"
     ]
    }
   ],
   "source": [
    "model = init_toy_model()\n",
    "trainer = ClassifierTrainer()\n",
    "# call the trainer to optimize the loss\n",
    "# Notice that we're using sample_batches=False, so we're performing Gradient Descent (no sampled batches of data)\n",
    "best_model, loss_history, _, _ = trainer.train(X, y, X, y,\n",
    "                                             model, two_layer_net,\n",
    "                                             reg=0.001,\n",
    "                                             learning_rate=1e-1, momentum=0.9, learning_rate_decay=1,\n",
    "                                             update='momentum', sample_batches=False,\n",
    "                                             num_epochs=100,\n",
    "                                             verbose=False)\n",
    "correct_loss = 0.494394\n",
    "print('Final loss with momentum SGD: %f. We get: %f' % (loss_history[-1], correct_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然也可以试试课上提到的 **RMSProp** 方式做SGD最优化:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting iteration  0\n",
      "Final loss with RMSProp: 0.439368. We get: 0.439368\n"
     ]
    }
   ],
   "source": [
    "model = init_toy_model()\n",
    "trainer = ClassifierTrainer()\n",
    "# call the trainer to optimize the loss\n",
    "# Notice that we're using sample_batches=False, so we're performing Gradient Descent (no sampled batches of data)\n",
    "best_model, loss_history, _, _ = trainer.train(X, y, X, y,\n",
    "                                             model, two_layer_net,\n",
    "                                             reg=0.001,\n",
    "                                             learning_rate=1e-1, momentum=0.9, learning_rate_decay=1,\n",
    "                                             update='rmsprop', sample_batches=False,\n",
    "                                             num_epochs=100,\n",
    "                                             verbose=False)\n",
    "correct_loss = 0.439368\n",
    "print('Final loss with RMSProp: %f. We get: %f' % (loss_history[-1], correct_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入数据\n",
    "我们手写了一个2层的全连接神经网络（感知器），并在 CIFAR-10数据集上试试效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nn/datasets/cifar-10-batches-py/data_batch_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5d7144c51544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# 看看数据维度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_CIFAR10_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train data shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train labels shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5d7144c51544>\u001b[0m in \u001b[0;36mget_CIFAR10_data\u001b[0;34m(num_training, num_validation, num_test)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0mcifar10_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'nn/datasets/cifar-10-batches-py'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_CIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar10_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# 采样数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/machinelearningstepping/3.neural_network/nn/data_utils.py\u001b[0m in \u001b[0;36mload_CIFAR10\u001b[0;34m(ROOT)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_batch_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_CIFAR_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/machinelearningstepping/3.neural_network/nn/data_utils.py\u001b[0m in \u001b[0;36mload_CIFAR_batch\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_CIFAR_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;34m\"\"\" 载入1个batch的cifar数据集 \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdatadict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nn/datasets/cifar-10-batches-py/data_batch_1'"
     ]
    }
   ],
   "source": [
    "from nn.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    载入CIFAR-10数据集，并做预处理。这一步和前一节课用softmax和SVM分类是一样的\n",
    "    \"\"\"\n",
    "    cifar10_dir = 'nn/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # 采样数据\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # 去均值\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # 调整维度\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# 看看数据维度\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练神经网络\n",
    "我们使用SGD with momentum进行最优化。每一轮迭代以后，我们把学习率衰减一点点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nn.classifiers.neural_net import init_two_layer_model\n",
    "from nn.classifier_trainer import ClassifierTrainer\n",
    "\n",
    "model = init_two_layer_model(32*32*3, 100, 10) # input size, hidden size, number of classes\n",
    "trainer = ClassifierTrainer()\n",
    "best_model, loss_history, train_acc, val_acc = trainer.train(X_train, y_train, X_val, y_val,\n",
    "                                             model, two_layer_net,\n",
    "                                             num_epochs=5, reg=1.0,\n",
    "                                             momentum=0.9, learning_rate_decay = 0.95,\n",
    "                                             learning_rate=1e-5, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练过程监控\n",
    "我们需要确保训练是正常进行的，你可以通过以下的办法去了解训练的状态：<br>\n",
    "1）绘出随迭代进行的损失值变化，我们希望是逐步减小的<br>\n",
    "2）可视化第一层的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_acc)\n",
    "plt.plot(val_acc)\n",
    "plt.legend(['Training accuracy', 'Validation accuracy'], loc='lower right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.vis_utils import visualize_grid\n",
    "\n",
    "# 可视化权重\n",
    "\n",
    "def show_net_weights(model):\n",
    "    plt.imshow(visualize_grid(model['W1'].T.reshape(-1, 32, 32, 3), padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调优参数\n",
    "\n",
    "**上面的图告诉我们什么？**. 我们看到loss的下降近乎是线性的，这预示着可能__我们的学习率设得太小了__。如果训练和交叉验证集上的准确率差别又不是特别大，也可能说明模型的__容量（学习能力）__很有限，可以提高隐层的结点个数，不过话说回来，如果隐层节点个数取得太多，训练集和交叉验证集上可能准确率差别就会很大了，这有可能说明是过拟合了。\n",
    "\n",
    "**调优**. 恩，你也听好多人吐槽过，说神经网络其实就是一个调参的活，这个，怎么说呢，有时候人家说的也没错。我们会对隐层结点个数，学习率，训练轮数和正则化参数进行优选。\n",
    "\n",
    "**关于准确率**. 在现在的这个图片数据集上，我们应该至少要取得50%以上的准确率，不然肯定是哪块出问题了，得回过头去检查一下咯。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.classifiers.neural_net import init_two_layer_model\n",
    "from nn.classifier_trainer import ClassifierTrainer\n",
    "\n",
    "best_model = None # 存储交叉验证集上拿到的最好的结果\n",
    "best_val_acc = -1\n",
    "# 很不好意思，这里直接列了一堆参数，然后用for循环做的cross-validation\n",
    "learning_rates = [1e-5, 5e-5, 1e-4]\n",
    "model_capacitys = [200, 300, 500, 1000]\n",
    "regularization_strengths = [1e0, 1e1]\n",
    "results = {}\n",
    "verbose = True\n",
    "\n",
    "for hidden_size in model_capacitys:\n",
    "    for lr in learning_rates:\n",
    "        for reg in regularization_strengths:\n",
    "            if verbose: \n",
    "                print \"Trainging Start: \"\n",
    "                print \"lr = %e, reg = %e, hidden_size = %e\" % (lr, reg, hidden_size)\n",
    "\n",
    "            model = init_two_layer_model(32*32*3, hidden_size, 10)\n",
    "            trainer = ClassifierTrainer()\n",
    "            output_model, loss_history, train_acc, val_acc = trainer.train(X_train, y_train, X_val, y_val,\n",
    "                                             model, two_layer_net,\n",
    "                                             num_epochs=5, reg=1.0,\n",
    "                                             momentum=0.9, learning_rate_decay = 0.95,\n",
    "                                             learning_rate=lr)\n",
    "\n",
    "\n",
    "            results[hidden_size, lr, reg] = (loss_history, train_acc, val_acc)\n",
    "\n",
    "            if verbose: \n",
    "                print(\"Training Complete: \")\n",
    "                print(\"Training accuracy = %f, Validation accuracy = %f \" % (train_acc[-1], val_acc[-1]))\n",
    "\n",
    "            if val_acc[-1] > best_val_acc:\n",
    "                best_val_acc = val_acc[-1]\n",
    "                best_model = output_model\n",
    "        \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化参数权重\n",
    "show_net_weights(best_model)\n",
    "\n",
    "# 在测试集上看准确率\n",
    "scores_test = two_layer_net(X_test, best_model)\n",
    "print('Test accuracy: ', np.mean(np.argmax(scores_test, axis=1) == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num = len(results)\n",
    "for i, (hsize, lr, reg) in enumerate(sorted(results)):\n",
    "    loss_history, train_acc, val_acc = results[hsize, lr, reg]\n",
    "    \n",
    "    if val_acc[-1] > 0.5: \n",
    "        plt.figure(i)\n",
    "        plt.title('hidden size {0} lr {1} reg {2} train accuracy'.format(hsize, lr, reg))\n",
    "        \n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(loss_history)\n",
    "        plt.title('Loss history')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(train_acc)\n",
    "        plt.plot(val_acc)\n",
    "        plt.legend(['Training accuracy', 'Validation accuracy'], loc='lower right')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Clasification accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在测试集上看看效果\n",
    "神经网络训练完了，咱们需要在测试集上看看效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test = two_layer_net(X_test, best_model)\n",
    "print('Test accuracy: ', np.mean(np.argmax(scores_test, axis=1) == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
