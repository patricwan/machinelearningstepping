{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 100 \n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99 \n",
    "\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "LAYER1_NODE = 500\n",
    "\n",
    "MODEL_SAVE_PATH='./deepNNModel/'\n",
    "MODEL_NAME='deepNNModel'\n",
    "\n",
    "import sys\n",
    "#sys.path.append(\"./\")\n",
    "import os\n",
    "import time\n",
    "#from InferenceUtil import inference\n",
    "\n",
    "\n",
    "class DeepNN:\n",
    "    def __init__(self,batchSize=BATCH_SIZE,learningRateBase=LEARNING_RATE_BASE, \n",
    "                 inputNode=INPUT_NODE, outputNode=OUTPUT_NODE, layer1Node=LAYER1_NODE,\n",
    "                 regularizationRate=REGULARIZATION_RATE, movingAverageDecay=MOVING_AVERAGE_DECAY,\n",
    "                 learningRateDecay=LEARNING_RATE_DECAY,trainingSteps=TRAINING_STEPS,\n",
    "                 modelSavePath=MODEL_SAVE_PATH, modelName=MODEL_NAME):\n",
    "        self.batchSize = batchSize\n",
    "        self.learningRateBase = learningRateBase\n",
    "        self.inputNode=inputNode\n",
    "        self.outputNode = outputNode\n",
    "        self.layer1Node = layer1Node\n",
    "        self.regularizationRate=regularizationRate\n",
    "        self.movingAverageDecay=movingAverageDecay\n",
    "        self.learningRateDecay=learningRateDecay\n",
    "        self.trainingSteps=trainingSteps\n",
    "        self.modelSavePath=modelSavePath\n",
    "        self.modelName=modelName\n",
    "    \n",
    "    #also known as train\n",
    "    def fit(self, trainData):\n",
    "        tf.reset_default_graph() \n",
    "        x = tf.placeholder(tf.float32, [None, self.inputNode], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, self.outputNode], name='y-input')\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(self.regularizationRate)\n",
    "        y = self.inference(x, regularizer)\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(self.movingAverageDecay, global_step)\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "        loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            self.learningRateBase,\n",
    "            global_step,\n",
    "            trainData.num_examples / self.batchSize, self.learningRateDecay,\n",
    "            staircase=True)\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "        with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            for i in range(self.trainingSteps):\n",
    "                xs, ys = trainData.next_batch(self.batchSize)\n",
    "                _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "                if i % 1000 == 0:\n",
    "                    print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "                    saver.save(sess, os.path.join(self.modelSavePath, self.modelName), global_step=global_step)\n",
    " \n",
    "        return None\n",
    "    \n",
    "    def get_weight_variable(self, shape, regularizer):\n",
    "        weights = tf.get_variable(\"weights\", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(weights))\n",
    "        return weights\n",
    "\n",
    "    def inference(self, input_tensor, regularizer):\n",
    "        with tf.variable_scope('layer1',reuse=tf.AUTO_REUSE):\n",
    "\n",
    "            weights = self.get_weight_variable([self.inputNode, self.layer1Node], regularizer)\n",
    "            biases = tf.get_variable(\"biases\", [self.layer1Node], initializer=tf.constant_initializer(0.0))\n",
    "            layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n",
    "\n",
    "        with tf.variable_scope('layer2',reuse=tf.AUTO_REUSE):\n",
    "            weights = self.get_weight_variable([self.layer1Node, self.outputNode], regularizer)\n",
    "            biases = tf.get_variable(\"biases\", [self.outputNode], initializer=tf.constant_initializer(0.0))\n",
    "            layer2 = tf.matmul(layer1, weights) + biases\n",
    "\n",
    "        return layer2\n",
    "\n",
    "    #also known as train\n",
    "    def predict(self, testData):\n",
    "        with tf.Graph().as_default() as g:\n",
    "            x = tf.placeholder(tf.float32, [None, self.inputNode], name='x-input')\n",
    "            y_ = tf.placeholder(tf.float32, [None, self.outputNode], name='y-input')\n",
    "            validate_feed = {x: testData.images, y_: testData.labels}\n",
    "\n",
    "            y = self.inference(x, None)\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "            variable_averages = tf.train.ExponentialMovingAverage(self.movingAverageDecay)\n",
    "            variables_to_restore = variable_averages.variables_to_restore()\n",
    "            saver = tf.train.Saver(variables_to_restore)\n",
    "            \n",
    "            iCount = 10\n",
    "            for iEachRound in range(0, iCount):\n",
    "                with tf.Session() as sess:\n",
    "                    ckpt = tf.train.get_checkpoint_state(self.modelSavePath)\n",
    "                    if ckpt and ckpt.model_checkpoint_path:\n",
    "                        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                        global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                        accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                        print(\"After %s training step(s), validation accuracy = %g\" % (global_step, accuracy_score))\n",
    "                    else:\n",
    "                        print('No checkpoint file found')\n",
    "                        return\n",
    "                time.sleep(100)\n",
    "        return y,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "After 1 training step(s), loss on training batch is 3.05762.\n",
      "After 1001 training step(s), loss on training batch is 0.249776.\n",
      "After 2001 training step(s), loss on training batch is 0.215415.\n",
      "After 3001 training step(s), loss on training batch is 0.149032.\n",
      "After 4001 training step(s), loss on training batch is 0.127039.\n",
      "After 5001 training step(s), loss on training batch is 0.106942.\n",
      "After 6001 training step(s), loss on training batch is 0.117086.\n",
      "After 7001 training step(s), loss on training batch is 0.102078.\n",
      "After 8001 training step(s), loss on training batch is 0.0960427.\n",
      "After 9001 training step(s), loss on training batch is 0.0751927.\n",
      "After 10001 training step(s), loss on training batch is 0.0684207.\n",
      "After 11001 training step(s), loss on training batch is 0.0698136.\n",
      "After 12001 training step(s), loss on training batch is 0.0595237.\n",
      "After 13001 training step(s), loss on training batch is 0.0512928.\n",
      "After 14001 training step(s), loss on training batch is 0.0571647.\n",
      "After 15001 training step(s), loss on training batch is 0.0489814.\n",
      "After 16001 training step(s), loss on training batch is 0.0479064.\n",
      "After 17001 training step(s), loss on training batch is 0.0428508.\n",
      "After 18001 training step(s), loss on training batch is 0.0551929.\n",
      "After 19001 training step(s), loss on training batch is 0.045294.\n",
      "After 20001 training step(s), loss on training batch is 0.0414643.\n",
      "After 21001 training step(s), loss on training batch is 0.039241.\n",
      "After 22001 training step(s), loss on training batch is 0.0431203.\n",
      "After 23001 training step(s), loss on training batch is 0.0362468.\n",
      "After 24001 training step(s), loss on training batch is 0.0366196.\n",
      "After 25001 training step(s), loss on training batch is 0.0384458.\n",
      "After 26001 training step(s), loss on training batch is 0.0396988.\n",
      "After 27001 training step(s), loss on training batch is 0.0357741.\n",
      "After 28001 training step(s), loss on training batch is 0.035992.\n",
      "After 29001 training step(s), loss on training batch is 0.0357548.\n",
      "After 29001 training step(s), validation accuracy = 0.9846\n",
      "After 29001 training step(s), validation accuracy = 0.9846\n",
      "After 29001 training step(s), validation accuracy = 0.9846\n",
      "After 29001 training step(s), validation accuracy = 0.9846\n",
      "After 29001 training step(s), validation accuracy = 0.9846\n",
      "After 29001 training step(s), validation accuracy = 0.9846\n",
      "After 29001 training step(s), validation accuracy = 0.9846\n",
      "After 29001 training step(s), validation accuracy = 0.9846\n",
      "After 29001 training step(s), validation accuracy = 0.9846\n",
      "After 29001 training step(s), validation accuracy = 0.9846\n",
      "y_predict  Tensor(\"layer2/add:0\", shape=(?, 10), dtype=float32)\n",
      "accuracy_score 0.9846\n"
     ]
    }
   ],
   "source": [
    "deepDNN = DeepNN()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"../data/MNIST_data\", one_hot=True)\n",
    "\n",
    "deepDNN.fit(mnist.train)\n",
    "y_predict, accuracy_score=deepDNN.predict(mnist.validation)\n",
    "print(\"y_predict \", y_predict)\n",
    "print(\"accuracy_score\",accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
