{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.01\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 6000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "NUM_LABELS = 10\n",
    "\n",
    "CONV1_DEEP = 32\n",
    "CONV1_SIZE = 5\n",
    "\n",
    "CONV2_DEEP = 64\n",
    "CONV2_SIZE = 5\n",
    "\n",
    "FC_SIZE = 512\n",
    "\n",
    "MODEL_SAVE_PATH='./convNNModel/'\n",
    "MODEL_NAME='convNNModel'\n",
    "\n",
    "class ConvNN:\n",
    "    def __init__(self,batchSize=BATCH_SIZE,learningRateBase=LEARNING_RATE_BASE, \n",
    "                 inputNode=INPUT_NODE, outputNode=OUTPUT_NODE, \n",
    "                 regularizationRate=REGULARIZATION_RATE, movingAverageDecay=MOVING_AVERAGE_DECAY,\n",
    "                 learningRateDecay=LEARNING_RATE_DECAY,trainingSteps=TRAINING_STEPS,\n",
    "                 imageSize=IMAGE_SIZE,numChannels=NUM_CHANNELS,numLabels=NUM_LABELS,\n",
    "                 conv1Deep=CONV1_DEEP,conv1Size=CONV1_SIZE,fcSize=FC_SIZE,\n",
    "                 conv2Deep=CONV2_DEEP,conv2Size=CONV2_SIZE,\n",
    "                 modelSavePath=MODEL_SAVE_PATH, modelName=MODEL_NAME):\n",
    "        self.batchSize = batchSize\n",
    "        self.learningRateBase = learningRateBase\n",
    "        self.inputNode=inputNode\n",
    "        self.outputNode = outputNode\n",
    "        self.regularizationRate=regularizationRate\n",
    "        self.movingAverageDecay=movingAverageDecay\n",
    "        self.learningRateDecay=learningRateDecay\n",
    "        self.trainingSteps=trainingSteps\n",
    "        self.imageSize=imageSize\n",
    "        self.numChannels=numChannels\n",
    "        self.numLabels=numLabels\n",
    "        self.conv1Deep=conv1Deep\n",
    "        self.conv1Size=conv1Size\n",
    "        self.conv2Deep=conv2Deep\n",
    "        self.conv2Size=conv2Size\n",
    "        self.fcSize=fcSize\n",
    "        self.modelSavePath=modelSavePath\n",
    "        self.modelName=modelName\n",
    "        \n",
    "    #also known as train\n",
    "    def fit(self, trainData):\n",
    "        tf.reset_default_graph() \n",
    "        x = tf.placeholder(tf.float32, [self.batchSize, self.imageSize,\n",
    "                                        self.imageSize, self.numChannels], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, self.outputNode], name='y-input')\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(self.regularizationRate)\n",
    "        y = self.inference(x,False,regularizer)\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(self.movingAverageDecay, global_step)\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "        loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "        learning_rate = tf.train.exponential_decay(self.learningRateBase,\n",
    "                                                    global_step,\n",
    "                                                    trainData.num_examples / self.batchSize, self.learningRateDecay,\n",
    "                                                    staircase=True)\n",
    "\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "        with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "\n",
    "        # 初始化TensorFlow持久化类。\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            for i in range(self.trainingSteps):\n",
    "                xs, ys = mnist.train.next_batch(self.batchSize)\n",
    "\n",
    "                reshaped_xs = np.reshape(xs, (\n",
    "                    self.batchSize,\n",
    "                    self.imageSize,\n",
    "                    self.imageSize,\n",
    "                    self.numChannels))\n",
    "                _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n",
    "\n",
    "                if i % 1000 == 0:\n",
    "                    print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "        return None\n",
    "        \n",
    "        #also known as train\n",
    "    def predict(self, testData):\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    def inference(self, input_tensor, train, regularizer):\n",
    "        with tf.variable_scope('layer1-conv1'):\n",
    "            conv1_weights = tf.get_variable(\n",
    "                \"weight\", [self.conv1Size, self.conv1Size, self.numChannels, self.conv1Deep],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            conv1_biases = tf.get_variable(\"bias\", [self.conv1Deep], initializer=tf.constant_initializer(0.0))\n",
    "            conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "\n",
    "        with tf.name_scope(\"layer2-pool1\"):\n",
    "            pool1 = tf.nn.max_pool(relu1, ksize = [1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "        with tf.variable_scope(\"layer3-conv2\"):\n",
    "            conv2_weights = tf.get_variable(\n",
    "                \"weight\", [self.conv2Size, self.conv2Size, self.conv1Deep, self.conv2Deep],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            conv2_biases = tf.get_variable(\"bias\", [self.conv2Deep], initializer=tf.constant_initializer(0.0))\n",
    "            conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "\n",
    "        with tf.name_scope(\"layer4-pool2\"):\n",
    "            pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            pool_shape = pool2.get_shape().as_list()\n",
    "            nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "            reshaped = tf.reshape(pool2, [pool_shape[0], nodes])\n",
    "\n",
    "        with tf.variable_scope('layer5-fc1'):\n",
    "            fc1_weights = tf.get_variable(\"weight\", [nodes, self.fcSize],\n",
    "                                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            if regularizer != None: tf.add_to_collection('losses', regularizer(fc1_weights))\n",
    "            fc1_biases = tf.get_variable(\"bias\", [self.fcSize], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "            fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
    "            if train: fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "\n",
    "        with tf.variable_scope('layer6-fc2'):\n",
    "            fc2_weights = tf.get_variable(\"weight\", [self.fcSize, self.numLabels],\n",
    "                                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            if regularizer != None: tf.add_to_collection('losses', regularizer(fc2_weights))\n",
    "            fc2_biases = tf.get_variable(\"bias\", [self.numLabels], initializer=tf.constant_initializer(0.1))\n",
    "            logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n",
    "        return logit\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "After 1 training step(s), loss on training batch is 5.66454.\n",
      "After 1001 training step(s), loss on training batch is 0.793069.\n",
      "After 2001 training step(s), loss on training batch is 0.731462.\n",
      "After 3001 training step(s), loss on training batch is 0.686523.\n"
     ]
    }
   ],
   "source": [
    "convDNN = ConvNN()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"../data/MNIST_data\", one_hot=True)\n",
    "\n",
    "convDNN.fit(mnist.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
