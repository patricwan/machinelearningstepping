{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "train = pd.read_csv('../data/loan_train.csv',encoding='utf-8')\n",
    "target = 'Disbursed'\n",
    "IDcol = 'ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "\n",
    "columns = [\"Gender\", \"City\", \"Employer_Name\"]\n",
    "\n",
    "for col in train.columns:\n",
    "    train[col] = labelencoder.fit_transform(train[col].astype(str))\n",
    "    \n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imported 2 forms of XGBoost:\n",
    "#xgb – this is the direct xgboost library. I will use a specific function “cv” from this library\n",
    "#XGBClassifier – this is an sklearn wrapper for XGBoost. This allows us to use sklearn’s Grid Search with parallel processing in the same way we did for GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function which will help us create XGBoost models and perform cross-validation. \n",
    "def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Disbursed'],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob))\n",
    "                    \n",
    "    #feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    #feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    #plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "#1.Choose a relatively high learning rate. Generally a learning rate of 0.1 works but somewhere between 0.05 to 0.3 should work for different problems. Determine the optimum number of trees for this learning rate. XGBoost has a very useful function called as “cv” which performs cross-validation at each boosting iteration and thus returns the optimum number of trees required.\n",
    "#2.Tune tree-specific parameters ( max_depth, min_child_weight, gamma, subsample, colsample_bytree) for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I’ll take up an example here.\n",
    "#3.Tune regularization parameters (lambda, alpha) for xgboost which can help reduce model complexity and enhance performance.\n",
    "#4.Lower the learning rate and decide the optimal parameters ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Fix learning rate and number of estimators for tuning tree-based parameters\n",
    "\n",
    "In order to decide on boosting parameters, we need to set some initial values of other parameters. Lets take the following values:\n",
    "\n",
    "    max_depth = 5 : This should be between 3-10. I’ve started with 5 but you can choose a different number as well. 4-6 can be good starting points.\n",
    "    min_child_weight = 1 : A smaller value is chosen because it is a highly imbalanced class problem and leaf nodes can have smaller size groups.\n",
    "    gamma = 0 : A smaller value like 0.1-0.2 can also be chosen for starting. This will anyways be tuned later.\n",
    "    subsample, colsample_bytree = 0.8 : This is a commonly used used start value. Typical values range between 0.5-0.9.\n",
    "    scale_pos_weight = 1: Because of high class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = XGBClassifier(\n",
    "     learning_rate =0.1,\n",
    "     n_estimators=1000,\n",
    "     max_depth=5,\n",
    "     min_child_weight=1,\n",
    "     gamma=0,\n",
    "     subsample=0.8,\n",
    "     colsample_bytree=0.8,\n",
    "     objective= 'binary:logistic',\n",
    "     nthread=4,\n",
    "     scale_pos_weight=1,\n",
    "     seed=27)\n",
    "modelfit(xgb1, train, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Tune max_depth and min_child_weight\n",
    "I’ll be doing some heavy-duty grid searched in this section which can take 15-30 mins or \n",
    "even more time to run depending on your system. \n",
    "You can vary the number of values you are testing based on what your system can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.csdn.net/nitric_acid/article/details/80490476\n",
    "max_depth_param = list(range(3,10,3))\n",
    "min_child_weight_param = list(range(1,6,3))\n",
    "\n",
    "param_test1 = dict(max_depth=max_depth_param, min_child_weight=min_child_weight_param)\n",
    "param_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    "          min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "          objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    "          param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch1.fit(train[predictors],train[target])      \n",
    "\n",
    "print(gsearch1.best_score_)\n",
    "print(gsearch1.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideal values are 5 for max_depth and 5 for min_child_weight. Lets go one step deeper and look for optimum values. \n",
    "We’ll search for values 1 above and below the optimum values because we took an interval of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'max_depth':[4,5,6],\n",
    " 'min_child_weight':[4,5,6]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n",
    "           min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "           objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "           param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch2.fit(train[predictors],train[target])\n",
    "\n",
    "print(gsearch2.best_score_)\n",
    "print(gsearch2.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we get the optimum values as 4 for max_depth and 6 for min_child_weight. \n",
    "Also, we can see the CV score increasing slightly. Note that as the model performance increases, it becomes exponentially difficult to achieve even marginal gains in performance. You would have noticed that here we got 6 as optimum value \n",
    "for min_child_weight but we haven’t tried values more than 6. We can do that as follow:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2b = {\n",
    " 'min_child_weight':[6,8,10,12]\n",
    "}\n",
    "gsearch2b = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=4,\n",
    "                         min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                         objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                         param_grid = param_test2b, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch2b.fit(train[predictors],train[target])\n",
    "\n",
    "modelfit(gsearch3.best_estimator_, train, predictors)\n",
    "print(gsearch2b.best_score_)\n",
    "print(gsearch2b.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Tune gamma\n",
    "\n",
    "Now lets tune gamma value using the parameters already tuned above. \n",
    "Gamma can take various values but I’ll check for 5 values here.\n",
    "You can go into more precise values as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,\n",
    " min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch3.fit(train[predictors],train[target])\n",
    "\n",
    "print(gsearch3.best_score_)\n",
    "print(gsearch3.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that our original value of gamma, i.e. 0 is the optimum one. Before proceeding, \n",
    "a good idea would be to re-calibrate the number of boosting rounds for the updated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2 = XGBClassifier(learning_rate =0.1,\n",
    "                     n_estimators=1000,\n",
    "                     max_depth=4,\n",
    "                     min_child_weight=6,\n",
    "                     gamma=0,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.8,\n",
    "                     objective= 'binary:logistic',\n",
    "                     nthread=4,\n",
    "                     scale_pos_weight=1,\n",
    "                     seed=27)\n",
    "modelfit(xgb2, train, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Tune subsample and colsample_bytree\n",
    "\n",
    "The next step would be try different subsample and colsample_bytree values. \n",
    "Lets do this in 2 stages as well and take values 0.6,0.7,0.8,0.9 for both to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    "  'subsample':[i/10.0 for i in range(6,10)],\n",
    "  'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    "                                                   min_child_weight=6, gamma=0, subsample=0.8, \n",
    "                                                   colsample_bytree=0.8, objective= 'binary:logistic', \n",
    "                                                   nthread=4, scale_pos_weight=1,seed=27), \n",
    "                                                   param_grid = param_test4, scoring='roc_auc',\n",
    "                                                   n_jobs=4,iid=False, cv=5)\n",
    "gsearch4.fit(train[predictors],train[target])\n",
    "\n",
    "print(gsearch4.best_score_)\n",
    "print(gsearch4.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we found 0.8 as the optimum value for both subsample \n",
    "and colsample_bytree. Now we should try values in 0.05 interval around these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(75,90,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(75,90,5)]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    "                                                   min_child_weight=6, gamma=0, subsample=0.8, \n",
    "                                                   colsample_bytree=0.8, objective= 'binary:logistic', \n",
    "                                                nthread=4, scale_pos_weight=1,seed=27), \n",
    "\n",
    "param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch5.fit(train[predictors],train[target])\n",
    "\n",
    "print(gsearch5.best_score_)\n",
    "print(gsearch5.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Tuning Regularization Parameters\n",
    "\n",
    "Next step is to apply regularization to reduce overfitting.\n",
    "Though many people don’t use this parameters much as gamma provides a substantial way of controlling complexity. \n",
    "But we should always try it. I’ll tune ‘reg_alpha’ value here and leave it upto you to \n",
    "try different values of ‘reg_lambda’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    "                                                   min_child_weight=6, gamma=0.1, subsample=0.8, \n",
    "                                                   colsample_bytree=0.8, objective= 'binary:logistic', \n",
    "                                                   nthread=4, scale_pos_weight=1,seed=27), \n",
    "                                                   param_grid = param_test6, scoring='roc_auc',\n",
    "                                                   n_jobs=4,iid=False, cv=5)\n",
    "gsearch6.fit(train[predictors],train[target])\n",
    "\n",
    "print(gsearch6.best_score_)\n",
    "print(gsearch6.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the CV score is less than the previous case. But the values tried are very widespread, \n",
    "we should try values closer to the optimum here (0.01) to see if we get something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n",
    "}\n",
    "gsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    "                                                   min_child_weight=6, gamma=0.1, subsample=0.8, \n",
    "                                                   colsample_bytree=0.8, objective= 'binary:logistic', \n",
    "                                                   nthread=4, scale_pos_weight=1,seed=27), \n",
    "                                                   param_grid = param_test7, scoring='roc_auc',\n",
    "                                                   n_jobs=4,iid=False, cv=5)\n",
    "gsearch7.fit(train[predictors],train[target])\n",
    "\n",
    "print(gsearch7.best_score_)\n",
    "print(gsearch7.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb3 = XGBClassifier(learning_rate =0.1,\n",
    "                     n_estimators=1000,\n",
    "                     max_depth=4,\n",
    "                     min_child_weight=6,\n",
    "                     gamma=0,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.8,\n",
    "                     reg_alpha=0.005,\n",
    "                     objective= 'binary:logistic',\n",
    "                     nthread=4,\n",
    "                     scale_pos_weight=1,\n",
    "                     seed=27)\n",
    "modelfit(xgb3, train, predictors)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Reducing Learning Rate\n",
    "\n",
    "Lastly, we should lower the learning rate and add more trees.\n",
    "Lets use the cv function of XGBoost to do the job again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb4 = XGBClassifier(learning_rate =0.01,\n",
    "                     n_estimators=5000,\n",
    "                     max_depth=4,\n",
    "                     min_child_weight=6,\n",
    "                     gamma=0,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.8,\n",
    "                     reg_alpha=0.005,\n",
    "                     objective= 'binary:logistic',\n",
    "                     nthread=4,\n",
    "                     scale_pos_weight=1,\n",
    "                     seed=27)\n",
    "modelfit(xgb4, train, predictors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
