{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train top5           date  store_nbr  item_nbr  unit_sales  onpromotion\n",
      "455 2014-04-01          1    103520    0.693147        False\n",
      "456 2014-04-02          1    103520    0.693147        False\n",
      "457 2014-04-03          1    103520    1.609438        False\n",
      "458 2014-04-04          1    103520    1.098612        False\n",
      "459 2014-04-05          1    103520    0.693147        False\n",
      "df_test top5                                       id  onpromotion\n",
      "store_nbr item_nbr date                              \n",
      "1         96995    2017-08-16  125497040        False\n",
      "          99197    2017-08-16  125497041        False\n",
      "          103501   2017-08-16  125497042        False\n",
      "          103520   2017-08-16  125497043        False\n",
      "          103665   2017-08-16  125497044        False\n",
      "df_2017 top5            date  store_nbr  item_nbr  unit_sales  onpromotion\n",
      "1461 2017-01-01         25     99197    0.693147        False\n",
      "1462 2017-01-02          1    103520    0.693147        False\n",
      "1463 2017-01-03          1     99197    1.386294        False\n",
      "1464 2017-01-04          1     99197    0.693147        False\n",
      "1465 2017-01-05          1     99197    0.693147        False\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import gc\n",
    "\n",
    "df_train = pd.read_csv(\n",
    "    '../../../data/favgrocery/trainDay.csv', usecols=[1, 2, 3, 4, 5],\n",
    "    converters={'unit_sales': lambda u: np.log1p(\n",
    "        float(u)) if float(u) > 0 else 0},\n",
    "    parse_dates=[\"date\"]\n",
    "    #skiprows=range(1, 66458909)  # 2016-01-01\n",
    ")\n",
    "df_train = df_train.dropna(axis = 0, how ='any')\n",
    "df_train = df_train.astype({\"onpromotion\":'bool'})\n",
    "    \n",
    "\n",
    "print(\"df_train top5\", df_train[:5])\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "    \"../../../data/favgrocery/test.csv\", usecols=[0, 1, 2, 3, 4],\n",
    "    parse_dates=[\"date\"]  # , date_parser=parser\n",
    ").set_index(\n",
    "    ['store_nbr', 'item_nbr', 'date']\n",
    ")\n",
    "df_test = df_test.dropna(axis = 0, how ='any')\n",
    "df_test = df_test.astype({\"onpromotion\":'bool'})\n",
    "print(\"df_test top5\", df_test[:5])\n",
    "\n",
    "items = pd.read_csv(\n",
    "    \"../../../data/favgrocery/items.csv\",\n",
    ").set_index(\"item_nbr\")\n",
    "\n",
    "stores = pd.read_csv(\n",
    "    \"../../../data/favgrocery/stores.csv\",\n",
    ").set_index(\"store_nbr\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "items['family'] = le.fit_transform(items['family'].values)\n",
    "\n",
    "stores['city'] = le.fit_transform(stores['city'].values)\n",
    "stores['state'] = le.fit_transform(stores['state'].values)\n",
    "stores['type'] = le.fit_transform(stores['type'].values)\n",
    "\n",
    "df_2017 = df_train.loc[df_train.date>=pd.datetime(2017,1,1)]\n",
    "print(\"df_2017 top5\", df_2017[:5])\n",
    "del df_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   onpromotion                                              \\\n",
      "date                2017-01-01 2017-01-02 2017-01-03 2017-01-04 2017-01-05   \n",
      "store_nbr item_nbr                                                           \n",
      "1         96995          False      False      False      False      False   \n",
      "          99197          False      False      False      False      False   \n",
      "          103520         False      False      False      False      False   \n",
      "          103665         False      False      False      False      False   \n",
      "          105574         False      False      False      False      False   \n",
      "          105575         False      False      False      False      False   \n",
      "25        99197          False      False      False      False      False   \n",
      "\n",
      "                                                                           \\\n",
      "date               2017-01-06 2017-01-07 2017-01-08 2017-01-09 2017-01-10   \n",
      "store_nbr item_nbr                                                          \n",
      "1         96995         False      False      False      False      False   \n",
      "          99197         False      False      False      False      False   \n",
      "          103520        False      False      False      False      False   \n",
      "          103665        False      False      False      False      False   \n",
      "          105574        False      False      False      False      False   \n",
      "          105575        False      False      False      False      False   \n",
      "25        99197         False      False      False      False      False   \n",
      "\n",
      "                      ...                                                  \\\n",
      "date                  ...     2017-08-06 2017-08-07 2017-08-08 2017-08-09   \n",
      "store_nbr item_nbr    ...                                                   \n",
      "1         96995       ...          False      False      False      False   \n",
      "          99197       ...          False      False      False      False   \n",
      "          103520      ...          False      False      False      False   \n",
      "          103665      ...          False      False      False      False   \n",
      "          105574      ...          False      False      False      False   \n",
      "          105575      ...          False      False      False      False   \n",
      "25        99197       ...          False      False      False      False   \n",
      "\n",
      "                                                                           \\\n",
      "date               2017-08-10 2017-08-11 2017-08-12 2017-08-13 2017-08-14   \n",
      "store_nbr item_nbr                                                          \n",
      "1         96995         False      False      False      False      False   \n",
      "          99197         False      False      False      False      False   \n",
      "          103520        False      False      False      False      False   \n",
      "          103665        False      False      False      False      False   \n",
      "          105574        False      False      False      False      False   \n",
      "          105575        False      False      False      False      False   \n",
      "25        99197         False      False      False      False      False   \n",
      "\n",
      "                               \n",
      "date               2017-08-15  \n",
      "store_nbr item_nbr             \n",
      "1         96995         False  \n",
      "          99197         False  \n",
      "          103520        False  \n",
      "          103665        False  \n",
      "          105574        False  \n",
      "          105575        False  \n",
      "25        99197         False  \n",
      "\n",
      "[7 rows x 227 columns]\n"
     ]
    }
   ],
   "source": [
    "promo_2017_train = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(\n",
    "        level=-1).fillna(False)\n",
    "\n",
    "print(promo_2017_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "promo_2017_train.columns = promo_2017_train.columns.get_level_values(1)\n",
    "\n",
    "promo_2017_test = df_test[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_2017_test.columns = promo_2017_test.columns.get_level_values(1)\n",
    "\n",
    "promo_2017_test = promo_2017_test.reindex(promo_2017_train.index).fillna(False)\n",
    "promo_2017 = pd.concat([promo_2017_train, promo_2017_test], axis=1)\n",
    "del promo_2017_test, promo_2017_train\n",
    "\n",
    "df_2017 = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(\n",
    "        level=-1).fillna(0)\n",
    "df_2017.columns = df_2017.columns.get_level_values(1)\n",
    "\n",
    "items = items.reindex(df_2017.index.get_level_values(1))\n",
    "stores = stores.reindex(df_2017.index.get_level_values(0))\n",
    "\n",
    "\n",
    "df_2017_item = df_2017.groupby('item_nbr')[df_2017.columns].sum()\n",
    "promo_2017_item = promo_2017.groupby('item_nbr')[promo_2017.columns].sum()\n",
    "\n",
    "df_2017_store_class = df_2017.reset_index()\n",
    "df_2017_store_class['class'] = items['class'].values\n",
    "df_2017_store_class_index = df_2017_store_class[['class', 'store_nbr']]\n",
    "df_2017_store_class = df_2017_store_class.groupby(['class', 'store_nbr'])[df_2017.columns].sum()\n",
    "\n",
    "df_2017_promo_store_class = promo_2017.reset_index()\n",
    "df_2017_promo_store_class['class'] = items['class'].values\n",
    "df_2017_promo_store_class_index = df_2017_promo_store_class[['class', 'store_nbr']]\n",
    "df_2017_promo_store_class = df_2017_promo_store_class.groupby(['class', 'store_nbr'])[promo_2017.columns].sum()\n",
    "\n",
    "def get_timespan(df, dt, minus, periods, freq='D'):\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]\n",
    "\n",
    "def prepare_dataset(df, promo_df, t2017, is_train=True, name_prefix=None):\n",
    "    #print(\"promo_df\", promo_df[:5])\n",
    "    X = {\n",
    "        \"promo_14_2017\": get_timespan(promo_df, t2017, 14, 14).sum(axis=1).values,\n",
    "        \"promo_60_2017\": get_timespan(promo_df, t2017, 60, 60).sum(axis=1).values,\n",
    "        \"promo_140_2017\": get_timespan(promo_df, t2017, 140, 140).sum(axis=1).values,\n",
    "        \"promo_3_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=16), 15, 3).sum(axis=1).values,\n",
    "        \"promo_7_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=16), 15, 7).sum(axis=1).values,\n",
    "        \"promo_14_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=16), 15, 14).sum(axis=1).values,\n",
    "    }\n",
    "\n",
    "    for i in [3, 7, 14, 30, 60, 140]:\n",
    "        tmp = get_timespan(df, t2017, i, i)\n",
    "        X['diff_%s_mean' % i] = tmp.diff(axis=1).mean(axis=1).values\n",
    "        X['mean_%s_decay' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n",
    "        X['mean_%s' % i] = tmp.mean(axis=1).values\n",
    "        X['median_%s' % i] = tmp.median(axis=1).values\n",
    "        X['min_%s' % i] = tmp.min(axis=1).values\n",
    "        X['max_%s' % i] = tmp.max(axis=1).values\n",
    "        X['std_%s' % i] = tmp.std(axis=1).values\n",
    "\n",
    "    for i in [3, 7, 14, 30, 60, 140]:\n",
    "        tmp = get_timespan(df, t2017 + timedelta(days=-7), i, i)\n",
    "        X['diff_%s_mean_2' % i] = tmp.diff(axis=1).mean(axis=1).values\n",
    "        X['mean_%s_decay_2' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n",
    "        X['mean_%s_2' % i] = tmp.mean(axis=1).values\n",
    "        X['median_%s_2' % i] = tmp.median(axis=1).values\n",
    "        X['min_%s_2' % i] = tmp.min(axis=1).values\n",
    "        X['max_%s_2' % i] = tmp.max(axis=1).values\n",
    "        X['std_%s_2' % i] = tmp.std(axis=1).values\n",
    "\n",
    "    for i in [7, 14, 30, 60, 140]:\n",
    "        tmp = get_timespan(df, t2017, i, i)\n",
    "        X['has_sales_days_in_last_%s' % i] = (tmp > 0).sum(axis=1).values\n",
    "        X['last_has_sales_day_in_last_%s' % i] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\n",
    "        X['first_has_sales_day_in_last_%s' % i] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values\n",
    "\n",
    "        tmp = get_timespan(promo_df, t2017, i, i)\n",
    "        X['has_promo_days_in_last_%s' % i] = (tmp > 0).sum(axis=1).values\n",
    "        X['last_has_promo_day_in_last_%s' % i] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\n",
    "        X['first_has_promo_day_in_last_%s' % i] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values\n",
    "\n",
    "    tmp = get_timespan(promo_df, t2017 + timedelta(days=16), 15, 15)\n",
    "    X['has_promo_days_in_after_15_days'] = (tmp > 0).sum(axis=1).values\n",
    "    X['last_has_promo_day_in_after_15_days'] = i - ((tmp > 0) * np.arange(15)).max(axis=1).values\n",
    "    X['first_has_promo_day_in_after_15_days'] = ((tmp > 0) * np.arange(15, 0, -1)).max(axis=1).values\n",
    "\n",
    "    for i in range(1, 16):\n",
    "        X['day_%s_2017' % i] = get_timespan(df, t2017, i, 1).values.ravel()\n",
    "\n",
    "    for i in range(7):\n",
    "        X['mean_4_dow{}_2017'.format(i)] = get_timespan(df, t2017, 28-i, 4, freq='7D').mean(axis=1).values\n",
    "        X['mean_20_dow{}_2017'.format(i)] = get_timespan(df, t2017, 140-i, 20, freq='7D').mean(axis=1).values\n",
    "\n",
    "    for i in range(-16, 16):\n",
    "        X[\"promo_{}\".format(i)] = promo_df[t2017 + timedelta(days=i)].values.astype(np.uint8)\n",
    "\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "    # start date and periods is 16 days\n",
    "    if is_train:\n",
    "        y = df[\n",
    "            pd.date_range(t2017, periods=16)\n",
    "        ].values\n",
    "        return X, y\n",
    "    if name_prefix is not None:\n",
    "        X.columns = ['%s_%s' % (name_prefix, c) for c in X.columns]\n",
    "    return X\n",
    "\n",
    "print(\"Preparing dataset...\")\n",
    "num_days = 8\n",
    "t2017 = date(2017, 5, 31)\n",
    "X_l, y_l = [], []\n",
    "for i in range(num_days):\n",
    "    delta = timedelta(days=7 * i)\n",
    "    X_tmp, y_tmp = prepare_dataset(df_2017, promo_2017, t2017 + delta)\n",
    "\n",
    "    X_tmp2 = prepare_dataset(df_2017_item, promo_2017_item, t2017 + delta, is_train=False, name_prefix='item')\n",
    "    X_tmp2.index = df_2017_item.index\n",
    "    X_tmp2 = X_tmp2.reindex(df_2017.index.get_level_values(1)).reset_index(drop=True)\n",
    "\n",
    "    X_tmp3 = prepare_dataset(df_2017_store_class, df_2017_promo_store_class, t2017 + delta, is_train=False, name_prefix='store_class')\n",
    "    X_tmp3.index = df_2017_store_class.index\n",
    "    X_tmp3 = X_tmp3.reindex(df_2017_store_class_index).reset_index(drop=True)\n",
    "\n",
    "    X_tmp = pd.concat([X_tmp, X_tmp2, X_tmp3, items.reset_index(), stores.reset_index()], axis=1)\n",
    "\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "\n",
    "X_train = pd.concat(X_l, axis=0)\n",
    "print(\"X_train columns \", X_train.columns)\n",
    "print(\"X_train top 5 \", X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train columns  (56, 16)\n",
      "y_train top 5  [[0.69314718 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.69314718 0.         0.\n",
      "  0.         0.         0.69314718 0.        ]\n",
      " [0.         1.38629436 1.09861229 1.94591015 1.09861229 1.09861229\n",
      "  0.         0.         0.69314718 0.         1.60943791 0.\n",
      "  1.09861229 0.69314718 0.         1.09861229]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.60943791 0.69314718 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.09861229\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "DataFrame(X_train).to_csv(\"refinedTrainDataX.csv\")\n",
    "\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "print(\"y_train columns \", y_train.shape)\n",
    "print(\"y_train top 5 \", y_train[:5])\n",
    "DataFrame(y_train).to_csv(\"refinedTrainDatay.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 561)\n",
      "(56, 16)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.zeros(5,4, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7342, 0.8014, 0.8818, 0.3403],\n",
       "         [0.5013, 0.4510, 0.1112, 0.3639],\n",
       "         [0.8678, 0.9006, 0.4366, 0.1365],\n",
       "         [0.5729, 0.4773, 0.4343, 0.8264]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.rand(4,4)\n",
    "y.view(1,4,4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
