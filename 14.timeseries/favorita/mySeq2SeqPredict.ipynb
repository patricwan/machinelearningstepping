{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import LSTM,GRU\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>everyDayHour</th>\n",
       "      <th>avg(cpu)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05/08/19_18</td>\n",
       "      <td>20.254444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05/08/19_19</td>\n",
       "      <td>22.451528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05/08/19_20</td>\n",
       "      <td>16.359259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05/08/19_21</td>\n",
       "      <td>10.932778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05/08/19_22</td>\n",
       "      <td>3.047917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  everyDayHour   avg(cpu)\n",
       "0  05/08/19_18  20.254444\n",
       "1  05/08/19_19  22.451528\n",
       "2  05/08/19_20  16.359259\n",
       "3  05/08/19_21  10.932778\n",
       "4  05/08/19_22   3.047917"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\n",
    "    \"../../../data/sflogs/loadCPU04t.csv\")\n",
    "\n",
    "df_train = df_train.astype({\"avg(cpu)\":'float'})\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>everyDayHour</th>\n",
       "      <th>avg(cpu)</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-08 18:00:00</td>\n",
       "      <td>20.254444</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-08 19:00:00</td>\n",
       "      <td>22.451528</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-08 20:00:00</td>\n",
       "      <td>16.359259</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-08 21:00:00</td>\n",
       "      <td>10.932778</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-08 22:00:00</td>\n",
       "      <td>3.047917</td>\n",
       "      <td>cpu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         everyDayHour   avg(cpu) metric\n",
       "0 2019-05-08 18:00:00  20.254444    cpu\n",
       "1 2019-05-08 19:00:00  22.451528    cpu\n",
       "2 2019-05-08 20:00:00  16.359259    cpu\n",
       "3 2019-05-08 21:00:00  10.932778    cpu\n",
       "4 2019-05-08 22:00:00   3.047917    cpu"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train[\"hour\"] =  df_train['everyDayHour'].map(lambda strDateHour: strDateHour[9:])\n",
    "df_train[\"everyDayHour\"] =  df_train['everyDayHour'].map(lambda strDateHour: \"20\"+strDateHour[6:8] \n",
    "                                                         + \"/\" + strDateHour[0:5] + \" \"\n",
    "                                                         + strDateHour[9:11] + \":00:00\")\n",
    "df_train[\"metric\"] = \"cpu\"\n",
    "#df_train.head()\n",
    "\n",
    "df_train[\"everyDayHour\"] = pd.to_datetime(df_train[\"everyDayHour\"], format='%Y%m%d %H:%M:%S')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>everyDayHour</th>\n",
       "      <th>2019-05-08 18:00:00</th>\n",
       "      <th>2019-05-08 19:00:00</th>\n",
       "      <th>2019-05-08 20:00:00</th>\n",
       "      <th>2019-05-08 21:00:00</th>\n",
       "      <th>2019-05-08 22:00:00</th>\n",
       "      <th>2019-05-08 23:00:00</th>\n",
       "      <th>2019-05-09 00:00:00</th>\n",
       "      <th>2019-05-09 01:00:00</th>\n",
       "      <th>2019-05-09 02:00:00</th>\n",
       "      <th>2019-05-09 03:00:00</th>\n",
       "      <th>...</th>\n",
       "      <th>2019-11-07 13:00:00</th>\n",
       "      <th>2019-11-07 14:00:00</th>\n",
       "      <th>2019-11-07 15:00:00</th>\n",
       "      <th>2019-11-07 16:00:00</th>\n",
       "      <th>2019-11-07 17:00:00</th>\n",
       "      <th>2019-11-07 18:00:00</th>\n",
       "      <th>2019-11-07 19:00:00</th>\n",
       "      <th>2019-11-07 20:00:00</th>\n",
       "      <th>2019-11-07 21:00:00</th>\n",
       "      <th>2019-11-07 22:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cpu</th>\n",
       "      <td>20.254444</td>\n",
       "      <td>22.451528</td>\n",
       "      <td>16.359259</td>\n",
       "      <td>10.932778</td>\n",
       "      <td>3.047917</td>\n",
       "      <td>7.469889</td>\n",
       "      <td>2.591429</td>\n",
       "      <td>1.671944</td>\n",
       "      <td>7.179815</td>\n",
       "      <td>18.505778</td>\n",
       "      <td>...</td>\n",
       "      <td>26.336926</td>\n",
       "      <td>30.060963</td>\n",
       "      <td>30.210574</td>\n",
       "      <td>24.985148</td>\n",
       "      <td>26.584407</td>\n",
       "      <td>21.189593</td>\n",
       "      <td>18.296315</td>\n",
       "      <td>16.747204</td>\n",
       "      <td>15.633907</td>\n",
       "      <td>15.351627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 4397 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "everyDayHour  2019-05-08 18:00:00  2019-05-08 19:00:00  2019-05-08 20:00:00  \\\n",
       "metric                                                                        \n",
       "cpu                     20.254444            22.451528            16.359259   \n",
       "\n",
       "everyDayHour  2019-05-08 21:00:00  2019-05-08 22:00:00  2019-05-08 23:00:00  \\\n",
       "metric                                                                        \n",
       "cpu                     10.932778             3.047917             7.469889   \n",
       "\n",
       "everyDayHour  2019-05-09 00:00:00  2019-05-09 01:00:00  2019-05-09 02:00:00  \\\n",
       "metric                                                                        \n",
       "cpu                      2.591429             1.671944             7.179815   \n",
       "\n",
       "everyDayHour  2019-05-09 03:00:00         ...           2019-11-07 13:00:00  \\\n",
       "metric                                    ...                                 \n",
       "cpu                     18.505778         ...                     26.336926   \n",
       "\n",
       "everyDayHour  2019-11-07 14:00:00  2019-11-07 15:00:00  2019-11-07 16:00:00  \\\n",
       "metric                                                                        \n",
       "cpu                     30.060963            30.210574            24.985148   \n",
       "\n",
       "everyDayHour  2019-11-07 17:00:00  2019-11-07 18:00:00  2019-11-07 19:00:00  \\\n",
       "metric                                                                        \n",
       "cpu                     26.584407            21.189593            18.296315   \n",
       "\n",
       "everyDayHour  2019-11-07 20:00:00  2019-11-07 21:00:00  2019-11-07 22:00:00  \n",
       "metric                                                                       \n",
       "cpu                     16.747204            15.633907            15.351627  \n",
       "\n",
       "[1 rows x 4397 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.pivot(index=\"metric\",columns=\"everyDayHour\",values=\"avg(cpu)\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2019-05-08 18:00:00', '2019-05-08 19:00:00',\n",
       "               '2019-05-08 20:00:00', '2019-05-08 21:00:00',\n",
       "               '2019-05-08 22:00:00', '2019-05-08 23:00:00',\n",
       "               '2019-05-09 00:00:00', '2019-05-09 01:00:00',\n",
       "               '2019-05-09 02:00:00', '2019-05-09 03:00:00',\n",
       "               ...\n",
       "               '2019-11-07 13:00:00', '2019-11-07 14:00:00',\n",
       "               '2019-11-07 15:00:00', '2019-11-07 16:00:00',\n",
       "               '2019-11-07 17:00:00', '2019-11-07 18:00:00',\n",
       "               '2019-11-07 19:00:00', '2019-11-07 20:00:00',\n",
       "               '2019-11-07 21:00:00', '2019-11-07 22:00:00'],\n",
       "              dtype='datetime64[ns]', name='everyDayHour', length=4397, freq=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_timespan(df, datetimeStart, minus, periods, freq='H'):\n",
    "    return df[pd.date_range(datetimeStart - timedelta(hours=minus), periods=periods, freq=freq)]\n",
    "\n",
    "t2019May19 = pd.to_datetime(\"2019-07-19 02:00:00\", format='%Y%m%d %H:%M:%S')\n",
    "\n",
    "dfRangeData = get_timespan(df_train,t2019May19, 12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(df, datetimeStart, is_train=True, name_prefix=None):\n",
    "    X = {}\n",
    "    #for i in [2, 7, 14, 28]:\n",
    "    for i in [2]:\n",
    "        tmp = get_timespan(df, datetimeStart, i, i)\n",
    "        #X['diff_%s_mean' % i] = tmp.diff(axis=1).mean(axis=1).values\n",
    "        #X['mean_%s_decay' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n",
    "        #X['mean_%s' % i] = tmp.mean(axis=1).values\n",
    "        X['median_%s' % i] = tmp.median(axis=1).values\n",
    "        #X['min_%s' % i] = tmp.min(axis=1).values\n",
    "        #X['max_%s' % i] = tmp.max(axis=1).values\n",
    "        #X['std_%s' % i] = tmp.std(axis=1).values\n",
    "    \n",
    "    for i in range(1, 24):\n",
    "        X['hour_%s_2019' % i] = get_timespan(df, datetimeStart, i, 1).values.ravel()\n",
    "        \n",
    "    \n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    # start date and periods is 12 days\n",
    "    if is_train:\n",
    "        yInput = df[pd.date_range(datetimeStart, periods=12)].values\n",
    "        yTarget = df[pd.date_range(datetimeStart + timedelta(hours=1), periods=12)].values\n",
    "        return X, yInput, yTarget\n",
    "    \n",
    "    return X\n",
    "\n",
    "#Z = prepare_dataset(df_train, t2019May19)\n",
    "#print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3800, 24)\n"
     ]
    }
   ],
   "source": [
    "X_l, y_lIn, y_lTg = [], [], []\n",
    "\n",
    "t2019Jul19 = pd.to_datetime(\"2019-05-21 02:00:00\", format='%Y%m%d %H:%M:%S')\n",
    "num_hours = 3800\n",
    "for i in range(num_hours):\n",
    "    delta = timedelta(hours = i)\n",
    "    X_tmp, y_tmp1, y_tmp2 = prepare_dataset(df_train, t2019Jul19 + delta)\n",
    "    \n",
    "    X_l.append(X_tmp)\n",
    "    y_lIn.append(y_tmp1)\n",
    "    y_lTg.append(y_tmp2)\n",
    "    \n",
    "X_train = pd.concat(X_l, axis=0)\n",
    "print(X_train.shape)\n",
    "DataFrame(X_train).to_csv(\"x_train_sfdc.csv\")\n",
    "\n",
    "y_trainIn = np.concatenate(y_lIn, axis=0)\n",
    "y_trainTarget = np.concatenate(y_lTg, axis=0)\n",
    "DataFrame(y_trainIn).to_csv(\"y_trainIn_sfdc.csv\")\n",
    "DataFrame(y_trainTarget).to_csv(\"y_trainTarget_sfdc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3800, 24, 1)\n",
      "(3800, 12, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.as_matrix()\n",
    "\n",
    "#reshape to 3D shape for X\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "y_trainIn = DataFrame(y_trainIn).as_matrix()\n",
    "y_trainIn = y_trainIn.reshape((y_trainIn.shape[0], y_trainIn.shape[1], 1))\n",
    "print(y_trainIn.shape)\n",
    "\n",
    "y_trainTarget = DataFrame(y_trainTarget).as_matrix()\n",
    "y_trainTarget = y_trainTarget.reshape((y_trainTarget.shape[0], y_trainTarget.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 32  # Latent dimensionality of the encoding space.\n",
    "#num_samples = 150 \n",
    "\n",
    "num_encoder_tokens = 1\n",
    "num_decoder_tokens = 1\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_input_data = X_train\n",
    "\n",
    "decoder_input_data = y_trainIn\n",
    "\n",
    "decoder_target_data = y_trainTarget\n",
    "\n",
    "# Run training\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "#model.compile(optimizer=opt, loss='mse',\n",
    "#              metrics=['mse'])\n",
    "#model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "#          batch_size=batch_size,\n",
    "#          epochs=epochs,\n",
    "#          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3800, 24, 1)\n",
      "(3800, 12, 1)\n",
      "(24, 10, 1)\n",
      "(12, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_trainTarget.shape)\n",
    "\n",
    "def generate_x_y_data(X_train, y_trainTarget, isTrain=True, batch_size=10):\n",
    "    # shape: (batch_size, seq_length, output_dim)\n",
    "    idx = 0\n",
    "    while True:\n",
    "        if idx + batch_size > X_train.shape[0]:\n",
    "            idx = 0\n",
    "        start = idx\n",
    "        idx += batch_size\n",
    "        yield X_train[start:start+batch_size,:,:], y_trainTarget[start:start+batch_size,:,:]\n",
    "    \n",
    "    # shape: (seq_length, batch_size, output_dim)\n",
    "    return None\n",
    "\n",
    "generateBatch = generate_x_y_data(X_train, y_trainTarget)\n",
    "xN, yN = next(generateBatch)\n",
    "\n",
    "batch_x = np.array(xN).transpose((1, 0, 2))\n",
    "batch_y = np.array(yN).transpose((1, 0, 2))\n",
    "# shape: (seq_length, batch_size, output_dim)\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "TensorFlow's version : 1.0 (or more)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  # Version 1.0 or 0.12\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "xN, yN = next(generateBatch)\n",
    "\n",
    "sample_x = np.array(xN).transpose((1, 0, 2))\n",
    "sample_y = np.array(yN).transpose((1, 0, 2))\n",
    "\n",
    "seq_lengthInput = sample_x.shape[0] \n",
    "seq_lengthOutput = sample_y.shape[0]\n",
    "batch_size = 10  # Low value used for live demo purposes - 100 and 1000 would be possible too, crank that up!\n",
    "\n",
    "input_dim = sample_x.shape[-1] \n",
    "output_dim = sample_y.shape[-1] \n",
    "\n",
    "# Output dimension (e.g.: multiple signals at once, tied in time)\n",
    "layers_stacked_count = 2  # Number of stacked recurrent cells, on the neural depth axis. \n",
    "\n",
    "# Optmizer: \n",
    "learning_rate = 0.007  # Small lr helps not to diverge during training. \n",
    "lr_decay = 0.92  # default: 0.9 . Simulated annealing.\n",
    "momentum = 0.5  # default: 0.0 . Momentum technique in weights update\n",
    "lambda_l2_reg = 0.003  # L2 regularization of weights - avoids overfitting\n",
    "\n",
    "nb_iters = 2500 # How many times we perform a training step (therefore how many times we show a batch). \n",
    "batch_size = 10 \n",
    "hidden_dim = 35 # Count of hidden neurons in the recurrent units.\n",
    "\n",
    "try:\n",
    "    tf.nn.seq2seq = tf.contrib.legacy_seq2seq\n",
    "    tf.nn.rnn_cell = tf.contrib.rnn\n",
    "    tf.nn.rnn_cell.GRUCell = tf.contrib.rnn.GRUCell\n",
    "    print(\"TensorFlow's version : 1.0 (or more)\")\n",
    "except: \n",
    "    print(\"TensorFlow's version : 0.12\")\n",
    "    \n",
    "\n",
    "    tf.reset_default_graph()\n",
    "# sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "with tf.variable_scope('Seq2seqPw',reuse=tf.AUTO_REUSE):\n",
    "\n",
    "    # Encoder: inputs\n",
    "    enc_inp = [\n",
    "        tf.placeholder(tf.float32, shape=(None, input_dim), name=\"inp_{}\".format(t))\n",
    "           for t in range(seq_lengthInput)\n",
    "    ]\n",
    "\n",
    "    # Decoder: expected outputs\n",
    "    expected_sparse_output = [\n",
    "        tf.placeholder(tf.float32, shape=(None, output_dim), name=\"expected_sparse_output_\".format(t))\n",
    "          for t in range(seq_lengthOutput)\n",
    "    ]\n",
    "    \n",
    "    # Give a \"GO\" token to the decoder. \n",
    "    # Note: we might want to fill the encoder with zeros or its own feedback rather than with \"+ enc_inp[:-1]\"\n",
    "    dec_inp = [ tf.zeros_like(enc_inp[0], dtype=np.float32, name=\"GO\") ] + enc_inp[:-1]\n",
    "\n",
    "    # Create a `layers_stacked_count` of stacked RNNs (GRU cells here). \n",
    "    cells = []\n",
    "    for i in range(layers_stacked_count):\n",
    "        with tf.variable_scope('RNN_{}'.format(i)):\n",
    "            cells.append(tf.nn.rnn_cell.GRUCell(hidden_dim))\n",
    "            # cells.append(tf.nn.rnn_cell.BasicLSTMCell(...))\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    \n",
    "    # Here, the encoder and the decoder uses the same cell, HOWEVER,\n",
    "    # the weights aren't shared among the encoder and decoder, we have two\n",
    "    # sets of weights created under the hood according to that function's def. \n",
    "    dec_outputs, dec_memory = tf.nn.seq2seq.basic_rnn_seq2seq(\n",
    "        enc_inp, \n",
    "        dec_inp, \n",
    "        cell\n",
    "    )\n",
    "    \n",
    "    # For reshaping the output dimensions of the seq2seq RNN: \n",
    "    w_out = tf.Variable(tf.random_normal([hidden_dim, output_dim]))\n",
    "    b_out = tf.Variable(tf.random_normal([output_dim]))\n",
    "    \n",
    "    # Final outputs: with linear rescaling for enabling possibly large and unrestricted output values.\n",
    "    output_scale_factor = tf.Variable(1.0, name=\"Output_ScaleFactor\", dtype=tf.float32)\n",
    "    \n",
    "    reshaped_outputs = [output_scale_factor*(tf.matmul(i, w_out) + b_out) for i in dec_outputs]\n",
    "\n",
    "\n",
    "# Training loss and optimizer\n",
    "\n",
    "with tf.variable_scope('Loss',reuse=tf.AUTO_REUSE):\n",
    "    # L2 loss\n",
    "    output_loss = 0\n",
    "    for _y, _Y in zip(reshaped_outputs, expected_sparse_output):\n",
    "        output_loss += tf.reduce_mean(tf.nn.l2_loss(_y - _Y))\n",
    "        \n",
    "    # L2 regularization (to avoid overfitting and to have a  better generalization capacity)\n",
    "    reg_loss = 0\n",
    "    for tf_var in tf.trainable_variables():\n",
    "        if not (\"Bias\" in tf_var.name or \"Output_\" in tf_var.name):\n",
    "            reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf.cast(tf_var,tf.float32)))\n",
    "            \n",
    "    loss = output_loss + lambda_l2_reg * reg_loss\n",
    "\n",
    "with tf.variable_scope('Optimizer',reuse=tf.AUTO_REUSE):\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=lr_decay, momentum=momentum)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(batch_size):\n",
    "    \"\"\"\n",
    "    Training step that optimizes the weights \n",
    "    provided some batch_size X and Y examples from the dataset. \n",
    "    \"\"\"\n",
    "    #X, Y = generate_x_y_data(isTrain=True, batch_size=batch_size)\n",
    "    xN, yN = next(generateBatch)\n",
    "    X = np.array(xN).transpose((1, 0, 2))\n",
    "    Y = np.array(yN).transpose((1, 0, 2))\n",
    "    feed_dict = {enc_inp[t]: X[t] for t in range(len(enc_inp))}\n",
    "    feed_dict.update({expected_sparse_output[t]: Y[t] for t in range(len(expected_sparse_output))})\n",
    "    _, loss_t = sess.run([train_op, loss], feed_dict)\n",
    "    return loss_t\n",
    "\n",
    "def test_batch(batch_size):\n",
    "    \"\"\"\n",
    "    Test step, does NOT optimizes. Weights are frozen by not\n",
    "    doing sess.run on the train_op. \n",
    "    \"\"\"\n",
    "    xN, yN = next(generateBatch)\n",
    "    X = np.array(xN).transpose((1, 0, 2))\n",
    "    Y = np.array(yN).transpose((1, 0, 2))\n",
    "    \n",
    "    feed_dict = {enc_inp[t]: X[t] for t in range(len(enc_inp))}\n",
    "    feed_dict.update({expected_sparse_output[t]: Y[t] for t in range(len(expected_sparse_output))})\n",
    "    loss_t = sess.run([loss], feed_dict)\n",
    "    return loss_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/2500, train loss: 6206.8642578125, \tTEST loss: 7558.41845703125\n",
      "Step 10/2500, train loss: 4637.1337890625, \tTEST loss: 2795.748779296875\n",
      "Step 20/2500, train loss: 4947.21044921875, \tTEST loss: 4695.51806640625\n",
      "Step 30/2500, train loss: 5462.1279296875, \tTEST loss: 7498.3037109375\n",
      "Step 40/2500, train loss: 12305.8701171875, \tTEST loss: 7604.4306640625\n",
      "Step 50/2500, train loss: 13582.525390625, \tTEST loss: 16721.82421875\n",
      "Step 60/2500, train loss: 15582.94921875, \tTEST loss: 13302.5234375\n",
      "Step 70/2500, train loss: 6922.8896484375, \tTEST loss: 8436.9736328125\n",
      "Step 80/2500, train loss: 6318.71435546875, \tTEST loss: 6861.359375\n",
      "Step 90/2500, train loss: 7537.73291015625, \tTEST loss: 6349.357421875\n",
      "Step 100/2500, train loss: 5528.35791015625, \tTEST loss: 12418.654296875\n",
      "Step 110/2500, train loss: 15087.533203125, \tTEST loss: 8453.6982421875\n",
      "Step 120/2500, train loss: 7512.51708984375, \tTEST loss: 12417.798828125\n",
      "Step 130/2500, train loss: 9274.8798828125, \tTEST loss: 7529.31005859375\n",
      "Step 140/2500, train loss: 5759.76953125, \tTEST loss: 4964.57763671875\n",
      "Step 150/2500, train loss: 1786.6676025390625, \tTEST loss: 2162.6904296875\n",
      "Step 160/2500, train loss: 3540.948486328125, \tTEST loss: 1336.4996337890625\n",
      "Step 170/2500, train loss: 670.73681640625, \tTEST loss: 1898.7489013671875\n",
      "Step 180/2500, train loss: 955.5780639648438, \tTEST loss: 359.4388427734375\n",
      "Step 190/2500, train loss: 566.432861328125, \tTEST loss: 954.4078979492188\n",
      "Step 200/2500, train loss: 461.2127380371094, \tTEST loss: 490.3343811035156\n",
      "Step 210/2500, train loss: 1839.155029296875, \tTEST loss: 1354.0916748046875\n",
      "Step 220/2500, train loss: 427.9332580566406, \tTEST loss: 1466.3524169921875\n",
      "Step 230/2500, train loss: 2108.388671875, \tTEST loss: 465.9803771972656\n",
      "Step 240/2500, train loss: 895.043212890625, \tTEST loss: 1171.9725341796875\n",
      "Step 250/2500, train loss: 519.2715454101562, \tTEST loss: 157.74342346191406\n",
      "Step 260/2500, train loss: 614.4931640625, \tTEST loss: 429.28369140625\n",
      "Step 270/2500, train loss: 478.02783203125, \tTEST loss: 968.0471801757812\n",
      "Step 280/2500, train loss: 1926.360595703125, \tTEST loss: 245.02833557128906\n",
      "Step 290/2500, train loss: 102.96026611328125, \tTEST loss: 1790.5120849609375\n",
      "Step 300/2500, train loss: 5302.0380859375, \tTEST loss: 659.5403442382812\n",
      "Step 310/2500, train loss: 619.9619750976562, \tTEST loss: 3893.075439453125\n",
      "Step 320/2500, train loss: 553.8135375976562, \tTEST loss: 699.6149291992188\n",
      "Step 330/2500, train loss: 1537.915283203125, \tTEST loss: 1485.041015625\n",
      "Step 340/2500, train loss: 93.15335845947266, \tTEST loss: 2753.83154296875\n",
      "Step 350/2500, train loss: 2330.393798828125, \tTEST loss: 4688.052734375\n",
      "Step 360/2500, train loss: 2546.720703125, \tTEST loss: 2922.06787109375\n",
      "Step 370/2500, train loss: 7274.73388671875, \tTEST loss: 4322.2666015625\n",
      "Step 380/2500, train loss: 5632.50341796875, \tTEST loss: 7476.57177734375\n",
      "Step 390/2500, train loss: 14228.392578125, \tTEST loss: 11280.7998046875\n",
      "Step 400/2500, train loss: 10598.875, \tTEST loss: 15515.6728515625\n",
      "Step 410/2500, train loss: 11901.314453125, \tTEST loss: 9311.216796875\n",
      "Step 420/2500, train loss: 6809.81298828125, \tTEST loss: 5396.8896484375\n",
      "Step 430/2500, train loss: 4525.10546875, \tTEST loss: 3568.663330078125\n",
      "Step 440/2500, train loss: 11860.7236328125, \tTEST loss: 6169.97265625\n",
      "Step 450/2500, train loss: 7922.392578125, \tTEST loss: 15828.654296875\n",
      "Step 460/2500, train loss: 7999.56689453125, \tTEST loss: 7669.83935546875\n",
      "Step 470/2500, train loss: 9620.1533203125, \tTEST loss: 11193.2314453125\n",
      "Step 480/2500, train loss: 7611.90869140625, \tTEST loss: 7296.05029296875\n",
      "Step 490/2500, train loss: 1666.30126953125, \tTEST loss: 3478.083740234375\n",
      "Step 500/2500, train loss: 1058.69287109375, \tTEST loss: 2232.660400390625\n",
      "Step 510/2500, train loss: 2907.3896484375, \tTEST loss: 766.4965209960938\n",
      "Step 520/2500, train loss: 456.97930908203125, \tTEST loss: 1519.34130859375\n",
      "Step 530/2500, train loss: 606.1082763671875, \tTEST loss: 291.4962463378906\n",
      "Step 540/2500, train loss: 1140.7802734375, \tTEST loss: 600.977294921875\n",
      "Step 550/2500, train loss: 182.3715362548828, \tTEST loss: 1384.9739990234375\n",
      "Step 560/2500, train loss: 1971.5267333984375, \tTEST loss: 775.5149536132812\n",
      "Step 570/2500, train loss: 1268.4110107421875, \tTEST loss: 2226.6640625\n",
      "Step 580/2500, train loss: 1256.63916015625, \tTEST loss: 366.88555908203125\n",
      "Step 590/2500, train loss: 440.200439453125, \tTEST loss: 689.1949462890625\n",
      "Step 600/2500, train loss: 704.4880981445312, \tTEST loss: 529.6904296875\n",
      "Step 610/2500, train loss: 911.1454467773438, \tTEST loss: 344.61529541015625\n",
      "Step 620/2500, train loss: 128.48199462890625, \tTEST loss: 1435.4718017578125\n",
      "Step 630/2500, train loss: 2742.66015625, \tTEST loss: 1524.9188232421875\n",
      "Step 640/2500, train loss: 211.49620056152344, \tTEST loss: 1807.4532470703125\n",
      "Step 650/2500, train loss: 3198.147216796875, \tTEST loss: 181.02821350097656\n",
      "Step 660/2500, train loss: 1939.974609375, \tTEST loss: 3808.711669921875\n",
      "Step 670/2500, train loss: 178.30116271972656, \tTEST loss: 949.1920166015625\n",
      "Step 680/2500, train loss: 1668.730224609375, \tTEST loss: 282.6802978515625\n",
      "Step 690/2500, train loss: 4824.89111328125, \tTEST loss: 2281.82373046875\n",
      "Step 700/2500, train loss: 3105.55224609375, \tTEST loss: 3671.166748046875\n",
      "Step 710/2500, train loss: 2207.1865234375, \tTEST loss: 4350.86572265625\n",
      "Step 720/2500, train loss: 7453.357421875, \tTEST loss: 5898.5341796875\n",
      "Step 730/2500, train loss: 6352.75732421875, \tTEST loss: 11356.3720703125\n",
      "Step 740/2500, train loss: 15594.3125, \tTEST loss: 13043.5078125\n",
      "Step 750/2500, train loss: 13821.5458984375, \tTEST loss: 15850.7919921875\n",
      "Step 760/2500, train loss: 5956.201171875, \tTEST loss: 5582.4140625\n",
      "Step 770/2500, train loss: 7974.23291015625, \tTEST loss: 6088.33642578125\n",
      "Step 780/2500, train loss: 2155.22900390625, \tTEST loss: 8477.1396484375\n",
      "Step 790/2500, train loss: 12422.5068359375, \tTEST loss: 5923.21826171875\n",
      "Step 800/2500, train loss: 11148.4423828125, \tTEST loss: 14497.46484375\n",
      "Step 810/2500, train loss: 8761.205078125, \tTEST loss: 7451.94970703125\n",
      "Step 820/2500, train loss: 8535.626953125, \tTEST loss: 8028.41357421875\n",
      "Step 830/2500, train loss: 6479.376953125, \tTEST loss: 5166.65771484375\n",
      "Step 840/2500, train loss: 3179.290771484375, \tTEST loss: 1177.293212890625\n",
      "Step 850/2500, train loss: 502.9029541015625, \tTEST loss: 2439.77587890625\n",
      "Step 860/2500, train loss: 2084.0849609375, \tTEST loss: 531.30322265625\n",
      "Step 870/2500, train loss: 795.3272094726562, \tTEST loss: 1080.6815185546875\n",
      "Step 880/2500, train loss: 435.2230529785156, \tTEST loss: 531.7259521484375\n",
      "Step 890/2500, train loss: 805.3583984375, \tTEST loss: 444.0896911621094\n",
      "Step 900/2500, train loss: 785.65478515625, \tTEST loss: 1911.3485107421875\n",
      "Step 910/2500, train loss: 2544.2978515625, \tTEST loss: 414.91705322265625\n",
      "Step 920/2500, train loss: 460.33441162109375, \tTEST loss: 2179.6474609375\n",
      "Step 930/2500, train loss: 879.2760009765625, \tTEST loss: 669.9067993164062\n",
      "Step 940/2500, train loss: 753.3422241210938, \tTEST loss: 548.4599609375\n",
      "Step 950/2500, train loss: 206.8072967529297, \tTEST loss: 659.3253784179688\n",
      "Step 960/2500, train loss: 2350.945556640625, \tTEST loss: 304.58856201171875\n",
      "Step 970/2500, train loss: 105.25337219238281, \tTEST loss: 2002.833984375\n",
      "Step 980/2500, train loss: 1082.0966796875, \tTEST loss: 127.2566909790039\n",
      "Step 990/2500, train loss: 1552.6300048828125, \tTEST loss: 4322.87744140625\n",
      "Step 1000/2500, train loss: 570.6724853515625, \tTEST loss: 636.737060546875\n",
      "Step 1010/2500, train loss: 3537.106689453125, \tTEST loss: 525.5549926757812\n",
      "Step 1020/2500, train loss: 559.0736694335938, \tTEST loss: 1567.26318359375\n",
      "Step 1030/2500, train loss: 2262.478759765625, \tTEST loss: 93.11207580566406\n",
      "Step 1040/2500, train loss: 2115.579833984375, \tTEST loss: 2282.14404296875\n",
      "Step 1050/2500, train loss: 4965.32763671875, \tTEST loss: 2797.056396484375\n",
      "Step 1060/2500, train loss: 3814.95703125, \tTEST loss: 6705.98486328125\n",
      "Step 1070/2500, train loss: 7982.3642578125, \tTEST loss: 5729.166015625\n",
      "Step 1080/2500, train loss: 10984.322265625, \tTEST loss: 14542.7001953125\n",
      "Step 1090/2500, train loss: 10332.791015625, \tTEST loss: 10349.140625\n",
      "Step 1100/2500, train loss: 14301.1015625, \tTEST loss: 11936.931640625\n",
      "Step 1110/2500, train loss: 6542.12451171875, \tTEST loss: 8016.365234375\n",
      "Step 1120/2500, train loss: 4171.2958984375, \tTEST loss: 1886.953369140625\n",
      "Step 1130/2500, train loss: 5926.97900390625, \tTEST loss: 13042.9091796875\n",
      "Step 1140/2500, train loss: 10032.5, \tTEST loss: 6612.080078125\n",
      "Step 1150/2500, train loss: 7484.5634765625, \tTEST loss: 9690.9423828125\n",
      "Step 1160/2500, train loss: 9187.2109375, \tTEST loss: 9098.5302734375\n",
      "Step 1170/2500, train loss: 8517.0859375, \tTEST loss: 8393.080078125\n",
      "Step 1180/2500, train loss: 2689.87548828125, \tTEST loss: 1282.464111328125\n",
      "Step 1190/2500, train loss: 4044.613525390625, \tTEST loss: 701.0133666992188\n",
      "Step 1200/2500, train loss: 1193.746826171875, \tTEST loss: 2592.8173828125\n",
      "Step 1210/2500, train loss: 1025.390625, \tTEST loss: 382.4085998535156\n",
      "Step 1220/2500, train loss: 645.7559814453125, \tTEST loss: 496.834228515625\n",
      "Step 1230/2500, train loss: 789.951171875, \tTEST loss: 1110.394287109375\n",
      "Step 1240/2500, train loss: 1335.8394775390625, \tTEST loss: 164.0971221923828\n",
      "Step 1250/2500, train loss: 383.9443054199219, \tTEST loss: 1888.841064453125\n",
      "Step 1260/2500, train loss: 1910.25732421875, \tTEST loss: 1718.6600341796875\n",
      "Step 1270/2500, train loss: 1006.933349609375, \tTEST loss: 1667.0299072265625\n",
      "Step 1280/2500, train loss: 275.1485290527344, \tTEST loss: 413.56817626953125\n",
      "Step 1290/2500, train loss: 958.0936279296875, \tTEST loss: 518.8699340820312\n",
      "Step 1300/2500, train loss: 74.23849487304688, \tTEST loss: 954.0537109375\n",
      "Step 1310/2500, train loss: 1415.5128173828125, \tTEST loss: 92.52501678466797\n",
      "Step 1320/2500, train loss: 373.9147033691406, \tTEST loss: 2072.065673828125\n",
      "Step 1330/2500, train loss: 950.6661987304688, \tTEST loss: 184.9624481201172\n",
      "Step 1340/2500, train loss: 1037.6685791015625, \tTEST loss: 3037.168701171875\n",
      "Step 1350/2500, train loss: 321.2630615234375, \tTEST loss: 1722.4874267578125\n",
      "Step 1360/2500, train loss: 1804.5469970703125, \tTEST loss: 146.1693115234375\n",
      "Step 1370/2500, train loss: 120.88523864746094, \tTEST loss: 1509.425048828125\n",
      "Step 1380/2500, train loss: 5658.623046875, \tTEST loss: 4549.23193359375\n",
      "Step 1390/2500, train loss: 1613.468994140625, \tTEST loss: 3630.550048828125\n",
      "Step 1400/2500, train loss: 6458.27880859375, \tTEST loss: 2253.693115234375\n",
      "Step 1410/2500, train loss: 5440.31494140625, \tTEST loss: 7573.10498046875\n",
      "Step 1420/2500, train loss: 7952.27783203125, \tTEST loss: 4967.06640625\n",
      "Step 1430/2500, train loss: 14943.9775390625, \tTEST loss: 15580.576171875\n",
      "Step 1440/2500, train loss: 12592.484375, \tTEST loss: 14100.3388671875\n",
      "Step 1450/2500, train loss: 7944.78564453125, \tTEST loss: 6940.9326171875\n",
      "Step 1460/2500, train loss: 5567.36083984375, \tTEST loss: 8012.77880859375\n",
      "Step 1470/2500, train loss: 4910.14892578125, \tTEST loss: 2393.000244140625\n",
      "Step 1480/2500, train loss: 7359.79345703125, \tTEST loss: 12370.517578125\n",
      "Step 1490/2500, train loss: 13637.7216796875, \tTEST loss: 11720.4453125\n",
      "Step 1500/2500, train loss: 9910.6279296875, \tTEST loss: 9675.94921875\n",
      "Step 1510/2500, train loss: 8609.884765625, \tTEST loss: 8581.2685546875\n",
      "Step 1520/2500, train loss: 7254.52392578125, \tTEST loss: 6081.1318359375\n",
      "Step 1530/2500, train loss: 1488.910888671875, \tTEST loss: 3148.4453125\n",
      "Step 1540/2500, train loss: 2903.6474609375, \tTEST loss: 390.069091796875\n",
      "Step 1550/2500, train loss: 1003.5899658203125, \tTEST loss: 2269.087890625\n",
      "Step 1560/2500, train loss: 643.7916870117188, \tTEST loss: 798.8939208984375\n",
      "Step 1570/2500, train loss: 1117.37890625, \tTEST loss: 450.1682434082031\n",
      "Step 1580/2500, train loss: 123.61933135986328, \tTEST loss: 794.1270751953125\n",
      "Step 1590/2500, train loss: 1943.3963623046875, \tTEST loss: 725.6138305664062\n",
      "Step 1600/2500, train loss: 493.9218444824219, \tTEST loss: 2390.36669921875\n",
      "Step 1610/2500, train loss: 1608.428955078125, \tTEST loss: 383.0586853027344\n",
      "Step 1620/2500, train loss: 1444.0289306640625, \tTEST loss: 744.4417114257812\n",
      "Step 1630/2500, train loss: 158.56336975097656, \tTEST loss: 888.54150390625\n",
      "Step 1640/2500, train loss: 840.7988891601562, \tTEST loss: 115.3393783569336\n",
      "Step 1650/2500, train loss: 331.8785400390625, \tTEST loss: 1933.684814453125\n",
      "Step 1660/2500, train loss: 1166.50048828125, \tTEST loss: 94.62568664550781\n",
      "Step 1670/2500, train loss: 685.4386596679688, \tTEST loss: 1214.3585205078125\n",
      "Step 1680/2500, train loss: 847.5543823242188, \tTEST loss: 1153.6221923828125\n",
      "Step 1690/2500, train loss: 3652.090087890625, \tTEST loss: 681.8831787109375\n",
      "Step 1700/2500, train loss: 78.84727478027344, \tTEST loss: 3600.13525390625\n",
      "Step 1710/2500, train loss: 1824.0362548828125, \tTEST loss: 1274.215087890625\n",
      "Step 1720/2500, train loss: 310.32366943359375, \tTEST loss: 2216.4921875\n",
      "Step 1730/2500, train loss: 3893.997314453125, \tTEST loss: 2032.2020263671875\n",
      "Step 1740/2500, train loss: 1612.0699462890625, \tTEST loss: 4868.80859375\n",
      "Step 1750/2500, train loss: 6017.17919921875, \tTEST loss: 3882.739501953125\n",
      "Step 1760/2500, train loss: 6122.99365234375, \tTEST loss: 8208.7734375\n",
      "Step 1770/2500, train loss: 10279.7578125, \tTEST loss: 10715.798828125\n",
      "Step 1780/2500, train loss: 12673.171875, \tTEST loss: 12195.3291015625\n",
      "Step 1790/2500, train loss: 11417.884765625, \tTEST loss: 14852.6962890625\n",
      "Step 1800/2500, train loss: 8794.728515625, \tTEST loss: 5812.900390625\n",
      "Step 1810/2500, train loss: 6345.91650390625, \tTEST loss: 6012.21142578125\n",
      "Step 1820/2500, train loss: 9885.4140625, \tTEST loss: 5887.78173828125\n",
      "Step 1830/2500, train loss: 9323.205078125, \tTEST loss: 10920.62109375\n",
      "Step 1840/2500, train loss: 8312.26953125, \tTEST loss: 7479.7841796875\n",
      "Step 1850/2500, train loss: 12049.5654296875, \tTEST loss: 9484.7060546875\n",
      "Step 1860/2500, train loss: 7570.7626953125, \tTEST loss: 8300.1708984375\n",
      "Step 1870/2500, train loss: 3890.432373046875, \tTEST loss: 2484.4052734375\n",
      "Step 1880/2500, train loss: 575.0800170898438, \tTEST loss: 3623.818603515625\n",
      "Step 1890/2500, train loss: 1539.9925537109375, \tTEST loss: 1457.4986572265625\n",
      "Step 1900/2500, train loss: 782.6221923828125, \tTEST loss: 799.7030029296875\n",
      "Step 1910/2500, train loss: 133.2930450439453, \tTEST loss: 618.6041870117188\n",
      "Step 1920/2500, train loss: 1157.9578857421875, \tTEST loss: 757.78271484375\n",
      "Step 1930/2500, train loss: 158.56390380859375, \tTEST loss: 1292.046875\n",
      "Step 1940/2500, train loss: 1752.9102783203125, \tTEST loss: 341.08221435546875\n",
      "Step 1950/2500, train loss: 733.1011352539062, \tTEST loss: 997.990234375\n",
      "Step 1960/2500, train loss: 1019.9617309570312, \tTEST loss: 938.4375\n",
      "Step 1970/2500, train loss: 853.051025390625, \tTEST loss: 295.0245056152344\n",
      "Step 1980/2500, train loss: 760.9700317382812, \tTEST loss: 950.0842895507812\n",
      "Step 1990/2500, train loss: 1005.5819702148438, \tTEST loss: 79.40933227539062\n",
      "Step 2000/2500, train loss: 178.5595245361328, \tTEST loss: 1498.563232421875\n",
      "Step 2010/2500, train loss: 1062.444580078125, \tTEST loss: 672.8745727539062\n",
      "Step 2020/2500, train loss: 1299.822265625, \tTEST loss: 881.1655883789062\n",
      "Step 2030/2500, train loss: 189.8174285888672, \tTEST loss: 905.7539672851562\n",
      "Step 2040/2500, train loss: 3669.31005859375, \tTEST loss: 692.0206909179688\n",
      "Step 2050/2500, train loss: 91.9638671875, \tTEST loss: 1959.1072998046875\n",
      "Step 2060/2500, train loss: 1179.010009765625, \tTEST loss: 119.1656723022461\n",
      "Step 2070/2500, train loss: 1577.2403564453125, \tTEST loss: 6837.89599609375\n",
      "Step 2080/2500, train loss: 4688.904296875, \tTEST loss: 2627.623046875\n",
      "Step 2090/2500, train loss: 940.845458984375, \tTEST loss: 6015.1416015625\n",
      "Step 2100/2500, train loss: 6006.6806640625, \tTEST loss: 5277.40380859375\n",
      "Step 2110/2500, train loss: 6129.91162109375, \tTEST loss: 5823.26318359375\n",
      "Step 2120/2500, train loss: 13637.7724609375, \tTEST loss: 14531.0458984375\n",
      "Step 2130/2500, train loss: 12686.767578125, \tTEST loss: 12949.9697265625\n",
      "Step 2140/2500, train loss: 8201.4794921875, \tTEST loss: 7799.71240234375\n",
      "Step 2150/2500, train loss: 10697.6513671875, \tTEST loss: 5951.67626953125\n",
      "Step 2160/2500, train loss: 2528.64599609375, \tTEST loss: 4569.86767578125\n",
      "Step 2170/2500, train loss: 8809.49609375, \tTEST loss: 7411.8828125\n",
      "Step 2180/2500, train loss: 15434.7666015625, \tTEST loss: 13683.166015625\n",
      "Step 2190/2500, train loss: 5169.73193359375, \tTEST loss: 8784.0859375\n",
      "Step 2200/2500, train loss: 11189.791015625, \tTEST loss: 8199.9775390625\n",
      "Step 2210/2500, train loss: 7644.6845703125, \tTEST loss: 8375.681640625\n",
      "Step 2220/2500, train loss: 2799.709228515625, \tTEST loss: 1931.963134765625\n",
      "Step 2230/2500, train loss: 1176.37548828125, \tTEST loss: 3210.545654296875\n",
      "Step 2240/2500, train loss: 1268.567626953125, \tTEST loss: 790.5\n",
      "Step 2250/2500, train loss: 1450.0941162109375, \tTEST loss: 711.743408203125\n",
      "Step 2260/2500, train loss: 149.47634887695312, \tTEST loss: 1150.8485107421875\n",
      "Step 2270/2500, train loss: 949.8088989257812, \tTEST loss: 131.88026428222656\n",
      "Step 2280/2500, train loss: 442.9245300292969, \tTEST loss: 2167.7099609375\n",
      "Step 2290/2500, train loss: 1644.791015625, \tTEST loss: 355.8050231933594\n",
      "Step 2300/2500, train loss: 1321.710205078125, \tTEST loss: 1453.516845703125\n",
      "Step 2310/2500, train loss: 314.4710388183594, \tTEST loss: 1609.3968505859375\n",
      "Step 2320/2500, train loss: 895.5755004882812, \tTEST loss: 183.829833984375\n",
      "Step 2330/2500, train loss: 98.1861343383789, \tTEST loss: 739.968017578125\n",
      "Step 2340/2500, train loss: 762.9303588867188, \tTEST loss: 190.14022827148438\n",
      "Step 2350/2500, train loss: 766.2300415039062, \tTEST loss: 1132.0592041015625\n",
      "Step 2360/2500, train loss: 427.5062561035156, \tTEST loss: 524.3704223632812\n",
      "Step 2370/2500, train loss: 3524.714599609375, \tTEST loss: 696.2359008789062\n",
      "Step 2380/2500, train loss: 80.92677307128906, \tTEST loss: 3471.891357421875\n",
      "Step 2390/2500, train loss: 4162.32861328125, \tTEST loss: 88.52426147460938\n",
      "Step 2400/2500, train loss: 156.82882690429688, \tTEST loss: 1316.3240966796875\n",
      "Step 2410/2500, train loss: 880.4155883789062, \tTEST loss: 422.65240478515625\n",
      "Step 2420/2500, train loss: 1271.3809814453125, \tTEST loss: 4353.2568359375\n",
      "Step 2430/2500, train loss: 7629.37109375, \tTEST loss: 1218.0965576171875\n",
      "Step 2440/2500, train loss: 2547.78955078125, \tTEST loss: 5232.2109375\n",
      "Step 2450/2500, train loss: 8869.548828125, \tTEST loss: 6344.396484375\n",
      "Step 2460/2500, train loss: 9114.9296875, \tTEST loss: 9384.77734375\n",
      "Step 2470/2500, train loss: 12926.169921875, \tTEST loss: 12124.6171875\n",
      "Step 2480/2500, train loss: 18085.537109375, \tTEST loss: 12244.5029296875\n",
      "Step 2490/2500, train loss: 2720.73876953125, \tTEST loss: 5862.4921875\n",
      "Step 2500/2500, train loss: 8557.06640625, \tTEST loss: 6054.01416015625\n",
      "Fin. train loss: 8557.06640625, \tTEST loss: 6054.01416015625\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for t in range(nb_iters+1):\n",
    "    train_loss = train_batch(batch_size)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if t % 10 == 0: \n",
    "        # Tester\n",
    "        test_loss = test_batch(batch_size)\n",
    "        test_losses.append(test_loss)\n",
    "        print(\"Step {}/{}, train loss: {}, \\tTEST loss: {}\".format(t, nb_iters, train_loss, test_loss))\n",
    "\n",
    "print(\"Fin. train loss: {}, \\tTEST loss: {}\".format(train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's visualize 5 predictions with our signals:\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "nb_predictions = 5\n",
    "print(\"Let's visualize {} predictions with our signals:\".format(nb_predictions))\n",
    "\n",
    "#X, Y = generate_x_y_data(isTrain=False, batch_size=nb_predictions)\n",
    "xN, yN = next(generateBatch)\n",
    "X = np.array(xN).transpose((1, 0, 2))\n",
    "Y = np.array(yN).transpose((1, 0, 2))\n",
    "    \n",
    "feed_dict = {enc_inp[t]: X[t] for t in range(seq_lengthInput)}\n",
    "outputs = np.array(sess.run([reshaped_outputs], feed_dict)[0])\n",
    "\n",
    "for j in range(nb_predictions): \n",
    "    plt.figure(figsize=(12, 3))\n",
    "    \n",
    "    for k in range(output_dim):\n",
    "        past = X[:,j,k]\n",
    "        expected = Y[:,j,k] \n",
    "        pred = outputs[:,j,k]\n",
    "        \n",
    "        label1 = \"Seen (past) values\" if k==0 else \"_nolegend_\"\n",
    "        label2 = \"True future values\" if k==0 else \"_nolegend_\"\n",
    "        label3 = \"Predictions\" if k==0 else \"_nolegend_\"\n",
    "        plt.plot(range(len(past)), past, \"o--b\", label=label1)\n",
    "        plt.plot(range(len(past), len(expected)+len(past)), expected, \"x--b\", label=label2)\n",
    "        plt.plot(range(len(past), len(pred)+len(past)), pred, \"o--y\", label=label3)\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"Predictions v.s. true values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.zeros(5,4)\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
