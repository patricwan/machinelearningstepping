{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Time Series forecast using seq2seq in TensorFlow\n",
    "\n",
    "This notebook implements the cast study of applying seq2seq model for time series data. \n",
    "\n",
    "The purpose is to showcase the effectiveness of seq2seq to learn the true patterns under the noisy signals. In addition, we are able to implement models with flexibility such as: \n",
    "\n",
    "- Variable input and output sequence lengths\n",
    "- Variable numbers of input and output signals \n",
    "\n",
    "This tutorial is divided into four parts - first we will be demonstrating how to train a basicseq2seq model on univariate data. The model is then easily applied to multivariate cases. We will then discuss about situation with outliers. And finally, we will showcase a real-world dataset to forecast pollution (pm2.5) in Beijing. \n",
    "\n",
    "To see the comprehensive explanations of each step, can jump to the post associated with this study - \n",
    "weiminwang.blog/2017/09/29/multivariate-time-series-forecast-using-seq2seq-in-tensorflow/\n",
    "\n",
    "This is a long notebook - you can choose the session of your interests by clicking on the links below: \n",
    "\n",
    "## Contents\n",
    "\n",
    "### 1) <b>[Univariate time series](#session1)</b> \n",
    "\n",
    "### 2) <b>[Multivariate time series](#session2)</b> \n",
    "\n",
    "### 3) <b>[Seq2seq for outliers/extreme events](#session3)</b> \n",
    "\n",
    "### 4) <b>[A case study - Beijing pollution data](#session4)</b> \n",
    "data credits go to UCI - https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='session1'></a>\n",
    "# Univariate time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import random\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 30, 105)\n",
    "y = 2 * np.sin(x)\n",
    "\n",
    "l1, = plt.plot(x[:85], y[:85], 'y', label = 'training samples')\n",
    "l2, = plt.plot(x[85:], y[85:105], 'c--', label = 'test samples')\n",
    "plt.legend(handles = [l1, l2], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Session2'></a>\n",
    "## Real-world training data may be noisier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y = y.copy()\n",
    "\n",
    "noise_factor = 0.5\n",
    "train_y += np.random.randn(105) * noise_factor\n",
    "\n",
    "l1, = plt.plot(x[:85], train_y[:85], 'yo', label = 'training samples')\n",
    "plt.plot(x[:85], y[:85], 'y:')\n",
    "l2, = plt.plot(x[85:], train_y[85:], 'co', label = 'test samples')\n",
    "plt.plot(x[85:], y[85:], 'c:')\n",
    "plt.legend(handles = [l1, l2], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function for generating training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_seq_len = 15\n",
    "output_seq_len = 20\n",
    "\n",
    "x = np.linspace(0, 30, 105)\n",
    "train_data_x = x[:85]\n",
    "\n",
    "def true_signal(x):\n",
    "    y = 2 * np.sin(x)\n",
    "    return y\n",
    "\n",
    "def noise_func(x, noise_factor = 1):\n",
    "    return np.random.randn(len(x)) * noise_factor\n",
    "\n",
    "def generate_y_values(x):\n",
    "    return true_signal(x) + noise_func(x)\n",
    "\n",
    "def generate_train_samples(x = train_data_x, batch_size = 10, input_seq_len = input_seq_len, output_seq_len = output_seq_len):\n",
    "\n",
    "    total_start_points = len(x) - input_seq_len - output_seq_len\n",
    "    start_x_idx = np.random.choice(range(total_start_points), batch_size)\n",
    "    \n",
    "    input_seq_x = [x[i:(i+input_seq_len)] for i in start_x_idx]\n",
    "    output_seq_x = [x[(i+input_seq_len):(i+input_seq_len+output_seq_len)] for i in start_x_idx]\n",
    "    \n",
    "    input_seq_y = [generate_y_values(x) for x in input_seq_x]\n",
    "    output_seq_y = [generate_y_values(x) for x in output_seq_x]\n",
    "    \n",
    "    #batch_x = np.array([[true_signal()]])\n",
    "    return np.array(input_seq_y), np.array(output_seq_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_seq, output_seq = generate_train_samples(batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1, = plt.plot(range(15), input_seq[1], 'go', label = 'input sequence for one sample')\n",
    "l2, = plt.plot(range(15, 35), output_seq[1], 'ro', label = 'output sequence for one sample')\n",
    "plt.legend(handles = [l1, l2], loc = 'lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(100):\n",
    "    temp = generate_y_values(x)\n",
    "    results.append(temp)\n",
    "results = np.array(results)\n",
    "\n",
    "for i in range(100):\n",
    "    l1, = plt.plot(results[i].reshape(105, -1), 'co', lw = 0.1, alpha = 0.05, label = 'noisy training data')\n",
    "\n",
    "l2, = plt.plot(true_signal(x), 'm', label = 'hidden true signal')\n",
    "plt.legend(handles = [l1, l2], loc = 'lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic_rnn_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from build_model_basic import * \n",
    "\n",
    "## Parameters\n",
    "learning_rate = 0.01\n",
    "lambda_l2_reg = 0.003  \n",
    "\n",
    "## Network Parameters\n",
    "# length of input signals\n",
    "input_seq_len = 15 \n",
    "# length of output signals\n",
    "output_seq_len = 20 \n",
    "# size of LSTM Cell\n",
    "hidden_dim = 64 \n",
    "# num of input signals\n",
    "input_dim = 1\n",
    "# num of output signals\n",
    "output_dim = 1\n",
    "# num of stacked lstm layers \n",
    "num_stacked_layers = 2 \n",
    "# gradient clipping - to avoid gradient exploding\n",
    "GRADIENT_CLIPPING = 2.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_iteractions = 100\n",
    "batch_size = 16\n",
    "KEEP_RATE = 0.5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "x = np.linspace(0, 30, 105)\n",
    "train_data_x = x[:85]\n",
    "\n",
    "rnn_model = build_graph(feed_previous=False)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(total_iteractions):\n",
    "        batch_input, batch_output = generate_train_samples(batch_size=batch_size)\n",
    "        \n",
    "        feed_dict = {rnn_model['enc_inp'][t]: batch_input[:,t].reshape(-1,input_dim) for t in range(input_seq_len)}\n",
    "        feed_dict.update({rnn_model['target_seq'][t]: batch_output[:,t].reshape(-1,output_dim) for t in range(output_seq_len)})\n",
    "        _, loss_t = sess.run([rnn_model['train_op'], rnn_model['loss']], feed_dict)\n",
    "        print(loss_t)\n",
    "        \n",
    "    temp_saver = rnn_model['saver']()\n",
    "    save_path = temp_saver.save(sess, os.path.join('./', 'univariate_ts_model0'))\n",
    "        \n",
    "print(\"Checkpoint saved at: \", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_seq_input = true_signal(train_data_x[-15:])\n",
    "\n",
    "rnn_model = build_graph(feed_previous=True)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver = rnn_model['saver']().restore(sess, os.path.join('./', 'univariate_ts_model0'))\n",
    "    \n",
    "    feed_dict = {rnn_model['enc_inp'][t]: test_seq_input[t].reshape(1,1) for t in range(input_seq_len)}\n",
    "    feed_dict.update({rnn_model['target_seq'][t]: np.zeros([1, output_dim]) for t in range(output_seq_len)})\n",
    "    final_preds = sess.run(rnn_model['reshaped_outputs'], feed_dict)\n",
    "    \n",
    "    final_preds = np.concatenate(final_preds, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the last 20 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1, = plt.plot(range(85), true_signal(train_data_x[:85]), label = 'Training truth')\n",
    "l2, = plt.plot(range(85, 105), y[85:], 'yo', label = 'Test truth')\n",
    "l3, = plt.plot(range(85, 105), final_preds.reshape(-1), 'ro', label = 'Test predictions')\n",
    "plt.legend(handles = [l1, l2, l3], loc = 'lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='session2'></a>\n",
    "# Multivariate time series: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 40, 130)\n",
    "x1 = 2 * np.sin(x)\n",
    "x2 = 2 * np.cos(x)\n",
    "\n",
    "y1 = 1.6*x1**4 - 2*x2 - 10\n",
    "y2 = 1.2*x2**2*x1 + 6*x2 - 6*x1\n",
    "y3 = 2*x1**3 + 2*x2**3 - x1*x2\n",
    "\n",
    "plt.title(\"Ground truth for 3 input signals\")\n",
    "l1, = plt.plot(y1, 'r', label = 'signal 1')\n",
    "l2, = plt.plot(y2, 'g', label = 'signal 2')\n",
    "l3, = plt.plot(y3, 'b', label = 'signal 3')\n",
    "\n",
    "plt.legend(handles = [l1, l2, l3], loc = 'upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both input and output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 40, 130)\n",
    "x1 = 2 * np.sin(x)\n",
    "x2 = 2 * np.cos(x)\n",
    "\n",
    "y1 = 1.6*x1**4 - 2*x2 - 10\n",
    "y2 = 1.2*x2**2 * x1 + 2*x2*3 - x1*6\n",
    "y3 = 2*x1**3 + 2*x2**3 - x1*x2\n",
    "\n",
    "plt.title(\"Discover hidden signals from observed facts\")\n",
    "l1, = plt.plot(y1, 'c:', label = 'input signals (observed facts)')\n",
    "plt.plot(y2, 'c:')\n",
    "plt.plot(y3, 'c:')\n",
    "\n",
    "l4, = plt.plot(2 * x1, 'm', label = 'output signals (hidden signals)')\n",
    "plt.plot(2 * x2, 'm') # multiplies by 2 just for visualization purpose\n",
    "\n",
    "plt.legend(handles = [l1, l4], loc = 'upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function that generates training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_seq_len = 15\n",
    "output_seq_len = 20\n",
    "x = np.linspace(0, 40, 130)\n",
    "train_data_x = x[:110]\n",
    "\n",
    "def true_output_signals(x):\n",
    "    x1 = 2 * np.sin(x)\n",
    "    x2 = 2 * np.cos(x)\n",
    "    return x1, x2\n",
    "\n",
    "def true_input_signals(x):\n",
    "    x1, x2 = true_output_signals(x)\n",
    "    y1 = 1.6*x1**4 - 2*x2 - 10\n",
    "    y2 = 1.2*x2**2 * x1 + 2*x2*3 - x1*6\n",
    "    y3 = 2*x1**3 + 2*x2**3 - x1*x2\n",
    "    return y1, y2, y3\n",
    "\n",
    "def noise_func(x, noise_factor = 2):\n",
    "    return np.random.randn(len(x)) * noise_factor\n",
    "\n",
    "def generate_samples_for_output(x):\n",
    "    x1, x2 = true_output_signals(x)\n",
    "    return x1+noise_func(x1, 0.5), \\\n",
    "           x2+noise_func(x2, 0.5)\n",
    "\n",
    "def generate_samples_for_input(x):\n",
    "    y1, y2, y3 = true_input_signals(x)\n",
    "    return y1+noise_func(y1, 2), \\\n",
    "           y2+noise_func(y2, 2), \\\n",
    "           y3+noise_func(y3, 2)\n",
    "\n",
    "def generate_train_samples(x = train_data_x, batch_size = 10):\n",
    "\n",
    "    total_start_points = len(x) - input_seq_len - output_seq_len\n",
    "    start_x_idx = np.random.choice(range(total_start_points), batch_size)\n",
    "    \n",
    "    input_seq_x = [x[i:(i+input_seq_len)] for i in start_x_idx]\n",
    "    output_seq_x = [x[(i+input_seq_len):(i+input_seq_len+output_seq_len)] for i in start_x_idx]\n",
    "    \n",
    "    input_seq_y = [generate_samples_for_input(x) for x in input_seq_x]\n",
    "    output_seq_y = [generate_samples_for_output(x) for x in output_seq_x]\n",
    "    \n",
    "    ## return shape: (batch_size, time_steps, feature_dims)\n",
    "    return np.array(input_seq_y).transpose(0, 2, 1), np.array(output_seq_y).transpose(0, 2, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize one data sample from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_seq, output_seq = generate_train_samples(batch_size=100)\n",
    "\n",
    "i1, i2, i3= plt.plot(range(input_seq_len), input_seq[0], 'yo', label = 'input_seqs_one_sample')\n",
    "o1, o2 = plt.plot(range(input_seq_len,(input_seq_len+output_seq_len)), output_seq[0], 'go', label = 'output_seqs_one_sample')\n",
    "plt.legend(handles = [i1, o1])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from build_model_multi_variate import * \n",
    "\n",
    "## Parameters\n",
    "learning_rate = 0.01\n",
    "lambda_l2_reg = 0.003  \n",
    "\n",
    "## Network Parameters\n",
    "# length of input signals\n",
    "input_seq_len = 15 \n",
    "# length of output signals\n",
    "output_seq_len = 20 \n",
    "# size of LSTM Cell\n",
    "hidden_dim = 64 \n",
    "# num of input signals\n",
    "input_dim = 3 \n",
    "# num of output signals\n",
    "output_dim = 2 \n",
    "# num of stacked lstm layers \n",
    "num_stacked_layers = 2 \n",
    "# gradient clipping - to avoid gradient exploding\n",
    "GRADIENT_CLIPPING = 2.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_iteractions = 100\n",
    "batch_size = 16\n",
    "KEEP_RATE = 0.5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "x = np.linspace(0, 40, 130)\n",
    "train_data_x = x[:110]\n",
    "\n",
    "rnn_model = build_graph(feed_previous=False)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    print(\"Training losses: \")\n",
    "    for i in range(total_iteractions):\n",
    "        batch_input, batch_output = generate_train_samples(batch_size=batch_size)\n",
    "        \n",
    "        feed_dict = {rnn_model['enc_inp'][t]: batch_input[:,t] for t in range(input_seq_len)}\n",
    "        feed_dict.update({rnn_model['target_seq'][t]: batch_output[:,t] for t in range(output_seq_len)})\n",
    "        _, loss_t = sess.run([rnn_model['train_op'], rnn_model['loss']], feed_dict)\n",
    "        print(loss_t)\n",
    "        \n",
    "    temp_saver = rnn_model['saver']()\n",
    "    save_path = temp_saver.save(sess, os.path.join('./', 'multivariate_ts_model0'))\n",
    "        \n",
    "print(\"Checkpoint saved at: \", save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_seq_input = np.array(generate_samples_for_input(train_data_x[-15:])).transpose(1,0)\n",
    "\n",
    "rnn_model = build_graph(feed_previous=True)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver = rnn_model['saver']().restore(sess, os.path.join('./', 'multivariate_ts_model0'))\n",
    "    \n",
    "    feed_dict = {rnn_model['enc_inp'][t]: test_seq_input[t].reshape(1, -1) for t in range(input_seq_len)}\n",
    "    feed_dict.update({rnn_model['target_seq'][t]: np.zeros([1, output_dim], dtype=np.float32) for t in range(output_seq_len)})\n",
    "    final_preds = sess.run(rnn_model['reshaped_outputs'], feed_dict)\n",
    "    \n",
    "    final_preds = np.concatenate(final_preds, axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize predictions over true signals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_seq_input = np.array(generate_samples_for_input(train_data_x[-15:])).transpose(1,0)\n",
    "test_seq_output = np.array(generate_samples_for_output(train_data_x[-20:])).transpose(1,0)\n",
    "plt.title(\"Input sequence, predicted and true output sequences\")\n",
    "i1, i2, i3, = plt.plot(range(15), np.array(true_input_signals(x[95:110])).transpose(1, 0), 'c:', label = 'true input sequence')\n",
    "p1, p2 = plt.plot(range(15, 35), 4 * final_preds, 'ro', label = 'predicted outputs')\n",
    "t1, t2 = plt.plot(range(15, 35), 4 * np.array(true_output_signals(x[110:])).transpose(1, 0), 'co', alpha = 0.6, label = 'true outputs')\n",
    "plt.legend(handles = [i1, p1, t1], loc = 'upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_seq_input = np.array(generate_samples_for_input(train_data_x[-15:])).transpose(1,0)\n",
    "test_seq_output = np.array(generate_samples_for_output(train_data_x[-20:])).transpose(1,0)\n",
    "plt.title(\"Predicted and true output sequences\")\n",
    "#i1, i2, i3, = plt.plot(range(15), np.array(true_input_signals(x[95:110])).transpose(1, 0), 'c:', label = 'true input sequence')\n",
    "p1, p2 = plt.plot(range(15, 35), final_preds, 'ro', label = 'predicted outputs')\n",
    "t1, t2 = plt.plot(range(15, 35), np.array(true_output_signals(x[110:])).transpose(1, 0), 'co', label = 'true outputs')\n",
    "plt.legend(handles = [p1, t1], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='session3'></a>\n",
    "# Extreme events / outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 60, 210)\n",
    "y = 2 * np.sin(x)\n",
    "num_events_train = 40\n",
    "num_events_test = 1\n",
    "extreme_factor = 2\n",
    "np.random.seed(10)\n",
    "train_events_x = np.random.choice(range(190), num_events_train)\n",
    "test_events_x = np.random.choice(range(190, 210), num_events_test)\n",
    "y[train_events_x] += extreme_factor\n",
    "y[test_events_x] += extreme_factor\n",
    "\n",
    "plt.title('No. of traffic v.s. days')\n",
    "\n",
    "l1, = plt.plot(x[:190], y[:190], 'y', label = 'training true signal')\n",
    "l2, = plt.plot(x[190:], y[190:], 'c--', label = 'test true signal')\n",
    "\n",
    "plt.legend(handles = [l1, l2], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_seq_len = 15\n",
    "output_seq_len = 20\n",
    "extreme_factor = 2 # additional units of traffic increase if extreme / outlier\n",
    "\n",
    "x = np.linspace(0, 60, 210)\n",
    "train_data_x = x[:190]\n",
    "\n",
    "np.random.seed(10)\n",
    "num_events_train = 40 # total number of extreme events in training data\n",
    "train_events_bool = np.zeros(190)\n",
    "train_events_x = np.random.choice(range(190), num_events_train)\n",
    "train_events_bool[train_events_x] = 1\n",
    "\n",
    "num_events_test = 1 # total number of extreme events in test data \n",
    "test_events_bool = np.zeros(20)\n",
    "test_events_x = np.random.choice(range(20), num_events_test)\n",
    "test_events_bool[test_events_x] = 1.\n",
    "                    \n",
    "def true_signal(x):\n",
    "    y = 2 * np.sin(x)\n",
    "    return y\n",
    "\n",
    "def noise_func(x, noise_factor = 1):\n",
    "    return np.random.randn(len(x)) * noise_factor\n",
    "\n",
    "def generate_y_values(x, event_bool):\n",
    "    return true_signal(x) + noise_func(x) + extreme_factor * event_bool\n",
    "\n",
    "def generate_train_samples(x = train_data_x, batch_size = 10, input_seq_len = input_seq_len, output_seq_len = output_seq_len, train_events_bool = train_events_bool):\n",
    "\n",
    "    total_start_points = len(x) - input_seq_len - output_seq_len\n",
    "    start_x_idx = np.random.choice(range(total_start_points), batch_size)\n",
    "    \n",
    "    input_seq_x = [x[i:(i+input_seq_len)] for i in start_x_idx]\n",
    "    input_seq_event_bool = [train_events_bool[i:(i+input_seq_len)] for i in start_x_idx]\n",
    "    \n",
    "    output_seq_x = [x[(i+input_seq_len):(i+input_seq_len+output_seq_len)] for i in start_x_idx]\n",
    "    output_seq_event_bool = [train_events_bool[(i+input_seq_len):(i+input_seq_len+output_seq_len)] for i in start_x_idx]\n",
    "    \n",
    "    input_seq_y = [generate_y_values(x, event_bool) for x, event_bool in zip(input_seq_x, input_seq_event_bool)]\n",
    "    output_seq_y = [generate_y_values(x, event_bool) for x, event_bool in zip(output_seq_x, output_seq_event_bool)]\n",
    "    \n",
    "    #batch_x = np.array([[true_signal()]])\n",
    "    return np.array(input_seq_y), np.array(output_seq_y), np.array(input_seq_event_bool), np.array(output_seq_event_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_seq, output_seq, input_seq_event_bool, output_seq_event_bool = generate_train_samples(batch_size=10)\n",
    "l1, = plt.plot(range(15), input_seq[0], 'go', label = 'input sequence for one sample')\n",
    "l2, = plt.plot(range(15, 35), output_seq[0], 'ro', label = 'output sequence for one sample')\n",
    "plt.legend(handles = [l1, l2], loc = 'lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Using the basic univariate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from build_model_basic import * \n",
    "\n",
    "## Parameters\n",
    "learning_rate = 0.01\n",
    "lambda_l2_reg = 0.003  \n",
    "\n",
    "## Network Parameters\n",
    "# length of input signals\n",
    "input_seq_len = 15 \n",
    "# length of output signals\n",
    "output_seq_len = 20 \n",
    "# size of LSTM Cell\n",
    "hidden_dim = 64 \n",
    "# num of input signals\n",
    "input_dim = 1\n",
    "# num of output signals\n",
    "output_dim = 1\n",
    "# num of stacked lstm layers \n",
    "num_stacked_layers = 2 \n",
    "# gradient clipping - to avoid gradient exploding\n",
    "GRADIENT_CLIPPING = 2.5 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_iteractions = 200\n",
    "batch_size = 16\n",
    "KEEP_RATE = 0.5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "x = np.linspace(0, 60, 210)\n",
    "train_data_x = x[:190]\n",
    "\n",
    "rnn_model = build_graph(feed_previous=False)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(total_iteractions):\n",
    "        batch_input, batch_output, input_seq_event_bool, output_seq_event_bool = generate_train_samples(batch_size=batch_size)\n",
    "        \n",
    "        feed_dict = {rnn_model['enc_inp'][t]: batch_input[:,t].reshape(-1,input_dim) for t in range(input_seq_len)}\n",
    "        feed_dict.update({rnn_model['target_seq'][t]: batch_output[:,t].reshape(-1,output_dim) for t in range(output_seq_len)})\n",
    "        _, loss_t = sess.run([rnn_model['train_op'], rnn_model['loss']], feed_dict)\n",
    "        print(loss_t)\n",
    "        \n",
    "    temp_saver = rnn_model['saver']()\n",
    "    save_path = temp_saver.save(sess, os.path.join('./', 'univariate_ts_model0'))\n",
    "        \n",
    "print(\"Checkpoint saved at: \", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_seq_input = true_signal(train_data_x[-15:])\n",
    "\n",
    "rnn_model = build_graph(feed_previous=True)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver = rnn_model['saver']().restore(sess, os.path.join('./', 'univariate_ts_model0'))\n",
    "    \n",
    "    feed_dict = {rnn_model['enc_inp'][t]: test_seq_input[t].reshape(1,1) for t in range(input_seq_len)}\n",
    "    feed_dict.update({rnn_model['target_seq'][t]: np.zeros([1, output_dim]) for t in range(output_seq_len)})\n",
    "    final_preds = sess.run(rnn_model['reshaped_outputs'], feed_dict)\n",
    "    \n",
    "    final_preds = np.concatenate(final_preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#l1, = plt.plot(range(190), true_signal(train_data_x[:190]), label = 'Training truth')\n",
    "l2, = plt.plot(range(190, 210), y[190:], 'yo', label = 'Test truth')\n",
    "l3, = plt.plot(range(190, 210), final_preds.reshape(-1), 'ro', label = 'Test predictions')\n",
    "plt.legend(handles = [l2, l3], loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Using seq2seq with extreme events input to decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from build_model_with_outliers import * \n",
    "\n",
    "## Parameters\n",
    "learning_rate = 0.01\n",
    "lambda_l2_reg = 0.003  \n",
    "\n",
    "## Network Parameters\n",
    "# length of input signals\n",
    "input_seq_len = 15 \n",
    "# length of output signals\n",
    "output_seq_len = 20 \n",
    "# size of LSTM Cell\n",
    "hidden_dim = 64 \n",
    "# num of input signals\n",
    "input_dim = 1\n",
    "# num of output signals\n",
    "output_dim = 1\n",
    "# num of stacked lstm layers \n",
    "num_stacked_layers = 2 \n",
    "# gradient clipping - to avoid gradient exploding\n",
    "GRADIENT_CLIPPING = 2.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_iteractions = 200\n",
    "batch_size = 16\n",
    "KEEP_RATE = 0.5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "x = np.linspace(0, 60, 210)\n",
    "train_data_x = x[:190]\n",
    "\n",
    "rnn_model = build_graph(feed_previous=False)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(total_iteractions):\n",
    "        batch_input, batch_output, batch_in_event_bool, batch_out_event_bool = generate_train_samples(batch_size=batch_size)\n",
    "        \n",
    "        feed_dict = {rnn_model['enc_inp'][t]: batch_input[:,t].reshape(-1,input_dim) for t in range(input_seq_len)}\n",
    "        feed_dict.update({rnn_model['target_seq'][t]: batch_output[:,t].reshape(-1,output_dim) for t in range(output_seq_len)})\n",
    "        #feed_dict.update({rnn_model['input_seq_extremes_bool'][t]: batch_in_event_bool[:,t].reshape(-1,1) for t in range(input_seq_len)})\n",
    "        feed_dict.update({rnn_model['output_seq_extremes_bool'][t]: batch_out_event_bool[:,t].reshape(-1,1) for t in range(output_seq_len)})\n",
    "        _, loss_t = sess.run([rnn_model['train_op'], rnn_model['loss']], feed_dict)\n",
    "        print(loss_t)\n",
    "        \n",
    "    temp_saver = rnn_model['saver']()\n",
    "    save_path = temp_saver.save(sess, os.path.join('./', 'univariate_ts_model_eventsBool'))\n",
    "        \n",
    "print(\"Checkpoint saved at: \", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_seq_input = true_signal(train_data_x[-15:])\n",
    "test_events_bool_input = train_events_bool[-15:]\n",
    "test_events_bool_output = test_events_bool.copy()\n",
    "\n",
    "rnn_model = build_graph(feed_previous=True)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver = rnn_model['saver']().restore(sess, os.path.join('./', 'univariate_ts_model_eventsBool'))\n",
    "    \n",
    "    feed_dict = {rnn_model['enc_inp'][t]: test_seq_input[t].reshape(1,1) for t in range(input_seq_len)}\n",
    "    feed_dict.update({rnn_model['target_seq'][t]: np.zeros([1, 1]) for t in range(output_seq_len)})\n",
    "    feed_dict.update({rnn_model['output_seq_extremes_bool'][t]: test_events_bool_output[t].reshape(1,1) for t in range(output_seq_len)})\n",
    "    final_preds = sess.run(rnn_model['reshaped_outputs'], feed_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_preds  = np.array(final_preds).reshape(-1)\n",
    "\n",
    "l2, = plt.plot(range(190, 210), y[190:], 'yo', label = 'Test truth')\n",
    "l3, = plt.plot(range(190, 210), final_preds.reshape(-1), 'ro', label = 'Test predictions')\n",
    "plt.legend(handles = [l2, l3], loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='session4'></a>\n",
    "# Real-world case - Beijing PM2.5 Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set can be downloaded from UCI website - https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data\n",
    "\n",
    "The remaining session will use this dataset as an example to demonstrate on how to apply seq2seq model to solve a real world problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('./PRSA_data_2010.1.1-2014.12.31.csv')\n",
    "print(df.head())\n",
    "\n",
    "cols_to_plot = [\"pm2.5\", \"DEWP\", \"TEMP\", \"PRES\", \"Iws\", \"Is\", \"Ir\"]\n",
    "i = 1\n",
    "# plot each column\n",
    "plt.figure(figsize = (10,12))\n",
    "for col in cols_to_plot:\n",
    "    plt.subplot(len(cols_to_plot), 1, i)\n",
    "    plt.plot(df[col])\n",
    "    plt.title(col, y=0.5, loc='left')\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - there are many other ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Fill NA with 0 \n",
    "#print(df.isnull().sum())\n",
    "df.fillna(0, inplace = True)\n",
    "\n",
    "## One-hot encode 'cbwd'\n",
    "temp = pd.get_dummies(df['cbwd'], prefix='cbwd')\n",
    "df = pd.concat([df, temp], axis = 1)\n",
    "del df['cbwd'], temp\n",
    "\n",
    "## Split into train and test - I used the last 1 month data as test, but it's up to you to decide the ratio\n",
    "df_train = df.iloc[:(-31*24), :].copy()\n",
    "df_test = df.iloc[-31*24:, :].copy()\n",
    "\n",
    "## take out the useful columns for modeling - you may also keep 'hour', 'day' or 'month' and to see if that will improve your accuracy\n",
    "X_train = df_train.loc[:, ['pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NE', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv']].values.copy()\n",
    "X_test = df_test.loc[:, ['pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NE', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv']].values.copy()\n",
    "y_train = df_train['pm2.5'].values.copy().reshape(-1, 1)\n",
    "y_test = df_test['pm2.5'].values.copy().reshape(-1, 1)\n",
    "\n",
    "## z-score transform x - not including those one-how columns!\n",
    "for i in range(X_train.shape[1]-4):\n",
    "    temp_mean = X_train[:, i].mean()\n",
    "    temp_std = X_train[:, i].std()\n",
    "    X_train[:, i] = (X_train[:, i] - temp_mean) / temp_std\n",
    "    X_test[:, i] = (X_test[:, i] - temp_mean) / temp_std\n",
    "    \n",
    "## z-score transform y\n",
    "y_mean = y_train.mean()\n",
    "y_std = y_train.std()\n",
    "y_train = (y_train - y_mean) / y_std\n",
    "y_test = (y_test - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training and test datasets in 3-D format - (batch_size, time_step, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_seq_len = 30\n",
    "output_seq_len = 5\n",
    "\n",
    "def generate_train_samples(x = X_train, y = y_train, batch_size = 10, input_seq_len = input_seq_len, output_seq_len = output_seq_len):\n",
    "\n",
    "    total_start_points = len(x) - input_seq_len - output_seq_len\n",
    "    start_x_idx = np.random.choice(range(total_start_points), batch_size, replace = False)\n",
    "    \n",
    "    input_batch_idxs = [list(range(i, i+input_seq_len)) for i in start_x_idx]\n",
    "    input_seq = np.take(x, input_batch_idxs, axis = 0)\n",
    "    \n",
    "    output_batch_idxs = [list(range(i+input_seq_len, i+input_seq_len+output_seq_len)) for i in start_x_idx]\n",
    "    output_seq = np.take(y, output_batch_idxs, axis = 0)\n",
    "    \n",
    "    return input_seq, output_seq # in shape: (batch_size, time_steps, feature_dim)\n",
    "\n",
    "def generate_test_samples(x = X_test, y = y_test, input_seq_len = input_seq_len, output_seq_len = output_seq_len):\n",
    "    \n",
    "    total_samples = x.shape[0]\n",
    "    \n",
    "    input_batch_idxs = [list(range(i, i+input_seq_len)) for i in range((total_samples-input_seq_len-output_seq_len))]\n",
    "    input_seq = np.take(x, input_batch_idxs, axis = 0)\n",
    "    \n",
    "    output_batch_idxs = [list(range(i+input_seq_len, i+input_seq_len+output_seq_len)) for i in range((total_samples-input_seq_len-output_seq_len))]\n",
    "    output_seq = np.take(y, output_batch_idxs, axis = 0)\n",
    "    \n",
    "    return input_seq, output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = generate_train_samples()\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_x, test_y = generate_test_samples()\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model - same multi-variate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.framework import dtypes\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "import os\n",
    "\n",
    "## Parameters\n",
    "learning_rate = 0.01\n",
    "lambda_l2_reg = 0.003  \n",
    "\n",
    "## Network Parameters\n",
    "# length of input signals\n",
    "input_seq_len = input_seq_len\n",
    "# length of output signals\n",
    "output_seq_len = output_seq_len\n",
    "# size of LSTM Cell\n",
    "hidden_dim = 64 \n",
    "# num of input signals\n",
    "input_dim = X_train.shape[1]\n",
    "# num of output signals\n",
    "output_dim = y_train.shape[1]\n",
    "# num of stacked lstm layers \n",
    "num_stacked_layers = 2 \n",
    "# gradient clipping - to avoid gradient exploding\n",
    "GRADIENT_CLIPPING = 2.5 \n",
    "\n",
    "def build_graph(feed_previous = False):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    global_step = tf.Variable(\n",
    "                  initial_value=0,\n",
    "                  name=\"global_step\",\n",
    "                  trainable=False,\n",
    "                  collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "    \n",
    "    weights = {\n",
    "        'out': tf.get_variable('Weights_out', \\\n",
    "                               shape = [hidden_dim, output_dim], \\\n",
    "                               dtype = tf.float32, \\\n",
    "                               initializer = tf.truncated_normal_initializer()),\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.get_variable('Biases_out', \\\n",
    "                               shape = [output_dim], \\\n",
    "                               dtype = tf.float32, \\\n",
    "                               initializer = tf.constant_initializer(0.)),\n",
    "    }\n",
    "                                          \n",
    "    with tf.variable_scope('Seq2seq'):\n",
    "        # Encoder: inputs\n",
    "        enc_inp = [\n",
    "            tf.placeholder(tf.float32, shape=(None, input_dim), name=\"inp_{}\".format(t))\n",
    "               for t in range(input_seq_len)\n",
    "        ]\n",
    "\n",
    "        # Decoder: target outputs\n",
    "        target_seq = [\n",
    "            tf.placeholder(tf.float32, shape=(None, output_dim), name=\"y\".format(t))\n",
    "              for t in range(output_seq_len)\n",
    "        ]\n",
    "\n",
    "        # Give a \"GO\" token to the decoder. \n",
    "        # If dec_inp are fed into decoder as inputs, this is 'guided' training; otherwise only the \n",
    "        # first element will be fed as decoder input which is then 'un-guided'\n",
    "        dec_inp = [ tf.zeros_like(target_seq[0], dtype=tf.float32, name=\"GO\") ] + target_seq[:-1]\n",
    "\n",
    "        with tf.variable_scope('LSTMCell'): \n",
    "            cells = []\n",
    "            for i in range(num_stacked_layers):\n",
    "                with tf.variable_scope('RNN_{}'.format(i)):\n",
    "                    cells.append(tf.contrib.rnn.LSTMCell(hidden_dim))\n",
    "            cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "         \n",
    "        def _rnn_decoder(decoder_inputs,\n",
    "                        initial_state,\n",
    "                        cell,\n",
    "                        loop_function=None,\n",
    "                        scope=None):\n",
    "          \"\"\"RNN decoder for the sequence-to-sequence model.\n",
    "          Args:\n",
    "            decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
    "            initial_state: 2D Tensor with shape [batch_size x cell.state_size].\n",
    "            cell: rnn_cell.RNNCell defining the cell function and size.\n",
    "            loop_function: If not None, this function will be applied to the i-th output\n",
    "              in order to generate the i+1-st input, and decoder_inputs will be ignored,\n",
    "              except for the first element (\"GO\" symbol). This can be used for decoding,\n",
    "              but also for training to emulate http://arxiv.org/abs/1506.03099.\n",
    "              Signature -- loop_function(prev, i) = next\n",
    "                * prev is a 2D Tensor of shape [batch_size x output_size],\n",
    "                * i is an integer, the step number (when advanced control is needed),\n",
    "                * next is a 2D Tensor of shape [batch_size x input_size].\n",
    "            scope: VariableScope for the created subgraph; defaults to \"rnn_decoder\".\n",
    "          Returns:\n",
    "            A tuple of the form (outputs, state), where:\n",
    "              outputs: A list of the same length as decoder_inputs of 2D Tensors with\n",
    "                shape [batch_size x output_size] containing generated outputs.\n",
    "              state: The state of each cell at the final time-step.\n",
    "                It is a 2D Tensor of shape [batch_size x cell.state_size].\n",
    "                (Note that in some cases, like basic RNN cell or GRU cell, outputs and\n",
    "                 states can be the same. They are different for LSTM cells though.)\n",
    "          \"\"\"\n",
    "          with variable_scope.variable_scope(scope or \"rnn_decoder\"):\n",
    "            state = initial_state\n",
    "            outputs = []\n",
    "            prev = None\n",
    "            for i, inp in enumerate(decoder_inputs):\n",
    "              if loop_function is not None and prev is not None:\n",
    "                with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "                  inp = loop_function(prev, i)\n",
    "              if i > 0:\n",
    "                variable_scope.get_variable_scope().reuse_variables()\n",
    "              output, state = cell(inp, state)\n",
    "              outputs.append(output)\n",
    "              if loop_function is not None:\n",
    "                prev = output\n",
    "          return outputs, state\n",
    "\n",
    "        def _basic_rnn_seq2seq(encoder_inputs,\n",
    "                              decoder_inputs,\n",
    "                              cell,\n",
    "                              feed_previous,\n",
    "                              dtype=dtypes.float32,\n",
    "                              scope=None):\n",
    "          \"\"\"Basic RNN sequence-to-sequence model.\n",
    "          This model first runs an RNN to encode encoder_inputs into a state vector,\n",
    "          then runs decoder, initialized with the last encoder state, on decoder_inputs.\n",
    "          Encoder and decoder use the same RNN cell type, but don't share parameters.\n",
    "          Args:\n",
    "            encoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
    "            decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n",
    "            feed_previous: Boolean; if True, only the first of decoder_inputs will be\n",
    "              used (the \"GO\" symbol), all other inputs will be generated by the previous \n",
    "              decoder output using _loop_function below. If False, decoder_inputs are used \n",
    "              as given (the standard decoder case).\n",
    "            dtype: The dtype of the initial state of the RNN cell (default: tf.float32).\n",
    "            scope: VariableScope for the created subgraph; default: \"basic_rnn_seq2seq\".\n",
    "          Returns:\n",
    "            A tuple of the form (outputs, state), where:\n",
    "              outputs: A list of the same length as decoder_inputs of 2D Tensors with\n",
    "                shape [batch_size x output_size] containing the generated outputs.\n",
    "              state: The state of each decoder cell in the final time-step.\n",
    "                It is a 2D Tensor of shape [batch_size x cell.state_size].\n",
    "          \"\"\"\n",
    "          with variable_scope.variable_scope(scope or \"basic_rnn_seq2seq\"):\n",
    "            enc_cell = copy.deepcopy(cell)\n",
    "            _, enc_state = rnn.static_rnn(enc_cell, encoder_inputs, dtype=dtype)\n",
    "            if feed_previous:\n",
    "                return _rnn_decoder(decoder_inputs, enc_state, cell, _loop_function)\n",
    "            else:\n",
    "                return _rnn_decoder(decoder_inputs, enc_state, cell)\n",
    "\n",
    "        def _loop_function(prev, _):\n",
    "          '''Naive implementation of loop function for _rnn_decoder. Transform prev from \n",
    "          dimension [batch_size x hidden_dim] to [batch_size x output_dim], which will be\n",
    "          used as decoder input of next time step '''\n",
    "          return tf.matmul(prev, weights['out']) + biases['out']\n",
    "        \n",
    "        dec_outputs, dec_memory = _basic_rnn_seq2seq(\n",
    "            enc_inp, \n",
    "            dec_inp, \n",
    "            cell, \n",
    "            feed_previous = feed_previous\n",
    "        )\n",
    "\n",
    "        reshaped_outputs = [tf.matmul(i, weights['out']) + biases['out'] for i in dec_outputs]\n",
    "        \n",
    "    # Training loss and optimizer\n",
    "    with tf.variable_scope('Loss'):\n",
    "        # L2 loss\n",
    "        output_loss = 0\n",
    "        for _y, _Y in zip(reshaped_outputs, target_seq):\n",
    "            output_loss += tf.reduce_mean(tf.pow(_y - _Y, 2))\n",
    "\n",
    "        # L2 regularization for weights and biases\n",
    "        reg_loss = 0\n",
    "        for tf_var in tf.trainable_variables():\n",
    "            if 'Biases_' in tf_var.name or 'Weights_' in tf_var.name:\n",
    "                reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "\n",
    "        loss = output_loss + lambda_l2_reg * reg_loss\n",
    "\n",
    "    with tf.variable_scope('Optimizer'):\n",
    "        optimizer = tf.contrib.layers.optimize_loss(\n",
    "                loss=loss,\n",
    "                learning_rate=learning_rate,\n",
    "                global_step=global_step,\n",
    "                optimizer='Adam',\n",
    "                clip_gradients=GRADIENT_CLIPPING)\n",
    "        \n",
    "    saver = tf.train.Saver\n",
    "    \n",
    "    return dict(\n",
    "        enc_inp = enc_inp, \n",
    "        target_seq = target_seq, \n",
    "        train_op = optimizer, \n",
    "        loss=loss,\n",
    "        saver = saver, \n",
    "        reshaped_outputs = reshaped_outputs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_iteractions = 100\n",
    "batch_size = 16\n",
    "KEEP_RATE = 0.5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "x = np.linspace(0, 40, 130)\n",
    "train_data_x = x[:110]\n",
    "\n",
    "rnn_model = build_graph(feed_previous=False)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    print(\"Training losses: \")\n",
    "    for i in range(total_iteractions):\n",
    "        batch_input, batch_output = generate_train_samples(batch_size=batch_size)\n",
    "        \n",
    "        feed_dict = {rnn_model['enc_inp'][t]: batch_input[:,t] for t in range(input_seq_len)}\n",
    "        feed_dict.update({rnn_model['target_seq'][t]: batch_output[:,t] for t in range(output_seq_len)})\n",
    "        _, loss_t = sess.run([rnn_model['train_op'], rnn_model['loss']], feed_dict)\n",
    "        print(loss_t)\n",
    "        \n",
    "    temp_saver = rnn_model['saver']()\n",
    "    save_path = temp_saver.save(sess, os.path.join('./', 'multivariate_ts_pollution_case'))\n",
    "        \n",
    "print(\"Checkpoint saved at: \", save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on test \n",
    "Notice the batch prediction which is different to previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_model = build_graph(feed_previous=True)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver = rnn_model['saver']().restore(sess,  os.path.join('./', 'multivariate_ts_pollution_case'))\n",
    "    \n",
    "    feed_dict = {rnn_model['enc_inp'][t]: test_x[:, t, :] for t in range(input_seq_len)} # batch prediction\n",
    "    feed_dict.update({rnn_model['target_seq'][t]: np.zeros([test_x.shape[0], output_dim], dtype=np.float32) for t in range(output_seq_len)})\n",
    "    final_preds = sess.run(rnn_model['reshaped_outputs'], feed_dict)\n",
    "    \n",
    "    final_preds = [np.expand_dims(pred, 1) for pred in final_preds]\n",
    "    final_preds = np.concatenate(final_preds, axis = 1)\n",
    "    print(\"Test mse is: \", np.mean((final_preds - test_y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## remove duplicate hours and concatenate into one long array\n",
    "test_y_expand = np.concatenate([test_y[i].reshape(-1) for i in range(0, test_y.shape[0], 5)], axis = 0)\n",
    "final_preds_expand = np.concatenate([final_preds[i].reshape(-1) for i in range(0, final_preds.shape[0], 5)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(final_preds_expand, color = 'orange', label = 'predicted')\n",
    "plt.plot(test_y_expand, color = 'blue', label = 'actual')\n",
    "plt.title(\"test data - one month\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
