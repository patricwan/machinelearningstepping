{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'seg_sentences'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e70a5659b5b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseg_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'[^a-zA-Z\\u4E00-\\u9FA5]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjpype\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'seg_sentences'"
     ]
    }
   ],
   "source": [
    "#encoding=utf8\n",
    "import json\n",
    "import sys,os,re\n",
    "import numpy\n",
    "from tokenizer import seg_sentences\n",
    "pattern=re.compile(u'[^a-zA-Z\\u4E00-\\u9FA5]')\n",
    "from jpype import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "keep_pos=\"n,an,vn,nr,ns,nt,nz,nb,nba,nbc,nbp,nf,ng,nh,nhd,o,nz,nx,ntu,nts,nto,nth,ntch,ntcf,ntcb,ntc,nt,nsf,ns,nrj,nrf,nr2,nr1,nr,nnt,nnd,nn,nmc,nm,nl,nit,nis,nic,ni,nhm,nhd\"\n",
    "keep_pos_set=set(keep_pos.split(\",\"))\n",
    "stop_pos=\"q,b,f,p,qg,qt,qv,r,rg,Rg,rr,ry,rys,ryt,ryv,rz,rzs,rzt,rzv,s,v,vd,vshi,vyou,vf,vx,vl,vg,vf,vi,m,mq,uzhe,ule,uguo,ude1,ude2,ude3,usuo,udeng,uv,uzhe,uyy,udh,uls,uzhi,ulian,d,dl,u,c,cc,bl,ad,ag,al,a,r,q,p,z,pba,pbei,d,dl,o,e,xx,xu,y,yg,z,wkz,wky,wyz,wyy,wj,ww,wt,wd,wf,wm,ws,wp,wb,wh,wn,t,tg,vi,id,ip,url,tel\"\n",
    "stop_pos_set = set(stop_pos.split(','))\n",
    "stop_ch='\"是\",\"由\"'\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, numpy.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, numpy.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, numpy.ndarray):\n",
    "            return obj.tolist()        \n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def _replace_c(text):\n",
    "    intab = \",?!\"\n",
    "    outtab = \"，？！\"    \n",
    "    deltab = \")(+_-.>< \"\n",
    "    trantab=text.maketrans(intab, outtab,deltab)\n",
    "    return text.translate(trantab)\n",
    "\n",
    "def tokenize_raw(text):           # 先以标点符号为单位切分，再使用hanlp的seg_sentences分词\n",
    "    split_sen=(i.strip() for i in re.split('。|,|，|：|:|？|！|\\t|\\n',_replace_c(text)) if len(i.strip())>5)    # 这里用()而不用[] 是因为（）是生成器，有利于减小内存，如果用[]生成list的话可能会内存不足\n",
    "    return [seg_sentences(sentence) for sentence in split_sen]  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def list_2_ngram(sentence, n=4, m=2):         # n-gram\n",
    "    if len(sentence) < n:\n",
    "        n = len(sentence)\n",
    "    temp=[tuple(sentence[i - k:i]) for k in range(m, n + 1) for i in range(k, len(sentence) + 1) ]\n",
    "    return [item for item in temp if len(''.join(item).strip())>1 and len(pattern.findall(''.join(item).strip()))==0]\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    #'PlatformESB 组件 子系统 监控 NBI 接口'\n",
    "    copus=[tokenize_raw(line.strip()) for line in open('text.txt','r',encoding='utf8') if len(line.strip())>0 and \"RESUMEDOCSSTARTFLAG\" not in line]\n",
    "    #['TM_拓扑 拓扑_管理 TM_拓扑_管理']\n",
    "    doc=[]\n",
    "    if len(copus)>1: \n",
    "        for list_copus in copus:\n",
    "            for t in list_copus:\n",
    "                doc.extend([' '.join(['_'.join(i) for i in list_2_ngram(t, n=4, m=2)])])\n",
    "    doc=list(filter(None,doc))                                   # 对分词进行n-gram，然后连接\n",
    "    fout=open(\"ngram2_4.txt\", \"w\", encoding='utf-8')\n",
    "\n",
    "    # 使用tfidf计算频率\n",
    "    vectorizer1=CountVectorizer()  #初始化一个计数类\n",
    "    \n",
    "    transformer=TfidfTransformer() #该类会统计每个词语的tf-idf权值\n",
    "    freq1=vectorizer1.fit_transform(doc)  # 计算词频 65x1179\n",
    "    tfidf=transformer.fit_transform(freq1)\n",
    "    word_freq=[freq1.getcol(i).sum() for i in range(freq1.shape[1])]\n",
    "                 \n",
    "    tfidf_sum=[tfidf.getcol(i).sum() for i in range(tfidf.shape[1])]\n",
    "\n",
    "    tfidf_dic=vectorizer1.vocabulary_\n",
    "    tfidf_dic=dict(zip(tfidf_dic.values(),tfidf_dic.keys())) # 反转\n",
    "\n",
    "    dic_filter={}\n",
    "    def _add(wq,tf,i):\n",
    "        dic_filter[tfidf_dic[i]]=[wq,tf]\n",
    "    for i,(word_freq_one,w_one) in enumerate(zip(word_freq,tfidf_sum)):\n",
    "        _add(word_freq_one, w_one, i)\n",
    "    sort_dic=dict(sorted(dic_filter.items(),key=lambda val:val[1],reverse=True))#,reverse=True为降序排列,返回list\n",
    "    fout.write(json.dumps(sort_dic, ensure_ascii=False,cls=NumpyEncoder))               \n",
    "    fout.close() \n",
    "shutdownJVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z  [ 0.02364054  0.06426166  0.1746813   0.474833    0.02364054  0.06426166\n",
      "  0.1746813 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "z = np.array([1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0])\n",
    "\n",
    "def calSoftMax(arrInput):\n",
    "    result = np.exp(arrInput)/sum(np.exp(arrInput))\n",
    "    return result\n",
    "\n",
    "print(\"z \", calSoftMax(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
