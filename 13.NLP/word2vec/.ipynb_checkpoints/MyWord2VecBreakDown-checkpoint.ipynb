{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading word 1000\n",
      "Unknown vocab size: 544\n",
      "eachVocab  <unk> 752 None None\n",
      "eachVocab  的 106 None None\n",
      "eachVocab  数学 56 None None\n",
      "eachVocab  在 25 None None\n",
      "eachVocab  科学 18 None None\n",
      "eachVocab  是 16 None None\n",
      "eachVocab  <bol> 12 None None\n",
      "eachVocab  <eol> 12 None None\n",
      "eachVocab  为 11 None None\n",
      "eachVocab  和 11 None None\n",
      "eachVocab  有 11 None None\n",
      "eachVocab  亦 11 None None\n",
      "eachVocab  被 10 None None\n",
      "eachVocab  中 10 None None\n",
      "eachVocab  了 10 None None\n",
      "eachVocab  但 9 None None\n",
      "eachVocab  研究 8 None None\n",
      "eachVocab  领域 8 None None\n",
      "eachVocab  其 8 None None\n",
      "eachVocab  等 7 None None\n",
      "eachVocab  而 7 None None\n",
      "eachVocab  也 7 None None\n",
      "eachVocab  上 7 None None\n",
      "eachVocab  及 7 None None\n",
      "eachVocab  数学家 6 None None\n",
      "eachVocab  形式 6 None None\n",
      "eachVocab  使用 6 None None\n",
      "eachVocab  发展 6 None None\n",
      "eachVocab  今日 6 None None\n",
      "eachVocab  应用 6 None None\n",
      "eachVocab  许多 6 None None\n",
      "eachVocab  且 6 None None\n",
      "eachVocab  如 6 None None\n",
      "eachVocab  符号 6 None None\n",
      "eachVocab  著 6 None None\n",
      "eachVocab  地 5 None None\n",
      "eachVocab  会 5 None None\n",
      "eachVocab  纯数学 5 None None\n",
      "eachVocab  于 5 None None\n",
      "eachVocab  不 5 None None\n",
      "syn0 (40, 100)\n",
      "syn1 (40, 100)\n",
      "Initializing Huffman tree\n",
      "encode_humffman vocab_size  40\n",
      "count [752, 106, 56, 25, 18, 16, 12, 12, 11, 11, 11, 11, 10, 10, 10, 9, 8, 8, 8, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0]\n",
      "parent [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "binary  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "After huffman encoding \n",
      "parent [78, 75, 73, 68, 63, 63, 60, 60, 57, 56, 56, 55, 54, 54, 53, 52, 51, 51, 50, 50, 49, 49, 48, 48, 47, 47, 46, 46, 45, 45, 44, 44, 43, 43, 42, 42, 41, 41, 40, 40, 52, 53, 55, 57, 58, 58, 59, 59, 61, 61, 62, 62, 64, 64, 65, 65, 66, 66, 67, 67, 68, 69, 69, 70, 70, 71, 71, 72, 72, 73, 74, 74, 75, 76, 76, 77, 77, 78]\n",
      "binary  [1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0]\n",
      "self  <__main__.Vocab object at 0x7fd94433c668>\n",
      "path, code   [38] [1]\n",
      "path, code   [38, 37, 35] [0, 0, 1]\n",
      "path, code   [38, 37, 36, 33] [0, 1, 0, 0]\n",
      "path, code   [38, 37, 35, 32, 28] [0, 0, 0, 1, 1]\n",
      "path, code   [38, 37, 36, 34, 30, 23] [0, 1, 1, 0, 0, 1]\n",
      "path, code   [38, 37, 36, 34, 30, 23] [0, 1, 1, 0, 0, 0]\n",
      "path, code   [38, 37, 35, 32, 28, 20] [0, 0, 0, 1, 0, 1]\n",
      "path, code   [38, 37, 35, 32, 28, 20] [0, 0, 0, 1, 0, 0]\n",
      "path, code   [38, 37, 36, 34, 31, 26, 17] [0, 1, 1, 1, 1, 1, 0]\n",
      "path, code   [38, 37, 36, 34, 31, 26, 16] [0, 1, 1, 1, 1, 0, 1]\n",
      "path, code   [38, 37, 36, 34, 31, 26, 16] [0, 1, 1, 1, 1, 0, 0]\n",
      "path, code   [38, 37, 36, 34, 31, 25, 15] [0, 1, 1, 1, 0, 1, 1]\n",
      "path, code   [38, 37, 36, 34, 31, 25, 14] [0, 1, 1, 1, 0, 0, 1]\n",
      "path, code   [38, 37, 36, 34, 31, 25, 14] [0, 1, 1, 1, 0, 0, 0]\n",
      "path, code   [38, 37, 36, 34, 30, 24, 13] [0, 1, 1, 0, 1, 1, 1]\n",
      "path, code   [38, 37, 36, 34, 30, 24, 12] [0, 1, 1, 0, 1, 0, 0]\n",
      "path, code   [38, 37, 36, 33, 29, 22, 11] [0, 1, 0, 1, 1, 1, 1]\n",
      "path, code   [38, 37, 36, 33, 29, 22, 11] [0, 1, 0, 1, 1, 1, 0]\n",
      "path, code   [38, 37, 36, 33, 29, 22, 10] [0, 1, 0, 1, 1, 0, 1]\n",
      "path, code   [38, 37, 36, 33, 29, 22, 10] [0, 1, 0, 1, 1, 0, 0]\n",
      "path, code   [38, 37, 36, 33, 29, 21, 9] [0, 1, 0, 1, 0, 1, 1]\n",
      "path, code   [38, 37, 36, 33, 29, 21, 9] [0, 1, 0, 1, 0, 1, 0]\n",
      "path, code   [38, 37, 36, 33, 29, 21, 8] [0, 1, 0, 1, 0, 0, 1]\n",
      "path, code   [38, 37, 36, 33, 29, 21, 8] [0, 1, 0, 1, 0, 0, 0]\n",
      "path, code   [38, 37, 35, 32, 27, 19, 7] [0, 0, 0, 0, 1, 1, 1]\n",
      "path, code   [38, 37, 35, 32, 27, 19, 7] [0, 0, 0, 0, 1, 1, 0]\n",
      "path, code   [38, 37, 35, 32, 27, 19, 6] [0, 0, 0, 0, 1, 0, 1]\n",
      "path, code   [38, 37, 35, 32, 27, 19, 6] [0, 0, 0, 0, 1, 0, 0]\n",
      "path, code   [38, 37, 35, 32, 27, 18, 5] [0, 0, 0, 0, 0, 1, 1]\n",
      "path, code   [38, 37, 35, 32, 27, 18, 5] [0, 0, 0, 0, 0, 1, 0]\n",
      "path, code   [38, 37, 35, 32, 27, 18, 4] [0, 0, 0, 0, 0, 0, 1]\n",
      "path, code   [38, 37, 35, 32, 27, 18, 4] [0, 0, 0, 0, 0, 0, 0]\n",
      "path, code   [38, 37, 36, 34, 31, 26, 17, 3] [0, 1, 1, 1, 1, 1, 1, 1]\n",
      "path, code   [38, 37, 36, 34, 31, 26, 17, 3] [0, 1, 1, 1, 1, 1, 1, 0]\n",
      "path, code   [38, 37, 36, 34, 31, 25, 15, 2] [0, 1, 1, 1, 0, 1, 0, 1]\n",
      "path, code   [38, 37, 36, 34, 31, 25, 15, 2] [0, 1, 1, 1, 0, 1, 0, 0]\n",
      "path, code   [38, 37, 36, 34, 30, 24, 13, 1] [0, 1, 1, 0, 1, 1, 0, 1]\n",
      "path, code   [38, 37, 36, 34, 30, 24, 13, 1] [0, 1, 1, 0, 1, 1, 0, 0]\n",
      "path, code   [38, 37, 36, 34, 30, 24, 12, 0] [0, 1, 1, 0, 1, 0, 1, 1]\n",
      "path, code   [38, 37, 36, 34, 30, 24, 12, 0] [0, 1, 1, 0, 1, 0, 1, 0]\n",
      "Huffman encoding done\n",
      "sent  [6, 0, 0, 0, 0, 1, 0, 24, 0, 12, 0, 5, 0, 0, 0, 0, 8, 0, 1, 0, 0, 0, 2, 0, 5, 0, 0, 16, 0, 0, 0, 0, 0, 19, 0, 1, 0, 0, 0, 0, 0, 0, 25, 4, 1, 0, 2, 0, 0, 9, 0, 1, 26, 0, 0, 0, 24, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 3, 0, 20, 3, 0, 0, 10, 0, 0, 1, 0, 0, 0, 0, 2, 1, 27, 0, 0, 0, 35, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 27, 0, 28, 28, 2, 26, 3, 0, 1, 17, 13, 0, 4, 0, 0, 9, 0, 19, 0, 11, 36, 0, 0, 1, 2, 0, 0, 0, 7]\n",
      "Alpha: 0.025000 Progress: 0 of 1230 (0.00%)sent  [6, 0, 0, 1, 27, 24, 21, 16, 37, 0, 2, 0, 1, 0, 0, 20, 0, 0, 0, 29, 8, 0, 0, 30, 16, 0, 37, 0, 15, 18, 0, 13, 21, 0, 30, 29, 0, 0, 0, 0, 0, 13, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 4, 2, 16, 0, 3, 18, 0, 0, 18, 0, 0, 8, 0, 0, 0, 1, 0, 1, 11, 36, 12, 0, 0, 2, 1, 18, 3, 0, 13, 0, 22, 1, 0, 25, 23, 3, 0, 13, 1, 0, 0, 25, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 38, 0, 0, 0, 0, 0, 15, 0, 21, 0, 0, 7]\n",
      "sent  [6, 22, 1, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 21, 12, 0, 0, 0, 1, 0, 0, 0, 8, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 16, 0, 2, 0, 1, 26, 0, 0, 0, 0, 2, 0, 0, 0, 22, 1, 2, 0, 0, 0, 0, 0, 26, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 10, 2, 0, 3, 0, 10, 0, 0, 7]\n",
      "sent  [6, 0, 0, 0, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 0, 19, 0, 1, 0, 0, 0, 0, 0, 10, 0, 0, 9, 0, 19, 0, 0, 21, 0, 35, 0, 14, 0, 22, 0, 10, 0, 30, 0, 1, 0, 0, 0, 0, 3, 0, 10, 0, 1, 0, 8, 0, 0, 0, 1, 0, 0, 0, 7]\n",
      "sent  [6, 0, 0, 0, 0, 0, 0, 20, 0, 1, 0, 0, 23, 0, 0, 1, 16, 0, 14, 0, 0, 0, 1, 0, 21, 3, 0, 0, 0, 2, 0, 0, 0, 7]\n",
      "sent  [6, 0, 2, 0, 0, 0, 35, 0, 31, 0, 4, 10, 0, 1, 0, 0, 1, 27, 0, 0, 38, 0, 0, 0, 30, 2, 0, 0, 0, 28, 0, 0, 0, 0, 7]\n",
      "sent  [6, 1, 0, 0, 0, 0, 38, 0, 2, 36, 0, 0, 1, 0, 13, 0, 2, 0, 1, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7]\n",
      "sent  [6, 0, 0, 37, 0, 29, 2, 23, 0, 0, 0, 1, 0, 0, 0, 10, 0, 0, 0, 0, 23, 0, 19, 0, 1, 0, 0, 0, 20, 0, 0, 21, 0, 14, 2, 1, 16, 0, 0, 0, 2, 1, 0, 0, 38, 0, 0, 0, 23, 0, 1, 0, 28, 31, 2, 0, 11, 0, 14, 30, 1, 0, 0, 9, 0, 5, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 23, 0, 0, 0, 1, 0, 20, 28, 1, 0, 0, 11, 0, 0, 1, 2, 0, 2, 0, 9, 0, 0, 1, 17, 0, 0, 0, 0, 0, 17, 1, 0, 0, 0, 31, 0, 0, 0, 1, 2, 0, 0, 5, 0, 1, 2, 0, 11, 10, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 7]\n",
      "sent  [6, 0, 0, 0, 0, 0, 1, 16, 17, 0, 1, 0, 8, 37, 9, 29, 2, 3, 29, 2, 0, 0, 12, 0, 0, 17, 0, 0, 14, 0, 0, 1, 0, 0, 7]\n",
      "sent  [6, 0, 9, 0, 4, 30, 24, 0, 2, 1, 0, 0, 3, 1, 0, 0, 0, 0, 0, 8, 0, 0, 0, 11, 0, 0, 1, 0, 0, 0, 5, 0, 0, 1, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 3, 0, 24, 1, 0, 33, 0, 0, 0, 3, 0, 1, 33, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 1, 0, 2, 0, 0, 1, 25, 0, 0, 0, 25, 36, 0, 14, 2, 1, 27, 15, 0, 0, 0, 0, 0, 0, 12, 0, 1, 0, 0, 1, 33, 0, 34, 0, 1, 0, 0, 0, 33, 0, 0, 1, 2, 33, 10, 0, 1, 0, 0, 0, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 9, 0, 0, 2, 0, 27, 2, 0, 11, 0, 0, 0, 0, 0, 11, 0, 34, 0, 1, 0, 19, 0, 3, 2, 0, 0, 34, 0, 1, 0, 2, 0, 11, 0, 32, 0, 0, 0, 19, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 15, 3, 0, 29, 13, 0, 0, 0, 34, 0, 1, 0, 15, 3, 0, 1, 0, 0, 26, 1, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 1, 0, 28, 0, 0, 1, 0, 0, 12, 0, 0, 18, 0, 11, 0, 7]\n",
      "sent  [6, 0, 5, 0, 35, 0, 0, 3, 0, 1, 0, 0, 0, 0, 1, 0, 15, 0, 0, 5, 10, 0, 1, 3, 25, 22, 0, 0, 0, 33, 15, 0, 0, 39, 0, 0, 0, 3, 0, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 2, 8, 4, 1, 0, 3, 0, 0, 0, 18, 0, 0, 38, 7]\n",
      "sent  [6, 4, 1, 0, 1, 0, 0, 8, 0, 17, 20, 0, 4, 0, 0, 4, 5, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 5, 37, 0, 5, 0, 4, 0, 0, 0, 0, 2, 0, 0, 9, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 31, 0, 0, 0, 0, 0, 0, 1, 4, 15, 3, 0, 0, 31, 0, 0, 0, 1, 2, 0, 32, 0, 23, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 0, 4, 17, 32, 0, 5, 18, 0, 8, 0, 34, 0, 0, 1, 2, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 11, 0, 34, 2, 3, 0, 1, 0, 0, 0, 14, 2, 39, 26, 4, 0, 1, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 4, 13, 0, 0, 24, 0, 1, 0, 0, 39, 0, 31, 0, 0, 5, 0, 0, 5, 0, 14, 18, 0, 0, 1, 0, 12, 0, 32, 0, 0, 12, 0, 32, 4, 0, 0, 0, 0, 0, 13, 0, 4, 9, 0, 0, 15, 3, 0, 22, 0, 36, 0, 2, 1, 0, 17, 0, 0, 3, 0, 0, 22, 1, 0, 0, 0, 0, 7]\n",
      "Alpha: 0.025000 Progress: 1230 of 1230 (100.00%)\n",
      "Completed training. Training took 0.0021280646324157715 minutes\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import struct\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from multiprocessing import Pool, Value, Array\n",
    "\n",
    "class VocabItem:\n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.count = 0\n",
    "        self.path = None # Path (list of indices) from the root to the word (leaf)\n",
    "        self.code = None # Huffman encoding\n",
    "        \n",
    "        \n",
    "class Vocab:\n",
    "    def __init__(self, fi, min_count):\n",
    "        vocab_items = []\n",
    "        vocab_hash = {}\n",
    "        word_count = 0\n",
    "        fi = open(fi, 'r')\n",
    "\n",
    "        # Add special tokens <bol> (beginning of line) and <eol> (end of line)\n",
    "        for token in ['<bol>', '<eol>']:\n",
    "            vocab_hash[token] = len(vocab_items)\n",
    "            vocab_items.append(VocabItem(token))\n",
    "\n",
    "        for line in fi:\n",
    "            tokens = line.split()\n",
    "            for token in tokens:\n",
    "                if token not in vocab_hash:\n",
    "                    vocab_hash[token] = len(vocab_items)\n",
    "                    vocab_items.append(VocabItem(token))\n",
    "                \n",
    "                vocab_items[vocab_hash[token]].count += 1\n",
    "                word_count += 1\n",
    "            \n",
    "                if word_count % 1000 == 0:\n",
    "                    sys.stdout.write(\"\\rReading word %d\" % word_count)\n",
    "                    sys.stdout.flush() \n",
    "                    \n",
    "            # Add special tokens <bol> (beginning of line) and <eol> (end of line)\n",
    "            vocab_items[vocab_hash['<bol>']].count += 1\n",
    "            vocab_items[vocab_hash['<eol>']].count += 1\n",
    "            word_count += 2\n",
    "        \n",
    "        #set the values to class level variables\n",
    "        self.bytes = fi.tell()\n",
    "        self.vocab_items = vocab_items         # List of VocabItem objects\n",
    "        self.vocab_hash = vocab_hash           # Mapping from each token to its index in vocab\n",
    "        self.word_count = word_count           # Total number of words in train file\n",
    "        \n",
    "        # Add special token <unk> (unknown),\n",
    "        # merge words occurring less than min_count into <unk>, and\n",
    "        # sort vocab in descending order by frequency in train file\n",
    "        self.__sort(min_count)\n",
    "        \n",
    "        for eachVocabItem in self.vocab_items:\n",
    "            print(\"eachVocab \", eachVocabItem.word, eachVocabItem.count, eachVocabItem.path, eachVocabItem.code)\n",
    "                    \n",
    "    def __getitem__(self, i):\n",
    "        return self.vocab_items[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_items)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.vocab_items)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.vocab_hash\n",
    "                    \n",
    "    def __sort(self, min_count):\n",
    "        tmp = []\n",
    "        tmp.append(VocabItem('<unk>'))\n",
    "        unk_hash = 0\n",
    "        \n",
    "        count_unk = 0\n",
    "        for token in self.vocab_items:\n",
    "            if token.count < min_count:\n",
    "                count_unk += 1\n",
    "                tmp[unk_hash].count += token.count\n",
    "            else:\n",
    "                tmp.append(token)\n",
    "        \n",
    "        #sort the VocabItems according by count\n",
    "        tmp.sort(key=lambda token : token.count, reverse=True)\n",
    "        \n",
    "        # Update vocab_hash\n",
    "        vocab_hash = {}\n",
    "        for i, token in enumerate(tmp):\n",
    "            vocab_hash[token.word] = i\n",
    "\n",
    "        self.vocab_items = tmp\n",
    "        self.vocab_hash = vocab_hash\n",
    "\n",
    "        print(\"\")\n",
    "        print('Unknown vocab size:', count_unk)\n",
    "     \n",
    "    def indices(self, tokens):\n",
    "        return [self.vocab_hash[token] if token in self else self.vocab_hash['<unk>'] for token in tokens]\n",
    "    \n",
    "    def encode_huffman(self):\n",
    "        # Build a Huffman tree\n",
    "        vocab_size = len(self)\n",
    "        print(\"encode_humffman vocab_size \", vocab_size)\n",
    "        count = [t.count for t in self] + [1e15] * (vocab_size - 1)\n",
    "        print(\"count\",count)\n",
    "        parent = [0] * (2 * vocab_size - 2)\n",
    "        binary = [0] * (2 * vocab_size - 2)\n",
    "        print(\"parent\",parent)\n",
    "        print(\"binary \", binary)\n",
    "        \n",
    "        pos1 = vocab_size - 1\n",
    "        pos2 = vocab_size\n",
    "        \n",
    "        for i in range(vocab_size - 1):\n",
    "            # Find min1\n",
    "            if pos1 >= 0:\n",
    "                if count[pos1] < count[pos2]:\n",
    "                    min1 = pos1\n",
    "                    pos1 -= 1\n",
    "                else:\n",
    "                    min1 = pos2\n",
    "                    pos2 += 1\n",
    "            else:\n",
    "                min1 = pos2\n",
    "                pos2 += 1\n",
    "\n",
    "            # Find min2\n",
    "            if pos1 >= 0:\n",
    "                if count[pos1] < count[pos2]:\n",
    "                    min2 = pos1\n",
    "                    pos1 -= 1\n",
    "                else:\n",
    "                    min2 = pos2\n",
    "                    pos2 += 1\n",
    "            else:\n",
    "                min2 = pos2\n",
    "                pos2 += 1\n",
    "\n",
    "            count[vocab_size + i] = count[min1] + count[min2]\n",
    "            parent[min1] = vocab_size + i\n",
    "            parent[min2] = vocab_size + i\n",
    "            binary[min2] = 1\n",
    "\n",
    "        # Assign binary code and path pointers to each vocab word\n",
    "        root_idx = 2 * vocab_size - 2\n",
    "        for i, token in enumerate(self):\n",
    "            path = [] # List of indices from the leaf to the root\n",
    "            code = [] # Binary Huffman encoding from the leaf to the root\n",
    "\n",
    "            node_idx = i\n",
    "            while node_idx < root_idx:\n",
    "                if node_idx >= vocab_size: path.append(node_idx)\n",
    "                code.append(binary[node_idx])\n",
    "                node_idx = parent[node_idx]\n",
    "            path.append(root_idx)\n",
    "\n",
    "            # These are path and code from the root to the leaf\n",
    "            token.path = [j - vocab_size for j in path[::-1]]\n",
    "            token.code = code[::-1]\n",
    "        print(\"After huffman encoding \")\n",
    "        print(\"parent\",parent)\n",
    "        print(\"binary \", binary)\n",
    "        print(\"self \", self)\n",
    "        for i, token in enumerate(self):\n",
    "            print(\"path, code  \", token.path, token.code)\n",
    "        print(\"Huffman encoding done\")\n",
    "        \n",
    "def sigmoid(z):\n",
    "    if z > 6:\n",
    "        return 1.0\n",
    "    elif z < -6:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1 / (1 + math.exp(-z))\n",
    "    \n",
    "def init_net(dim, vocab_size):\n",
    "    # Init syn0 with random numbers from a uniform distribution on the interval [-0.5, 0.5]/dim\n",
    "    tmp = np.random.uniform(low=-0.5/dim, high=0.5/dim, size=(vocab_size, dim))\n",
    "    syn0 = np.ctypeslib.as_ctypes(tmp)\n",
    "    syn0 = Array(syn0._type_, syn0, lock=False)\n",
    "    print(\"syn0\", tmp.shape)\n",
    "\n",
    "    # Init syn1 with zeros\n",
    "    tmp = np.zeros(shape=(vocab_size, dim))\n",
    "    syn1 = np.ctypeslib.as_ctypes(tmp)\n",
    "    syn1 = Array(syn1._type_, syn1, lock=False)\n",
    "    print(\"syn1\", tmp.shape)\n",
    "    \n",
    "    return (syn0, syn1)\n",
    "    \n",
    "def train_process(pid):\n",
    "    # Set fi to point to the right chunk of training file\n",
    "    start = vocab.bytes / num_processes * pid\n",
    "    end = vocab.bytes if pid == num_processes - 1 else vocab.bytes / num_processes * (pid + 1)\n",
    "    fi.seek(start)\n",
    "    #print 'Worker %d beginning training at %d, ending at %d' % (pid, start, end)\n",
    "\n",
    "    alpha = starting_alpha\n",
    "    \n",
    "    word_count = 0\n",
    "    last_word_count = 0\n",
    "    \n",
    "    while fi.tell() < end:\n",
    "        line = fi.readline().strip()\n",
    "        # Skip blank lines\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Init sent, a list of indices of words in line\n",
    "        sent = vocab.indices(['<bol>'] + line.split() + ['<eol>'])\n",
    "        print(\"sent \", sent)\n",
    "        \n",
    "        for sent_pos, token in enumerate(sent):\n",
    "            if word_count % 10000 == 0:\n",
    "                global_word_count.value += (word_count - last_word_count)\n",
    "                last_word_count = word_count\n",
    "\n",
    "                # Recalculate alpha\n",
    "                alpha = starting_alpha * (1 - float(global_word_count.value) / vocab.word_count)\n",
    "                if alpha < starting_alpha * 0.0001: alpha = starting_alpha * 0.0001\n",
    "\n",
    "                # Print progress info\n",
    "                sys.stdout.write(\"\\rAlpha: %f Progress: %d of %d (%.2f%%)\" %\n",
    "                                 (alpha, global_word_count.value, vocab.word_count,\n",
    "                                  float(global_word_count.value) / vocab.word_count * 100))\n",
    "                sys.stdout.flush()\n",
    "    \n",
    "            # Randomize window size, where win is the max window size\n",
    "            current_win = np.random.randint(low=1, high=win+1)\n",
    "            context_start = max(sent_pos - current_win, 0)\n",
    "            context_end = min(sent_pos + current_win + 1, len(sent))\n",
    "            context = sent[context_start:sent_pos] + sent[sent_pos+1:context_end] # Turn into an iterator?   \n",
    "            #print(\"context \", context)\n",
    "        \n",
    "            # CBOW\n",
    "            if cbow:\n",
    "                # Compute neu1\n",
    "                neu1 = np.mean(np.array([syn0[c] for c in context]), axis=0)\n",
    "                assert len(neu1) == dim, 'neu1 and dim do not agree'\n",
    "\n",
    "                # Init neu1e with zeros\n",
    "                neu1e = np.zeros(dim)\n",
    "\n",
    "                # Compute neu1e and update syn1\n",
    "                if neg > 0:\n",
    "                    classifiers = [(token, 1)] + [(target, 0) for target in table.sample(neg)]\n",
    "                else:\n",
    "                    classifiers = zip(vocab[token].path, vocab[token].code)\n",
    "                for target, label in classifiers:\n",
    "                    z = np.dot(neu1, syn1[target])\n",
    "                    p = sigmoid(z)\n",
    "                    g = alpha * (label - p)\n",
    "                    neu1e += g * syn1[target] # Error to backpropagate to syn0\n",
    "                    syn1[target] += g * neu1  # Update syn1\n",
    "\n",
    "                # Update syn0\n",
    "                for context_word in context:\n",
    "                    syn0[context_word] += neu1e\n",
    "\n",
    "            # Skip-gram\n",
    "            else:\n",
    "                for context_word in context:\n",
    "                    # Init neu1e with zeros\n",
    "                    neu1e = np.zeros(dim)\n",
    "\n",
    "                    # Compute neu1e and update syn1\n",
    "                    if neg > 0:\n",
    "                        classifiers = [(token, 1)] + [(target, 0) for target in table.sample(neg)]\n",
    "                    else:\n",
    "                        classifiers = zip(vocab[token].path, vocab[token].code)\n",
    "                    for target, label in classifiers:\n",
    "                        z = np.dot(syn0[context_word], syn1[target])\n",
    "                        p = sigmoid(z)\n",
    "                        g = alpha * (label - p)\n",
    "                        neu1e += g * syn1[target]              # Error to backpropagate to syn0\n",
    "                        syn1[target] += g * syn0[context_word] # Update syn1\n",
    "\n",
    "                    # Update syn0\n",
    "                    syn0[context_word] += neu1e\n",
    "\n",
    "            word_count += 1\n",
    "\n",
    "    # Print progress info\n",
    "    global_word_count.value += (word_count - last_word_count)\n",
    "    sys.stdout.write(\"\\rAlpha: %f Progress: %d of %d (%.2f%%)\" %\n",
    "                     (alpha, global_word_count.value, vocab.word_count,\n",
    "                      float(global_word_count.value)/vocab.word_count * 100))\n",
    "    sys.stdout.flush()\n",
    "    fi.close()\n",
    "\n",
    "    \n",
    "def __init_process(*args):\n",
    "    global vocab, syn0, syn1, table, cbow, neg, dim, starting_alpha\n",
    "    global win, num_processes, global_word_count, fi\n",
    "    \n",
    "    vocab, syn0_tmp, syn1_tmp, table, cbow, neg, dim, starting_alpha, win, num_processes, global_word_count = args[:-1]\n",
    "    fi = open(args[-1], 'r')\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', RuntimeWarning)\n",
    "        syn0 = np.ctypeslib.as_array(syn0_tmp)\n",
    "        syn1 = np.ctypeslib.as_array(syn1_tmp)\n",
    "\n",
    "        \n",
    "def train(fi, fo, cbow, neg, dim, alpha, win, min_count, num_processes, binary):\n",
    "    # Read train file to init vocab\n",
    "    vocab = Vocab(fi, min_count)\n",
    "    # Init net\n",
    "    syn0, syn1 = init_net(dim, len(vocab))\n",
    "    \n",
    "    global_word_count = Value('i', 0)\n",
    "    table = None\n",
    "    if neg > 0:\n",
    "        print('Initializing unigram table')\n",
    "        #table = UnigramTable(vocab)\n",
    "    else:\n",
    "        print('Initializing Huffman tree')\n",
    "        vocab.encode_huffman()\n",
    "        \n",
    "    # Begin training using num_processes workers\n",
    "    t0 = time.time()\n",
    "    pool = Pool(processes=num_processes, initializer=__init_process,\n",
    "                initargs=(vocab, syn0, syn1, table, cbow, neg, dim, alpha,\n",
    "                          win, num_processes, global_word_count, fi))\n",
    "    pool.map(train_process, range(num_processes))\n",
    "    t1 = time.time()\n",
    "    print(\"\")\n",
    "    print('Completed training. Training took', (t1 - t0) / 60, 'minutes')\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument('-train', help='Training file', dest='fi', required=True)\n",
    "    #parser.add_argument('-model', help='Output model file', dest='fo', required=True)\n",
    "    #parser.add_argument('-cbow', help='1 for CBOW, 0 for skip-gram', dest='cbow', default=1, type=int)\n",
    "    #parser.add_argument('-negative', help='Number of negative examples (>0) for negative sampling, 0 for hierarchical softmax', dest='neg', default=5, type=int)\n",
    "    #parser.add_argument('-dim', help='Dimensionality of word embeddings', dest='dim', default=100, type=int)\n",
    "    #parser.add_argument('-alpha', help='Starting alpha', dest='alpha', default=0.025, type=float)\n",
    "    #parser.add_argument('-window', help='Max window length', dest='win', default=5, type=int) \n",
    "    #parser.add_argument('-min-count', help='Min count for words used to learn <unk>', dest='min_count', default=5, type=int)\n",
    "    #parser.add_argument('-processes', help='Number of processes', dest='num_processes', default=1, type=int)\n",
    "    #parser.add_argument('-binary', help='1 for output model in binary format, 0 otherwise', dest='binary', default=0, type=int)\n",
    "    #TO DO: parser.add_argument('-epoch', help='Number of training epochs', dest='epoch', default=1, type=int)\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    train(\"smalltest.txt\", \"mytestOut.txt\", True, 0, 100, 0.025, 5, \n",
    "                            5, 1, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
