{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To plot learning curve graph\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for pretty print\n",
    "from pprint import pprint\n",
    "\n",
    "# for tokenizer\n",
    "import re\n",
    "\n",
    "# for word counter in vocabulary dictionary\n",
    "from collections import Counter\n",
    "\n",
    "# for checkpoint paths\n",
    "import os\n",
    "\n",
    "# for fancy progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# for output_projection\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# for sampling probability decay\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# maximum length of input and target sentences including paddings\n",
    "enc_sentence_length = 10\n",
    "dec_sentence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch_size: 2\n",
    "input_batches = [\n",
    "    ['Hi What is your name?', 'Nice to meet you!'],\n",
    "    ['Which programming language do you use?', 'See you later.'],\n",
    "    ['Where do you live?', 'What is your major?'],\n",
    "    ['What do you want to drink?', 'What is your favorite beer?']]\n",
    "\n",
    "target_batches = [\n",
    "    ['Hi this is Jaemin.', 'Nice to meet you too!'],\n",
    "    ['I like Python.', 'Bye Bye.'],\n",
    "    ['I live in Seoul, South Korea.', 'I study industrial engineering.'],\n",
    "    ['Beer please!', 'Leffe brown!']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi What is your name?',\n",
       " 'Nice to meet you!',\n",
       " 'Which programming language do you use?',\n",
       " 'See you later.',\n",
       " 'Where do you live?',\n",
       " 'What is your major?',\n",
       " 'What do you want to drink?',\n",
       " 'What is your favorite beer?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_sentences = []\n",
    "for input_batch in input_batches:\n",
    "    all_input_sentences.extend(input_batch)\n",
    "    \n",
    "all_target_sentences = []\n",
    "for target_batch in target_batches:\n",
    "    all_target_sentences.extend(target_batch)\n",
    "\n",
    "# Example\n",
    "all_input_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '?', '?', '\"', 'sdfs', '%', '@', '#', '%']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(sentence):\n",
    "    tokens = re.findall(r\"[\\w]+|[^\\s\\w]\", sentence)\n",
    "    return tokens\n",
    "\n",
    "# Example\n",
    "tokenizer('Hello world?? \"sdfs%@#%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'!': 12,\n",
      "  '.': 19,\n",
      "  '?': 1,\n",
      "  'Hi': 8,\n",
      "  'Nice': 10,\n",
      "  'See': 17,\n",
      "  'What': 3,\n",
      "  'Where': 20,\n",
      "  'Which': 13,\n",
      "  '_PAD': 0,\n",
      "  'beer': 26,\n",
      "  'do': 6,\n",
      "  'drink': 24,\n",
      "  'favorite': 25,\n",
      "  'is': 4,\n",
      "  'language': 15,\n",
      "  'later': 18,\n",
      "  'live': 21,\n",
      "  'major': 22,\n",
      "  'meet': 11,\n",
      "  'name': 9,\n",
      "  'programming': 14,\n",
      "  'to': 7,\n",
      "  'use': 16,\n",
      "  'want': 23,\n",
      "  'you': 2,\n",
      "  'your': 5},\n",
      " {0: '_PAD',\n",
      "  1: '?',\n",
      "  2: 'you',\n",
      "  3: 'What',\n",
      "  4: 'is',\n",
      "  5: 'your',\n",
      "  6: 'do',\n",
      "  7: 'to',\n",
      "  8: 'Hi',\n",
      "  9: 'name',\n",
      "  10: 'Nice',\n",
      "  11: 'meet',\n",
      "  12: '!',\n",
      "  13: 'Which',\n",
      "  14: 'programming',\n",
      "  15: 'language',\n",
      "  16: 'use',\n",
      "  17: 'See',\n",
      "  18: 'later',\n",
      "  19: '.',\n",
      "  20: 'Where',\n",
      "  21: 'live',\n",
      "  22: 'major',\n",
      "  23: 'want',\n",
      "  24: 'drink',\n",
      "  25: 'favorite',\n",
      "  26: 'beer'},\n",
      " 26)\n",
      "\n",
      "\n",
      "({'!': 2,\n",
      "  ',': 19,\n",
      "  '.': 1,\n",
      "  'Beer': 25,\n",
      "  'Bye': 4,\n",
      "  'Hi': 5,\n",
      "  'I': 3,\n",
      "  'Jaemin': 8,\n",
      "  'Korea': 21,\n",
      "  'Leffe': 27,\n",
      "  'Nice': 9,\n",
      "  'Python': 15,\n",
      "  'Seoul': 18,\n",
      "  'South': 20,\n",
      "  '_PAD': 0,\n",
      "  'brown': 28,\n",
      "  'engineering': 24,\n",
      "  'in': 17,\n",
      "  'industrial': 23,\n",
      "  'is': 7,\n",
      "  'like': 14,\n",
      "  'live': 16,\n",
      "  'meet': 11,\n",
      "  'please': 26,\n",
      "  'study': 22,\n",
      "  'this': 6,\n",
      "  'to': 10,\n",
      "  'too': 13,\n",
      "  'you': 12},\n",
      " {0: '_PAD',\n",
      "  1: '.',\n",
      "  2: '!',\n",
      "  3: 'I',\n",
      "  4: 'Bye',\n",
      "  5: 'Hi',\n",
      "  6: 'this',\n",
      "  7: 'is',\n",
      "  8: 'Jaemin',\n",
      "  9: 'Nice',\n",
      "  10: 'to',\n",
      "  11: 'meet',\n",
      "  12: 'you',\n",
      "  13: 'too',\n",
      "  14: 'like',\n",
      "  15: 'Python',\n",
      "  16: 'live',\n",
      "  17: 'in',\n",
      "  18: 'Seoul',\n",
      "  19: ',',\n",
      "  20: 'South',\n",
      "  21: 'Korea',\n",
      "  22: 'study',\n",
      "  23: 'industrial',\n",
      "  24: 'engineering',\n",
      "  25: 'Beer',\n",
      "  26: 'please',\n",
      "  27: 'Leffe',\n",
      "  28: 'brown'},\n",
      " 28)\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(sentences, is_target=False, max_vocab_size=None):\n",
    "    word_counter = Counter()\n",
    "    vocab = dict()\n",
    "    reverse_vocab = dict()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence)\n",
    "        word_counter.update(tokens)\n",
    "        \n",
    "    if max_vocab_size is None:\n",
    "        max_vocab_size = len(word_counter)\n",
    "    \n",
    "    if is_target:\n",
    "        vocab['_GO'] = 0\n",
    "        vocab['_PAD'] = 1\n",
    "        vocab_idx = 2\n",
    "        for key, value in word_counter.most_common(max_vocab_size):\n",
    "            vocab[key] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "    else:\n",
    "        vocab['_PAD'] = 0\n",
    "        vocab_idx = 1\n",
    "        for key, value in word_counter.most_common(max_vocab_size):\n",
    "            vocab[key] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "            \n",
    "    for key, value in vocab.items():\n",
    "        reverse_vocab[value] = key\n",
    "            \n",
    "    return vocab, reverse_vocab, max_vocab_size\n",
    "\n",
    "# Example\n",
    "pprint(build_vocab(all_input_sentences))\n",
    "print('\\n')\n",
    "pprint(build_vocab(all_target_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vocabulary size: 26\n",
      "target vocabulary size: 28\n"
     ]
    }
   ],
   "source": [
    "enc_vocab, enc_reverse_vocab, enc_vocab_size = build_vocab(all_input_sentences)\n",
    "dec_vocab, dec_reverse_vocab, dec_vocab_size = build_vocab(all_target_sentences, is_target=True)\n",
    "\n",
    "print('input vocabulary size:', enc_vocab_size)\n",
    "print('target vocabulary size:', dec_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice 10\n",
      "to 7\n",
      "meet 11\n",
      "you 2\n",
      "! 12\n"
     ]
    }
   ],
   "source": [
    "def token2idx(word, vocab):\n",
    "    return vocab[word]\n",
    "\n",
    "for token in tokenizer('Nice to meet you!'):\n",
    "    print(token, token2idx(token, enc_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi What is your name?\n",
      "([8, 3, 4, 5, 9, 1, 0, 0, 0, 0], 6)\n",
      "Hi this is Jaemin.\n",
      "([0, 6, 7, 8, 9, 2, 1, 1, 1, 1, 1], 5)\n"
     ]
    }
   ],
   "source": [
    "def sent2idx(sent, vocab=enc_vocab, max_sentence_length=enc_sentence_length, is_target=False):\n",
    "    tokens = tokenizer(sent)\n",
    "    current_length = len(tokens)\n",
    "    pad_length = max_sentence_length - current_length\n",
    "    if is_target:\n",
    "        return [0] + [token2idx(token, vocab) for token in tokens] + [1] * pad_length, current_length\n",
    "    else:\n",
    "        return [token2idx(token, vocab) for token in tokens] + [0] * pad_length, current_length\n",
    "\n",
    "# Enc Example\n",
    "print('Hi What is your name?')\n",
    "print(sent2idx('Hi What is your name?'))\n",
    "\n",
    "# Dec Example\n",
    "print('Hi this is Jaemin.')\n",
    "print(sent2idx('Hi this is Jaemin.', vocab=dec_vocab, max_sentence_length=dec_sentence_length, is_target=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idx2token(idx, reverse_vocab):\n",
    "    return reverse_vocab[idx]\n",
    "\n",
    "def idx2sent(indices, reverse_vocab=dec_reverse_vocab):\n",
    "    return \" \".join([idx2token(idx, reverse_vocab) for idx in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters / Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DemoConfig:\n",
    "    \n",
    "    # Model\n",
    "    hidden_size = 30\n",
    "    enc_emb_size = 30\n",
    "    dec_emb_size = 30\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell\n",
    "    \n",
    "    # Training\n",
    "    optimizer = tf.train.RMSPropOptimizer\n",
    "    n_epoch = 801\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Sampling Probability\n",
    "    # with decay => 'Curriculumn Learning'\n",
    "    sampling_probability_list = np.linspace(\n",
    "        start=0.0,\n",
    "        stop=1.0,\n",
    "        num=n_epoch,\n",
    "        dtype=np.float32)\n",
    "\n",
    "    # Checkpoint path\n",
    "    ckpt_dir = './ckpt_dir/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, config, mode='training'):\n",
    "        assert mode in ['training', 'evaluation', 'inference']\n",
    "        self.mode = mode\n",
    "\n",
    "        # Model\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.enc_emb_size = config.enc_emb_size\n",
    "        self.dec_emb_size = config.dec_emb_size\n",
    "        self.cell = config.cell\n",
    "        \n",
    "        # Training\n",
    "        self.optimizer = config.optimizer\n",
    "        self.n_epoch = config.n_epoch\n",
    "        self.learning_rate = config.learning_rate\n",
    "        \n",
    "        # Sampling Probability\n",
    "        self.sampling_probability_list = config.sampling_probability_list\n",
    "        \n",
    "        # Checkpoint path\n",
    "        self.ckpt_dir = config.ckpt_dir\n",
    "        \n",
    "    def add_placeholders(self):\n",
    "        self.enc_inputs = tf.placeholder(\n",
    "            tf.int32,\n",
    "            shape=[None, enc_sentence_length],\n",
    "            name='input_sentences')\n",
    "\n",
    "        self.enc_sequence_length = tf.placeholder(\n",
    "            tf.int32,\n",
    "            shape=[None,],\n",
    "            name='input_sequence_length')\n",
    "        \n",
    "        if self.mode == 'training':\n",
    "            self.dec_inputs = tf.placeholder(\n",
    "                tf.int32,\n",
    "                shape=[None, dec_sentence_length+1],\n",
    "                name='target_sentences')\n",
    "\n",
    "            self.dec_sequence_length = tf.placeholder(\n",
    "                tf.int32,\n",
    "                shape=[None,],\n",
    "                name='target_sequence_length')\n",
    "\n",
    "            self.sampling_probability = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=[],\n",
    "                name='sampling_probability')\n",
    "            # 0.0 ≤ sampling_probability ≤ 1.0\n",
    "            # 0.0: no sampling => `ScheduledEmbedidngTrainingHelper` is equivalent to `TrainingHelper`\n",
    "            # 1.0: always sampling => `ScheduledEmbedidngTrainingHelper` is equivalent to `GreedyEmbeddingHelper`\n",
    "            # Inceasing sampling over steps => Curriculum Learning\n",
    "            \n",
    "    def add_encoder(self):\n",
    "        with tf.variable_scope('Encoder') as scope:\n",
    "            with tf.device('/cpu:0'):\n",
    "                self.enc_Wemb = tf.get_variable('embedding',\n",
    "                    initializer=tf.random_uniform([enc_vocab_size+1, self.enc_emb_size]),\n",
    "                    dtype=tf.float32)\n",
    "\n",
    "            # [Batch_size x enc_sent_len x embedding_size]\n",
    "            enc_emb_inputs = tf.nn.embedding_lookup(\n",
    "                self.enc_Wemb, self.enc_inputs, name='emb_inputs')\n",
    "            enc_cell = self.cell(self.hidden_size)\n",
    "\n",
    "            # enc_outputs: [batch_size x enc_sent_len x embedding_size]\n",
    "            # enc_last_state: [batch_size x embedding_size]\n",
    "            enc_outputs, self.enc_last_state = tf.nn.dynamic_rnn(\n",
    "                cell=enc_cell,\n",
    "                inputs=enc_emb_inputs,\n",
    "                sequence_length=self.enc_sequence_length,\n",
    "                time_major=False,\n",
    "                dtype=tf.float32)\n",
    "            \n",
    "    def add_decoder(self):\n",
    "        with tf.variable_scope('Decoder') as scope:\n",
    "            with tf.device('/cpu:0'):\n",
    "                self.dec_Wemb = tf.get_variable('embedding',\n",
    "                    initializer=tf.random_uniform([dec_vocab_size+2, self.dec_emb_size]),\n",
    "                    dtype=tf.float32)\n",
    "\n",
    "            dec_cell = self.cell(self.hidden_size)\n",
    "\n",
    "            # output projection (replacing `OutputProjectionWrapper`)\n",
    "            output_layer = Dense(dec_vocab_size+2, name='output_projection')\n",
    "            \n",
    "            if self.mode == 'training':\n",
    "\n",
    "                # maximum unrollings in current batch = max(dec_sent_len) + 1(GO symbol)\n",
    "                max_dec_len = tf.reduce_max(self.dec_sequence_length+1, name='max_dec_len')\n",
    "\n",
    "                dec_emb_inputs = tf.nn.embedding_lookup(\n",
    "                    self.dec_Wemb, self.dec_inputs, name='emb_inputs')\n",
    "\n",
    "                training_helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "                    inputs=dec_emb_inputs,\n",
    "                    sequence_length=self.dec_sequence_length+1,\n",
    "                    embedding=self.dec_Wemb,\n",
    "                    sampling_probability=self.sampling_probability,\n",
    "                    time_major=False,\n",
    "                    name='training_helper')                \n",
    "\n",
    "                training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell=dec_cell,\n",
    "                    helper=training_helper,\n",
    "                    initial_state=self.enc_last_state,\n",
    "                    output_layer=output_layer) \n",
    "\n",
    "                train_dec_outputs, train_dec_last_state,_ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    training_decoder,\n",
    "                    output_time_major=False,\n",
    "                    impute_finished=True,\n",
    "                    maximum_iterations=max_dec_len)\n",
    "                \n",
    "                # dec_outputs: collections.namedtuple(rnn_outputs, sample_id)\n",
    "                # dec_outputs.rnn_output: [batch_size x max(dec_sequence_length) x dec_vocab_size+2], tf.float32\n",
    "                # dec_outputs.sample_id [batch_size], tf.int32\n",
    "                \n",
    "                # logits: [batch_size x max_dec_len x dec_vocab_size+2]\n",
    "                logits = tf.identity(train_dec_outputs.rnn_output, name='logits')\n",
    "                \n",
    "                # targets: [batch_size x max_dec_len x dec_vocab_size+2]\n",
    "                targets = tf.slice(self.dec_inputs, [0, 0], [-1, max_dec_len], 'targets')\n",
    "                \n",
    "                # masks: [batch_size x max_dec_len]\n",
    "                # => ignore outputs after `dec_senquence_length+1` when calculating loss\n",
    "                masks = tf.sequence_mask(self.dec_sequence_length+1, max_dec_len, dtype=tf.float32, name='masks')\n",
    "                \n",
    "                # Control loss dimensions with `average_across_timesteps` and `average_across_batch`\n",
    "                # internal: `tf.nn.sparse_softmax_cross_entropy_with_logits`\n",
    "                self.batch_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "                    logits=logits,\n",
    "                    targets=targets,\n",
    "                    weights=masks,\n",
    "                    name='batch_loss')\n",
    "                \n",
    "                # prediction sample for validation\n",
    "                # some sample_id are overwritten with '-1's\n",
    "                self.valid_predictions = tf.argmax(logits, axis=2, name='valid_predictions')\n",
    "                \n",
    "                # List of training variables\n",
    "                # self.training_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "            \n",
    "            elif self.mode == 'inference':\n",
    "\n",
    "                batch_size = tf.shape(self.enc_inputs)[0:1]\n",
    "                start_tokens = tf.zeros(batch_size, dtype=tf.int32)\n",
    "\n",
    "                inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                    embedding=self.dec_Wemb,\n",
    "                    start_tokens=start_tokens,\n",
    "                    end_token=1)\n",
    "                \n",
    "                inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell=dec_cell,\n",
    "                    helper=inference_helper,\n",
    "                    initial_state=self.enc_last_state,\n",
    "                    output_layer=output_layer)\n",
    "                \n",
    "                infer_dec_outputs, infer_dec_last_state,_ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    inference_decoder,\n",
    "                    output_time_major=False,\n",
    "                    impute_finished=True,\n",
    "                    maximum_iterations=dec_sentence_length)\n",
    "                \n",
    "                # [batch_size x dec_sentence_length], tf.int32\n",
    "                self.predictions = tf.identity(infer_dec_outputs.sample_id, name='predictions')\n",
    "                # equivalent to tf.argmax(infer_dec_outputs.rnn_output, axis=2, name='predictions')\n",
    "\n",
    "                # List of training variables\n",
    "                # self.training_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        \n",
    "    def add_training_op(self):\n",
    "        self.training_op = self.optimizer(self.learning_rate, name='training_op').minimize(self.batch_loss)\n",
    "        \n",
    "    def save(self, sess, var_list=None, save_path=None):\n",
    "        print(f'Saving model at {save_path}')\n",
    "        if hasattr(self, 'training_variables'):\n",
    "            var_list = self.training_variables\n",
    "        saver = tf.train.Saver(var_list)\n",
    "        saver.save(sess, save_path, write_meta_graph=False)\n",
    "        \n",
    "    def restore(self, sess, var_list=None, ckpt_path=None):\n",
    "        if hasattr(self, 'training_variables'):\n",
    "            var_list = self.training_variables\n",
    "        self.restorer = tf.train.Saver(var_list)\n",
    "        self.restorer.restore(sess, ckpt_path)\n",
    "        print('Restore Finished!')\n",
    "        \n",
    "    def summary(self):\n",
    "        summary_writer = tf.summary.FileWriter(\n",
    "            logdir=self.ckpt_dir,\n",
    "            graph=tf.get_default_graph())\n",
    "        \n",
    "    def build(self):\n",
    "        self.add_placeholders()\n",
    "        self.add_encoder()\n",
    "        self.add_decoder()\n",
    "        \n",
    "    def train(self, sess, data, from_scratch=False, load_ckpt=None, save_path=None):\n",
    "        \n",
    "        # Restore Checkpoint\n",
    "        if from_scratch is False and os.path.isfile(load_ckpt):\n",
    "            self.restore(sess, load_ckpt)\n",
    "    \n",
    "        # Add Optimizer to current graph\n",
    "        self.add_training_op()\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        input_batches, target_batches = data\n",
    "        loss_history = []\n",
    "        \n",
    "        for epoch in tqdm(range(self.n_epoch)):\n",
    "\n",
    "            all_preds = []\n",
    "            epoch_loss = 0\n",
    "            for input_batch, target_batch in zip(input_batches, target_batches):\n",
    "                input_batch_tokens = []\n",
    "                target_batch_tokens = []\n",
    "                input_batch_sent_lens = []\n",
    "                target_batch_sent_lens = []\n",
    "\n",
    "                for input_sent in input_batch:\n",
    "                    tokens, sent_len = sent2idx(input_sent)\n",
    "                    input_batch_tokens.append(tokens)\n",
    "                    input_batch_sent_lens.append(sent_len)\n",
    "\n",
    "                for target_sent in target_batch:\n",
    "                    tokens, sent_len = sent2idx(target_sent,\n",
    "                                 vocab=dec_vocab,\n",
    "                                 max_sentence_length=dec_sentence_length,\n",
    "                                 is_target=True)\n",
    "                    target_batch_tokens.append(tokens)\n",
    "                    target_batch_sent_lens.append(sent_len)\n",
    "       \n",
    "                # Evaluate 3 ops in the graph\n",
    "                # => valid_predictions, loss, training_op(optimzier)\n",
    "                batch_valid_preds, batch_loss, _ = sess.run(\n",
    "                    [self.valid_predictions, self.batch_loss, self.training_op],\n",
    "                    feed_dict={\n",
    "                        self.enc_inputs: input_batch_tokens,\n",
    "                        self.enc_sequence_length: input_batch_sent_lens,\n",
    "                        self.dec_inputs: target_batch_tokens,\n",
    "                        self.dec_sequence_length: target_batch_sent_lens,\n",
    "                        self.sampling_probability: self.sampling_probability_list[epoch]\n",
    "                    }\n",
    "                )\n",
    "                # loss_history.append(batch_loss)\n",
    "                epoch_loss += batch_loss\n",
    "                all_preds.append(batch_valid_preds)\n",
    "                \n",
    "            loss_history.append(epoch_loss)\n",
    "                        \n",
    "            # Logging every 400 epochs\n",
    "            if epoch % 400 == 0:\n",
    "                print('Epoch', epoch)\n",
    "                print(f'Sampling probability: {self.sampling_probability_list[epoch]:.3f}')\n",
    "                for input_batch, target_batch, batch_preds in zip(input_batches, target_batches, all_preds):\n",
    "                    for input_sent, target_sent, pred in zip(input_batch, target_batch, batch_preds):\n",
    "                        print(f'\\tInput: {input_sent}')\n",
    "                        print(f'\\tPrediction:', idx2sent(pred, reverse_vocab=dec_reverse_vocab))\n",
    "                        print(f'\\tTarget: {target_sent}\\n')\n",
    "                print(f'\\tepoch loss: {epoch_loss:.2f}\\n')\n",
    "                \n",
    "        if save_path:\n",
    "            self.save(sess, save_path=save_path)\n",
    "\n",
    "        return loss_history\n",
    "    \n",
    "    def inference(self, sess, data, load_ckpt):\n",
    "\n",
    "        self.restore(sess, ckpt_path=load_ckpt)\n",
    "                \n",
    "        input_batch, target_batch = data\n",
    "        \n",
    "        batch_preds = []\n",
    "        batch_tokens = []\n",
    "        batch_sent_lens = []\n",
    "\n",
    "        for input_sent in input_batch:\n",
    "            tokens, sent_len = sent2idx(input_sent)\n",
    "            batch_tokens.append(tokens)\n",
    "            batch_sent_lens.append(sent_len)\n",
    "            \n",
    "        batch_preds = sess.run(\n",
    "            self.predictions,\n",
    "            feed_dict={\n",
    "                self.enc_inputs: batch_tokens,\n",
    "                self.enc_sequence_length: batch_sent_lens,\n",
    "            })\n",
    "\n",
    "        for input_sent, target_sent, pred in zip(input_batch, target_batch, batch_preds):\n",
    "            print('Input:', input_sent)\n",
    "            print('Prediction:', idx2sent(pred, reverse_vocab=dec_reverse_vocab))\n",
    "            print('Target:', target_sent, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if models are sucessufully built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model built!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = DemoConfig()\n",
    "model = Seq2SeqModel(config, mode='training')\n",
    "model.build()\n",
    "# model.summary()\n",
    "print('Training model built!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference model built!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = DemoConfig()\n",
    "model = Seq2SeqModel(config, mode='inference')\n",
    "model.build()\n",
    "# model.summary()\n",
    "print('Inference model built!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "  1%|          | 6/801 [00:00<05:16,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Sampling probability: 0.000\n",
      "\tInput: Hi What is your name?\n",
      "\tPrediction: brown brown too too too too _GO\n",
      "\tTarget: Hi this is Jaemin.\n",
      "\n",
      "\tInput: Nice to meet you!\n",
      "\tPrediction: to too too too too too too\n",
      "\tTarget: Nice to meet you too!\n",
      "\n",
      "\tInput: Which programming language do you use?\n",
      "\tPrediction: brown too too too too\n",
      "\tTarget: I like Python.\n",
      "\n",
      "\tInput: See you later.\n",
      "\tPrediction: too too too too _GO\n",
      "\tTarget: Bye Bye.\n",
      "\n",
      "\tInput: Where do you live?\n",
      "\tPrediction: too too too too too too too too too\n",
      "\tTarget: I live in Seoul, South Korea.\n",
      "\n",
      "\tInput: What is your major?\n",
      "\tPrediction: too too too too too too _GO _GO _GO\n",
      "\tTarget: I study industrial engineering.\n",
      "\n",
      "\tInput: What do you want to drink?\n",
      "\tPrediction: Hi brown too too\n",
      "\tTarget: Beer please!\n",
      "\n",
      "\tInput: What is your favorite beer?\n",
      "\tPrediction: Hi too too too\n",
      "\tTarget: Leffe brown!\n",
      "\n",
      "\tepoch loss: 13.89\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 405/801 [00:10<00:09, 41.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400\n",
      "Sampling probability: 0.500\n",
      "\tInput: Hi What is your name?\n",
      "\tPrediction: _GO Hi this is Jaemin . _GO\n",
      "\tTarget: Hi this is Jaemin.\n",
      "\n",
      "\tInput: Nice to meet you!\n",
      "\tPrediction: _GO Nice to meet you too !\n",
      "\tTarget: Nice to meet you too!\n",
      "\n",
      "\tInput: Which programming language do you use?\n",
      "\tPrediction: _GO I like Python .\n",
      "\tTarget: I like Python.\n",
      "\n",
      "\tInput: See you later.\n",
      "\tPrediction: _GO Bye Bye . _GO\n",
      "\tTarget: Bye Bye.\n",
      "\n",
      "\tInput: Where do you live?\n",
      "\tPrediction: _GO I live in Seoul , South Korea .\n",
      "\tTarget: I live in Seoul, South Korea.\n",
      "\n",
      "\tInput: What is your major?\n",
      "\tPrediction: _GO I study industrial engineering . _GO _GO _GO\n",
      "\tTarget: I study industrial engineering.\n",
      "\n",
      "\tInput: What do you want to drink?\n",
      "\tPrediction: _GO Beer please !\n",
      "\tTarget: Beer please!\n",
      "\n",
      "\tInput: What is your favorite beer?\n",
      "\tPrediction: _GO Leffe brown !\n",
      "\tTarget: Leffe brown!\n",
      "\n",
      "\tepoch loss: 0.07\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 801/801 [00:20<00:00, 43.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800\n",
      "Sampling probability: 1.000\n",
      "\tInput: Hi What is your name?\n",
      "\tPrediction: _GO Hi this is Jaemin . _GO\n",
      "\tTarget: Hi this is Jaemin.\n",
      "\n",
      "\tInput: Nice to meet you!\n",
      "\tPrediction: _GO Nice to meet you too !\n",
      "\tTarget: Nice to meet you too!\n",
      "\n",
      "\tInput: Which programming language do you use?\n",
      "\tPrediction: _GO I like Python .\n",
      "\tTarget: I like Python.\n",
      "\n",
      "\tInput: See you later.\n",
      "\tPrediction: _GO Bye Bye . _GO\n",
      "\tTarget: Bye Bye.\n",
      "\n",
      "\tInput: Where do you live?\n",
      "\tPrediction: _GO I live in Seoul , South Korea .\n",
      "\tTarget: I live in Seoul, South Korea.\n",
      "\n",
      "\tInput: What is your major?\n",
      "\tPrediction: _GO I study industrial engineering . _GO _GO _GO\n",
      "\tTarget: I study industrial engineering.\n",
      "\n",
      "\tInput: What do you want to drink?\n",
      "\tPrediction: _GO Beer please !\n",
      "\tTarget: Beer please!\n",
      "\n",
      "\tInput: What is your favorite beer?\n",
      "\tPrediction: _GO Leffe brown !\n",
      "\tTarget: Leffe brown!\n",
      "\n",
      "\tepoch loss: 0.00\n",
      "\n",
      "Saving model at ./ckpt_dir/epoch_801_sampling\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()     \n",
    "with tf.Session() as sess:\n",
    "    config = DemoConfig()\n",
    "    model = Seq2SeqModel(config, mode='training')\n",
    "    model.build()\n",
    "    data = (input_batches, target_batches)\n",
    "    loss_history = model.train(sess, data, from_scratch=True, save_path=model.ckpt_dir+f'epoch_{model.n_epoch}_sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABI8AAAJcCAYAAABwj4S5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(range(model.n_epoch), loss_history)\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Global step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore Finished!\n",
      "Input: Hi What is your name?\n",
      "Prediction: _GO Hi this is Jaemin . . . . .\n",
      "Target: Hi this is Jaemin. \n",
      "\n",
      "Input: Nice to meet you!\n",
      "Prediction: _GO Nice to meet you too ! ! ! !\n",
      "Target: Nice to meet you too! \n",
      "\n",
      "Restore Finished!\n",
      "Input: Which programming language do you use?\n",
      "Prediction: _GO I like Python . . . . . .\n",
      "Target: I like Python. \n",
      "\n",
      "Input: See you later.\n",
      "Prediction: _GO Bye Bye . . . . . . .\n",
      "Target: Bye Bye. \n",
      "\n",
      "Restore Finished!\n",
      "Input: Where do you live?\n",
      "Prediction: _GO I live in Seoul , South Korea . .\n",
      "Target: I live in Seoul, South Korea. \n",
      "\n",
      "Input: What is your major?\n",
      "Prediction: _GO I study industrial engineering . . . . .\n",
      "Target: I study industrial engineering. \n",
      "\n",
      "Restore Finished!\n",
      "Input: What do you want to drink?\n",
      "Prediction: _GO Beer please ! ! ! ! ! ! !\n",
      "Target: Beer please! \n",
      "\n",
      "Input: What is your favorite beer?\n",
      "Prediction: _GO Leffe brown ! ! ! ! ! ! !\n",
      "Target: Leffe brown! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    config = DemoConfig()\n",
    "    model = Seq2SeqModel(config, mode='inference')\n",
    "    model.build()\n",
    "    for input_batch, target_batch in zip(input_batches, target_batches):\n",
    "        data = (input_batch, target_batch)\n",
    "        model.inference(sess, data, load_ckpt=model.ckpt_dir+f'epoch_{model.n_epoch}_sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better performance than without scheduled sampling!\n",
    "- A word of caution: http://www.inference.vc/scheduled-sampling-for-rnns-scoring-rule-interpretation/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
