{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jpype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a10ecde8e164>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjpype\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdrop_pos_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'xu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'xx'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'yg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wh'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wky'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wkz'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wp'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ws'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wyy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wyz'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'u'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ud'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ude1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ude2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ude3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'udeng'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'udh'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jpype'"
     ]
    }
   ],
   "source": [
    "#encoding=utf8\n",
    "import os,gc,re,sys\n",
    "from itertools import chain\n",
    "from jpype import *\n",
    "drop_pos_set=set(['xu','xx','y','yg','wh','wky','wkz','wp','ws','wyy','wyz','wb','u','ud','ude1','ude2','ude3','udeng','udh'])\n",
    "\n",
    "djclass_path=\"-Djava.class.path=\"+\"/home/kuo/NLP/module\"+os.sep+\"hanlp\"+os.sep+\"hanlp-1.6.2.jar:\"+\"/home/kuo/NLP/module\"+os.sep+\"hanlp\"\n",
    "startJVM(getDefaultJVMPath(),djclass_path,\"-Xms1g\",\"-Xmx1g\")\n",
    "Tokenizer = JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer')\n",
    "def to_string(sentence,return_generator=False):\n",
    "    if return_generator:\n",
    "        return (word_pos_item.toString().split('/') for word_pos_item in Tokenizer.segment(sentence))\n",
    "    else:\n",
    "        return [(word_pos_item.toString().split('/')[0],word_pos_item.toString().split('/')[1]) for word_pos_item in Tokenizer.segment(sentence)]   \n",
    "def to_string_hanlp(sentence,return_generator=False):\n",
    "    if return_generator:\n",
    "        return (word_pos_item.toString().split('/') for word_pos_item in HanLP.segment(sentence))\n",
    "    else:\n",
    "        return [(word_pos_item.toString().split('/')[0],word_pos_item.toString().split('/')[1]) for word_pos_item in Tokenizer.segment(sentence)]      \n",
    "def seg_sentences(sentence,with_filter=True,return_generator=False):  \n",
    "    segs=to_string(sentence,return_generator=return_generator)\n",
    "    if with_filter:\n",
    "        g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair)==2 and word_pos_pair[0]!=' ' and word_pos_pair[1] not in drop_pos_set]\n",
    "    else:\n",
    "        g = [word_pos_pair[0] for word_pos_pair in segs if len(word_pos_pair)==2 and word_pos_pair[0]!=' ']\n",
    "    return iter(g) if return_generator else g\n",
    "def cut_hanlp(raw_sentence,return_list=True):\n",
    "    if len(raw_sentence.strip())>0:\n",
    "        return to_string(raw_sentence) if return_list else iter(to_string(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'seg_sentences'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-dd72cdb28bfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseg_sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'seg_sentences'"
     ]
    }
   ],
   "source": [
    "#encoding=utf8\n",
    "import json,re\n",
    "import numpy \n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "import os\n",
    "import time\n",
    "#from tokenizer import seg_sentences\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "\n",
    "pattern=re.compile(u'[^a-zA-Z\\u4E00-\\u9FA5]')\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, numpy.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, numpy.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, numpy.ndarray):\n",
    "            return obj.tolist()        \n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "def _replace_c(text):\n",
    "    intab = \",?!\"\n",
    "    outtab = \"，？！\"    \n",
    "    deltab = \")(+_-.>< \"\n",
    "    trantab=text.maketrans(intab, outtab,deltab)\n",
    "    return text.translate(trantab)\n",
    "def remove_phase(phase_list):\n",
    "    remove_phase=\"aa,ab,abc,ad,ao,az,a写字楼,a区,a地块,a客户,a施工方,a系列,a项目,a系统\"\n",
    "    remove_phase_set=set(remove_phase.split(\",\"))\n",
    "    phase_list_set=set(phase_list)\n",
    "    phase_list_set.difference(remove_phase_set)\n",
    "    return list(phase_list_set)\n",
    "\n",
    "def get_words(sentence):     \n",
    "\n",
    "    segs = (word_pos_item.toString().split('/') for word_pos_item in StandardTokenizer.segment(sentence))\n",
    "    segs = (tuple(word_pos_pair) for word_pos_pair in segs if len(word_pos_pair)==2)\n",
    "    segs = ((word.strip(),pos) for word, pos in segs if pos  in keep_pos_set)\n",
    "    segs = ((word, pos) for word, pos in segs if word and not pattern.search(word))\n",
    "    result = ' '.join(w for w,pos in segs if len(w)>0)    \n",
    "    return result\n",
    "def get_words_no_space(sentence):       \n",
    "    segs = (word_pos_item.toString().split('/') for word_pos_item in StandardTokenizer.segment(sentence))\n",
    "    segs = (tuple(word_pos_pair) for word_pos_pair in segs if len(word_pos_pair)==2)\n",
    "    segs = ((word.strip(),pos) for word, pos in segs if pos  in keep_pos_set)\n",
    "    segs = ((word, pos) for word, pos in segs if word and not pattern.search(word))\n",
    "    result = [w for w,pos in segs]  \n",
    "    return result\n",
    " \n",
    "def tokenize_raw(text):\n",
    "    split_sen=(i.strip() for i in re.split('。|,|，|：|:|？|！|\\t|\\n',_replace_c(text)) if len(i.strip())>5)\n",
    "    return [get_words(sentence) for sentence in split_sen]  \n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return [tok.strip() for tok in text.split(\" \")]\n",
    "def tokenize_no_space(text):\n",
    "    return [get_words_no_space( text)]\n",
    "def tokenize_triple(text):\n",
    "    return \"_\".join([ tok for tok in text.split(\" \")])\n",
    "def pro(text):\n",
    "    fout=open(\"triple.txt\", \"w\", encoding='utf-8')\n",
    "    vectorize=CountVectorizer(input='content', encoding='utf-8', decode_error='strict', \n",
    "                              strip_accents=None, lowercase=True, \n",
    "                   preprocessor=None, tokenizer=tokenize, \n",
    "                   stop_words=None, \n",
    "                   token_pattern=r\"[a-zA-Z\\u4E00-\\u9FA5]\", \n",
    "                   ngram_range=(3,3), analyzer='word', max_df=0.7, \n",
    "                   min_df=50, max_features=None, vocabulary=None, \n",
    "                   binary=False, dtype=np.int64)\n",
    "    freq=vectorize.fit(text)\n",
    "    vectorizer1=CountVectorizer(max_df=0.7, \n",
    "                                min_df=50,tokenizer=None)\n",
    "    freq1=vectorizer1.fit_transform(('_'.join(i.split(\" \")) for i in freq.vocabulary_.keys()))   \n",
    "    transformer=TfidfTransformer()#该类会统计每个词语的tf-idf权值  \n",
    "    word_freq=(freq1[:][i].sum() for i in range(freq1.shape[1]))\n",
    "    tfidf=transformer.fit_transform(freq1)#第一个fit_transform是计算tf-idf，第二个                \n",
    "    tfidf_sum=(tfidf[:][i].sum() for i in range(tfidf.shape[1]))\n",
    "    tfidf_dic=vectorizer1.get_feature_names()\n",
    "    \n",
    "    dic_filter={}\n",
    "    \n",
    "    def _add(wq,tf,i):\n",
    "        dic_filter[tfidf_dic[i]]=[wq,tf]\n",
    "    for i,(word_freq_one,w_one) in enumerate(zip(word_freq,tfidf_sum)):\n",
    "        _add(word_freq_one, w_one, i)\n",
    "    sort_dic=dict(sorted(dic_filter.items(),key=lambda val:val[1],reverse=False))#,reverse=False为降序排列,返回list\n",
    "    fout.write(json.dumps(sort_dic, ensure_ascii=False,cls=NumpyEncoder)+\"\\n\")               \n",
    "    fout.close()    \n",
    "def gen_dic(in_path,save_path):\n",
    "    fp=open(in_path,'r',encoding='utf-8')\n",
    "    fout=open(save_path,'w',encoding='utf-8')\n",
    "    \n",
    "    copus=[list(json.loads(line).keys()) for line in fp]\n",
    "    copus=[''.join(ph.split(\"_\")) for phase in copus for ph in phase] \n",
    "    copus=remove_phase(copus)\n",
    "    for i in copus:\n",
    "        fout.write(i+\" \"+\"nresume\"+\" \"+str(10)+\"\\n\")\n",
    "    fout.close()\n",
    "  #  fout.write()\n",
    "def remove_n(text):\n",
    "    intab = \"\"\n",
    "    outtab = \"\"    \n",
    "    deltab = \"\\n\\t \"\n",
    "    trantab=text.maketrans(intab, outtab,deltab)\n",
    "    return text.translate(trantab)\n",
    "\n",
    "def generate_ngram(sentence, n=4, m=2):           # 生成n-gram\n",
    "    if len(sentence) < n:\n",
    "        n = len(sentence)\n",
    "    temp=[tuple(sentence[i - k:i]) for k in range(m, n + 1) for i in range(k, len(sentence) + 1) ]                       # 生成2个或者3个的gram\n",
    "    return [item for item in temp if len(''.join(item).strip())>1 and len(pattern.findall(''.join(item).strip()))==0]    # 去掉非字母汉字的符号\n",
    "  \n",
    "if __name__==\"__main__\":\n",
    "    # 分字进行n-gram\n",
    "    copus_character=[generate_ngram(line.strip())  for line  in open('text.txt','r',encoding='utf8') if len(line.strip())>0 and \"RESUMEDOCSSTARTFLAG\" not in line]    \n",
    "    # 先用hanlp分词，在对词进行n-gram\n",
    "    copus_word=[generate_ngram(seg_sentences(line.strip(),with_filter=True) ) for line  in open('text.txt','r',encoding='utf8') if len(line.strip())>0 and \"RESUMEDOCSSTARTFLAG\" not in line]\n",
    "    copus_word=chain.from_iterable(copus_word)\n",
    "    copus_word=['_'.join(item) for item in copus_word]\n",
    "    fout=open(\"ngram2_3.txt\", \"w\", encoding='utf-8')\n",
    "    \n",
    "    dic_filter={}                     # 统计词频\n",
    "    for item in copus_word:\n",
    "        if item in dic_filter:\n",
    "            dic_filter[item]+=1\n",
    "        else:\n",
    "            dic_filter[item]=1\n",
    "    sort_dic=dict(sorted(dic_filter.items(),key=lambda val:val[1],reverse=True))       #reverse=True为降序排列,返回list\n",
    "    fout.write(json.dumps(sort_dic, ensure_ascii=False,cls=NumpyEncoder))   \n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
