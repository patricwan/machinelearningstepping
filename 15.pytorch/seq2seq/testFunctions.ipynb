{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 169657 sentence pairs\n",
      "Trimmed to 12658 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "en 4731\n",
      "fr 3045\n",
      "['je songe a acheter un nouveau parasol .', 'i m thinking about buying a new parasol .']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('../../data/nlp/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('fr', 'en', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3],\n",
      "        [6],\n",
      "        [9]])\n",
      "tensor([[[-0.9823, -0.5065,  0.0998, -0.6540,  0.7317]],\n",
      "\n",
      "        [[ 0.2911,  1.9907,  0.6614,  1.1899,  0.8165]],\n",
      "\n",
      "        [[-0.6200, -1.4782, -1.1334,  0.8738, -0.5603]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(60, 5)\n",
    "indexes = [3,6,9]\n",
    "torchIndex = torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "print(torchIndex)\n",
    "print(embedding(torchIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor tensor([[   2],\n",
      "        [2306],\n",
      "        [1054],\n",
      "        [ 303],\n",
      "        [3098],\n",
      "        [ 229],\n",
      "        [2095],\n",
      "        [   5],\n",
      "        [   1]])\n",
      "target tensor tensor([[   2],\n",
      "        [   3],\n",
      "        [ 819],\n",
      "        [2533],\n",
      "        [1175],\n",
      "        [  16],\n",
      "        [1849],\n",
      "        [   4],\n",
      "        [   1]])\n",
      "input tensor tensor([[   6],\n",
      "        [ 337],\n",
      "        [   7],\n",
      "        [ 281],\n",
      "        [ 118],\n",
      "        [2931],\n",
      "        [   5],\n",
      "        [   1]])\n",
      "target tensor tensor([[   2],\n",
      "        [   3],\n",
      "        [ 163],\n",
      "        [1730],\n",
      "        [ 745],\n",
      "        [   4],\n",
      "        [   1]])\n",
      "input tensor tensor([[153],\n",
      "        [118],\n",
      "        [236],\n",
      "        [606],\n",
      "        [  5],\n",
      "        [  1]])\n",
      "target tensor tensor([[ 90],\n",
      "        [ 91],\n",
      "        [171],\n",
      "        [ 73],\n",
      "        [  4],\n",
      "        [  1]])\n",
      "input tensor tensor([[  25],\n",
      "        [  79],\n",
      "        [ 730],\n",
      "        [  53],\n",
      "        [1509],\n",
      "        [   5],\n",
      "        [   1]])\n",
      "target tensor tensor([[ 15],\n",
      "        [ 47],\n",
      "        [402],\n",
      "        [561],\n",
      "        [937],\n",
      "        [  4],\n",
      "        [  1]])\n",
      "input tensor tensor([[  25],\n",
      "        [  26],\n",
      "        [  78],\n",
      "        [ 276],\n",
      "        [1970],\n",
      "        [   5],\n",
      "        [   1]])\n",
      "target tensor tensor([[  15],\n",
      "        [  16],\n",
      "        [  51],\n",
      "        [ 174],\n",
      "        [1078],\n",
      "        [   4],\n",
      "        [   1]])\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "n_iters = 5 \n",
    "\n",
    "training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    training_pair = training_pairs[iter - 1]\n",
    "    input_tensor = training_pair[0]\n",
    "    target_tensor = training_pair[1]\n",
    "    print(\"input tensor\", input_tensor)\n",
    "    print(\"target tensor\",target_tensor)\n",
    "    \n",
    "print(input_tensor.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 5, 7, 8, 4, 6])\n",
      "tensor([[1, 5, 7, 8, 4, 6]])\n"
     ]
    }
   ],
   "source": [
    "indexes = [1,5,7,8,4, 6]\n",
    "torchIndex = torch.tensor(indexes, dtype=torch.long, device=device)\n",
    "print(torchIndex)\n",
    "torchIndex = torchIndex.view(1, -1)\n",
    "print(torchIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 5, 6])\n",
      "tensor([[[0.4040],\n",
      "         [0.0765],\n",
      "         [0.8860],\n",
      "         [0.3710],\n",
      "         [0.1769]],\n",
      "\n",
      "        [[0.8206],\n",
      "         [0.6776],\n",
      "         [0.6237],\n",
      "         [0.4009],\n",
      "         [0.3440]],\n",
      "\n",
      "        [[0.5848],\n",
      "         [0.8869],\n",
      "         [0.5716],\n",
      "         [0.2636],\n",
      "         [0.9967]],\n",
      "\n",
      "        [[0.5107],\n",
      "         [0.8953],\n",
      "         [0.6219],\n",
      "         [0.9482],\n",
      "         [0.7885]]])\n",
      "torch.Size([4, 5, 1])\n",
      "tensor([[[[0.4040]],\n",
      "\n",
      "         [[0.0765]],\n",
      "\n",
      "         [[0.8860]],\n",
      "\n",
      "         [[0.3710]],\n",
      "\n",
      "         [[0.1769]]],\n",
      "\n",
      "\n",
      "        [[[0.8206]],\n",
      "\n",
      "         [[0.6776]],\n",
      "\n",
      "         [[0.6237]],\n",
      "\n",
      "         [[0.4009]],\n",
      "\n",
      "         [[0.3440]]],\n",
      "\n",
      "\n",
      "        [[[0.5848]],\n",
      "\n",
      "         [[0.8869]],\n",
      "\n",
      "         [[0.5716]],\n",
      "\n",
      "         [[0.2636]],\n",
      "\n",
      "         [[0.9967]]],\n",
      "\n",
      "\n",
      "        [[[0.5107]],\n",
      "\n",
      "         [[0.8953]],\n",
      "\n",
      "         [[0.6219]],\n",
      "\n",
      "         [[0.9482]],\n",
      "\n",
      "         [[0.7885]]]])\n",
      "torch.Size([4, 5, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.tensor([3,5,6])\n",
    "print(x)\n",
    "y = torch.rand(4,5,1)\n",
    "print(y)\n",
    "print(y.shape)\n",
    "y = y.unsqueeze(-1)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617],\n",
      "        [ 0.6213, -0.4519, -0.1661]]) tensor([[-1.5228,  0.3817, -1.0276]])\n",
      "tensor([[ 0.6614,  0.2669,  0.0617],\n",
      "        [ 0.6213, -0.4519, -0.1661]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x = torch.randn(2,3)\n",
    "\n",
    "y = torch.randn(1,3)\n",
    "\n",
    "print(x,y)\n",
    "\n",
    "a=torch.rand((1,2))\n",
    "b=torch.rand((1,2))\n",
    "\n",
    "c=torch.stack((a,b),0)\n",
    "\n",
    "c.size()\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x = torch.randn(2,3)\n",
    "\n",
    "print(x)\n",
    "\n",
    "x.transpose(0,1)\n",
    "\n",
    "x = torch.randn(2,3,4)\n",
    "\n",
    "print(x.size())\n",
    "\n",
    "x_p = x.permute(1,0,2) \n",
    "\n",
    "b = torch.Tensor(2,1)\n",
    "b.shape\n",
    "Out[28]: torch.Size([2, 1])\n",
    "\n",
    "# 不加参数，去掉所有为元素个数为1的维度\n",
    "b_ = b.squeeze()\n",
    "b_.shape\n",
    "Out[30]: torch.Size([2])\n",
    "\n",
    "# 加上参数，去掉第一维的元素为1，不起作用，因为第一维有2个元素\n",
    "b_ = b.squeeze(0)\n",
    "b_.shape \n",
    "Out[32]: torch.Size([2, 1])\n",
    "\n",
    "# 这样就可以了\n",
    "b_ = b.squeeze(1)\n",
    "b_.shape\n",
    "Out[34]: torch.Size([2])\n",
    "\n",
    "# 增加一个维度\n",
    "b_ = b.unsqueeze(2)\n",
    "b_.shape\n",
    "Out[36]: torch.Size([2, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor(233.)\n",
      "tensor([[233.0000,   0.4107,  -0.9081],\n",
      "        [  2.5286,  -0.9880,   0.5423]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 2)\n",
    "y = x.transpose (0, 1)\n",
    "x[0, 0] = 233\n",
    "print(y.shape)\n",
    "print(y[0, 0])\n",
    "\n",
    "y = y.contiguous()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.06346652 0.12693304 0.19039955 0.25386607 0.31733259\n",
      " 0.38079911 0.44426563 0.50773215 0.57119866 0.63466518 0.6981317\n",
      " 0.76159822 0.82506474 0.88853126 0.95199777 1.01546429 1.07893081\n",
      " 1.14239733 1.20586385 1.26933037 1.33279688 1.3962634  1.45972992\n",
      " 1.52319644 1.58666296 1.65012947 1.71359599 1.77706251 1.84052903\n",
      " 1.90399555 1.96746207 2.03092858 2.0943951  2.15786162 2.22132814\n",
      " 2.28479466 2.34826118 2.41172769 2.47519421 2.53866073 2.60212725\n",
      " 2.66559377 2.72906028 2.7925268  2.85599332 2.91945984 2.98292636\n",
      " 3.04639288 3.10985939 3.17332591 3.23679243 3.30025895 3.36372547\n",
      " 3.42719199 3.4906585  3.55412502 3.61759154 3.68105806 3.74452458\n",
      " 3.8079911  3.87145761 3.93492413 3.99839065 4.06185717 4.12532369\n",
      " 4.1887902  4.25225672 4.31572324 4.37918976 4.44265628 4.5061228\n",
      " 4.56958931 4.63305583 4.69652235 4.75998887 4.82345539 4.88692191\n",
      " 4.95038842 5.01385494 5.07732146 5.14078798 5.2042545  5.26772102\n",
      " 5.33118753 5.39465405 5.45812057 5.52158709 5.58505361 5.64852012\n",
      " 5.71198664 5.77545316 5.83891968 5.9023862  5.96585272 6.02931923\n",
      " 6.09278575 6.15625227 6.21971879 6.28318531]\n",
      "[ 0.00000000e+00  6.34239197e-02  1.26592454e-01  1.89251244e-01\n",
      "  2.51147987e-01  3.12033446e-01  3.71662456e-01  4.29794912e-01\n",
      "  4.86196736e-01  5.40640817e-01  5.92907929e-01  6.42787610e-01\n",
      "  6.90079011e-01  7.34591709e-01  7.76146464e-01  8.14575952e-01\n",
      "  8.49725430e-01  8.81453363e-01  9.09631995e-01  9.34147860e-01\n",
      "  9.54902241e-01  9.71811568e-01  9.84807753e-01  9.93838464e-01\n",
      "  9.98867339e-01  9.99874128e-01  9.96854776e-01  9.89821442e-01\n",
      "  9.78802446e-01  9.63842159e-01  9.45000819e-01  9.22354294e-01\n",
      "  8.95993774e-01  8.66025404e-01  8.32569855e-01  7.95761841e-01\n",
      "  7.55749574e-01  7.12694171e-01  6.66769001e-01  6.18158986e-01\n",
      "  5.67059864e-01  5.13677392e-01  4.58226522e-01  4.00930535e-01\n",
      "  3.42020143e-01  2.81732557e-01  2.20310533e-01  1.58001396e-01\n",
      "  9.50560433e-02  3.17279335e-02 -3.17279335e-02 -9.50560433e-02\n",
      " -1.58001396e-01 -2.20310533e-01 -2.81732557e-01 -3.42020143e-01\n",
      " -4.00930535e-01 -4.58226522e-01 -5.13677392e-01 -5.67059864e-01\n",
      " -6.18158986e-01 -6.66769001e-01 -7.12694171e-01 -7.55749574e-01\n",
      " -7.95761841e-01 -8.32569855e-01 -8.66025404e-01 -8.95993774e-01\n",
      " -9.22354294e-01 -9.45000819e-01 -9.63842159e-01 -9.78802446e-01\n",
      " -9.89821442e-01 -9.96854776e-01 -9.99874128e-01 -9.98867339e-01\n",
      " -9.93838464e-01 -9.84807753e-01 -9.71811568e-01 -9.54902241e-01\n",
      " -9.34147860e-01 -9.09631995e-01 -8.81453363e-01 -8.49725430e-01\n",
      " -8.14575952e-01 -7.76146464e-01 -7.34591709e-01 -6.90079011e-01\n",
      " -6.42787610e-01 -5.92907929e-01 -5.40640817e-01 -4.86196736e-01\n",
      " -4.29794912e-01 -3.71662456e-01 -3.12033446e-01 -2.51147987e-01\n",
      " -1.89251244e-01 -1.26592454e-01 -6.34239197e-02 -2.44929360e-16]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TIME_STEP = 10\n",
    "\n",
    "steps = np.linspace(0, np.pi*2, 100, dtype=np.float)\n",
    "\n",
    "print(steps)\n",
    "x1_np = np.sin(steps)\n",
    "x2_np = np.cos(steps)\n",
    "x3_np = np.tan(steps)\n",
    "\n",
    "print(x1_np)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_seq_len = 15\n",
    "output_seq_len = 15\n",
    "batch_size = 1200\n",
    "\n",
    "\n",
    "input_dim = 3\n",
    "output_dim = 3\n",
    "x = np.linspace(0, 40, 130)\n",
    "\n",
    "def true_output_signals(x):\n",
    "    x1 = 2 * np.sin(x)\n",
    "    x2 = 3 * np.cos(x)\n",
    "    x3 = 4 * np.tan(x)\n",
    "    #x4 = 5 * np.log(x+2)\n",
    "\n",
    "    return x1, x2, x3  #, x4\n",
    "\n",
    "def true_input_signals(x):\n",
    "    x1, x2, x3 = true_output_signals(x) #, x4\n",
    "    \n",
    "    y1 = 1.6*x1**4 - 2*x2 - 10 * x3\n",
    "    y2 = 1.2*x2**2 * x1 + 2*x2*3 - x1*6   #+ x4 *5\n",
    "    y3 = 2*x1**3 + 2*x2**3 - x1*x2 - x3   #+ 2* x4\n",
    "    \n",
    "    return y1, y2, y3\n",
    "\n",
    "def noise_func(x, noise_factor = 2):\n",
    "    return np.random.randn(len(x)) * noise_factor\n",
    "\n",
    "def generate_samples_for_output(x):\n",
    "    x1, x2, x3 = true_output_signals(x)   #, x4 \n",
    "    return x1+noise_func(x1, 0.5), \\\n",
    "           x2+noise_func(x2, 0.5), \\\n",
    "           x3+noise_func(x3, 0.5)  #, \\\n",
    "           #x4+noise_func(x4, 0.5)\n",
    "\n",
    "def generate_samples_for_input(x):\n",
    "    y1, y2, y3 = true_input_signals(x)\n",
    "    return y1+noise_func(y1, 2), \\\n",
    "           y2+noise_func(y2, 2), \\\n",
    "           y3+noise_func(y3, 2)\n",
    "\n",
    "def generate_train_samples(x = train_data_x, batch_size = 10):\n",
    "\n",
    "    total_start_points = len(x) - input_seq_len - output_seq_len\n",
    "    start_x_idx = np.random.choice(range(total_start_points), batch_size)\n",
    "    \n",
    "    input_seq_x = [x[i:(i+input_seq_len)] for i in start_x_idx]\n",
    "    output_seq_x = [x[(i+input_seq_len):(i+input_seq_len+output_seq_len)] for i in start_x_idx]\n",
    "    \n",
    "    input_seq_y = [generate_samples_for_input(x) for x in input_seq_x]\n",
    "    output_seq_y = [generate_samples_for_output(x) for x in output_seq_x]\n",
    "    \n",
    "    ## return shape: (batch_size, time_steps, feature_dims)\n",
    "    return np.array(input_seq_y).transpose(0, 2, 1), np.array(output_seq_y).transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 15, 3)\n",
      "(1200, 15, 3)\n"
     ]
    }
   ],
   "source": [
    "input_seq, output_seq = generate_train_samples(batch_size=batch_size)\n",
    "print(input_seq.shape)\n",
    "print(output_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=150, model_type=\"GRU\"):\n",
    "    \n",
    "    # Setting common hyperparameters\n",
    "    #batchData = next(iter(train_loader))\n",
    "    #print(\"batch shape\" , batchData[0].shape)\n",
    "    #input_dim = batchData[0].shape[2]\n",
    "    #output_dim = input_dim\n",
    "    n_layers = 1\n",
    "    # Instantiating the models\n",
    "    if model_type == \"GRU\":\n",
    "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    else:\n",
    "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.clock()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            counter += 1\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            else:\n",
    "                h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            if counter%200 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.clock()\n",
    "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
    "        print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python36/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/usr/local/python36/lib/python3.6/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([15, 15, 3])) that is different to the input size (torch.Size([15, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n",
      "Epoch 1/150 Done, Total Loss: 4813.04262008667\n",
      "Total Time Elapsed: 2.5499999999999545 seconds\n",
      "Epoch 2/150 Done, Total Loss: 4800.480706977844\n",
      "Total Time Elapsed: 2.4300000000000637 seconds\n",
      "Epoch 3/150 Done, Total Loss: 4791.738552665711\n",
      "Total Time Elapsed: 2.4299999999998363 seconds\n",
      "Epoch 4/150 Done, Total Loss: 4784.153258895874\n",
      "Total Time Elapsed: 2.5300000000002 seconds\n",
      "Epoch 5/150 Done, Total Loss: 4778.186190319061\n",
      "Total Time Elapsed: 2.339999999999918 seconds\n",
      "Epoch 6/150 Done, Total Loss: 4773.815562057495\n",
      "Total Time Elapsed: 2.25 seconds\n",
      "Epoch 7/150 Done, Total Loss: 4769.625167274475\n",
      "Total Time Elapsed: 2.349999999999909 seconds\n",
      "Epoch 8/150 Done, Total Loss: 4767.494065856934\n",
      "Total Time Elapsed: 2.4200000000000728 seconds\n",
      "Epoch 9/150 Done, Total Loss: 4765.21652765274\n",
      "Total Time Elapsed: 2.5299999999999727 seconds\n",
      "Epoch 10/150 Done, Total Loss: 4763.81964635849\n",
      "Total Time Elapsed: 2.3299999999999272 seconds\n",
      "Epoch 11/150 Done, Total Loss: 4762.582493305206\n",
      "Total Time Elapsed: 2.4600000000000364 seconds\n",
      "Epoch 12/150 Done, Total Loss: 4761.339971351624\n",
      "Total Time Elapsed: 2.3200000000001637 seconds\n",
      "Epoch 13/150 Done, Total Loss: 4760.809061813355\n",
      "Total Time Elapsed: 2.2999999999999545 seconds\n",
      "Epoch 14/150 Done, Total Loss: 4760.500255393982\n",
      "Total Time Elapsed: 2.4600000000000364 seconds\n",
      "Epoch 15/150 Done, Total Loss: 4760.178101921081\n",
      "Total Time Elapsed: 2.0599999999999454 seconds\n",
      "Epoch 16/150 Done, Total Loss: 4760.084245681763\n",
      "Total Time Elapsed: 2.589999999999918 seconds\n",
      "Epoch 17/150 Done, Total Loss: 4759.386826705932\n",
      "Total Time Elapsed: 2.740000000000009 seconds\n",
      "Epoch 18/150 Done, Total Loss: 4759.122066688537\n",
      "Total Time Elapsed: 2.410000000000082 seconds\n",
      "Epoch 19/150 Done, Total Loss: 4759.722240447998\n",
      "Total Time Elapsed: 2.3999999999998636 seconds\n",
      "Epoch 20/150 Done, Total Loss: 4759.433526802063\n",
      "Total Time Elapsed: 2.4700000000000273 seconds\n",
      "Epoch 21/150 Done, Total Loss: 4758.72195968628\n",
      "Total Time Elapsed: 2.6200000000001182 seconds\n",
      "Epoch 22/150 Done, Total Loss: 4759.092517471314\n",
      "Total Time Elapsed: 2.4700000000000273 seconds\n",
      "Epoch 23/150 Done, Total Loss: 4758.85975227356\n",
      "Total Time Elapsed: 2.5399999999999636 seconds\n",
      "Epoch 24/150 Done, Total Loss: 4758.904010009765\n",
      "Total Time Elapsed: 2.519999999999982 seconds\n",
      "Epoch 25/150 Done, Total Loss: 4759.283597946167\n",
      "Total Time Elapsed: 2.480000000000018 seconds\n",
      "Epoch 26/150 Done, Total Loss: 4758.711053276062\n",
      "Total Time Elapsed: 2.5599999999999454 seconds\n",
      "Epoch 27/150 Done, Total Loss: 4758.961039543152\n",
      "Total Time Elapsed: 2.3900000000001 seconds\n",
      "Epoch 28/150 Done, Total Loss: 4758.830054664611\n",
      "Total Time Elapsed: 2.5299999999999727 seconds\n",
      "Epoch 29/150 Done, Total Loss: 4758.7651443481445\n",
      "Total Time Elapsed: 1.9099999999998545 seconds\n",
      "Epoch 30/150 Done, Total Loss: 4759.332777786255\n",
      "Total Time Elapsed: 1.5900000000001455 seconds\n",
      "Epoch 31/150 Done, Total Loss: 4759.050713348389\n",
      "Total Time Elapsed: 1.7799999999999727 seconds\n",
      "Epoch 32/150 Done, Total Loss: 4758.7304634094235\n",
      "Total Time Elapsed: 1.8099999999999454 seconds\n",
      "Epoch 33/150 Done, Total Loss: 4758.648370552063\n",
      "Total Time Elapsed: 1.6900000000000546 seconds\n",
      "Epoch 34/150 Done, Total Loss: 4758.650695228576\n",
      "Total Time Elapsed: 1.6900000000000546 seconds\n",
      "Epoch 35/150 Done, Total Loss: 4759.008830833435\n",
      "Total Time Elapsed: 1.699999999999818 seconds\n",
      "Epoch 36/150 Done, Total Loss: 4758.465574455261\n",
      "Total Time Elapsed: 1.759999999999991 seconds\n",
      "Epoch 37/150 Done, Total Loss: 4759.110401916504\n",
      "Total Time Elapsed: 1.6800000000000637 seconds\n",
      "Epoch 38/150 Done, Total Loss: 4759.223813056946\n",
      "Total Time Elapsed: 1.7999999999999545 seconds\n",
      "Epoch 39/150 Done, Total Loss: 4758.388854789734\n",
      "Total Time Elapsed: 1.6600000000000819 seconds\n",
      "Epoch 40/150 Done, Total Loss: 4758.280761337281\n",
      "Total Time Elapsed: 1.490000000000009 seconds\n",
      "Epoch 41/150 Done, Total Loss: 4758.995714378357\n",
      "Total Time Elapsed: 1.7000000000000455 seconds\n",
      "Epoch 42/150 Done, Total Loss: 4758.832817268371\n",
      "Total Time Elapsed: 1.6599999999998545 seconds\n",
      "Epoch 43/150 Done, Total Loss: 4758.798907470703\n",
      "Total Time Elapsed: 2.050000000000182 seconds\n",
      "Epoch 44/150 Done, Total Loss: 4759.041595649719\n",
      "Total Time Elapsed: 3.269999999999982 seconds\n",
      "Epoch 45/150 Done, Total Loss: 4758.953410148621\n",
      "Total Time Elapsed: 2.2000000000000455 seconds\n",
      "Epoch 46/150 Done, Total Loss: 4758.296296691895\n",
      "Total Time Elapsed: 2.099999999999909 seconds\n",
      "Epoch 47/150 Done, Total Loss: 4758.289031410217\n",
      "Total Time Elapsed: 2.410000000000082 seconds\n",
      "Epoch 48/150 Done, Total Loss: 4759.018400382995\n",
      "Total Time Elapsed: 2.7899999999999636 seconds\n",
      "Epoch 49/150 Done, Total Loss: 4758.670283317566\n",
      "Total Time Elapsed: 2.9700000000000273 seconds\n",
      "Epoch 50/150 Done, Total Loss: 4758.480074119568\n",
      "Total Time Elapsed: 3.0599999999999454 seconds\n",
      "Epoch 51/150 Done, Total Loss: 4758.709458732605\n",
      "Total Time Elapsed: 2.3799999999998818 seconds\n",
      "Epoch 52/150 Done, Total Loss: 4759.548228645324\n",
      "Total Time Elapsed: 2.790000000000191 seconds\n",
      "Epoch 53/150 Done, Total Loss: 4759.019283485412\n",
      "Total Time Elapsed: 2.6399999999998727 seconds\n",
      "Epoch 54/150 Done, Total Loss: 4759.007921028137\n",
      "Total Time Elapsed: 1.7699999999999818 seconds\n",
      "Epoch 55/150 Done, Total Loss: 4758.756685829163\n",
      "Total Time Elapsed: 1.9700000000000273 seconds\n",
      "Epoch 56/150 Done, Total Loss: 4758.5853099823\n",
      "Total Time Elapsed: 2.019999999999982 seconds\n",
      "Epoch 57/150 Done, Total Loss: 4758.712552261352\n",
      "Total Time Elapsed: 2.1200000000001182 seconds\n",
      "Epoch 58/150 Done, Total Loss: 4759.207266807556\n",
      "Total Time Elapsed: 2.3999999999998636 seconds\n",
      "Epoch 59/150 Done, Total Loss: 4758.684796714782\n",
      "Total Time Elapsed: 2.25 seconds\n",
      "Epoch 60/150 Done, Total Loss: 4758.949575424194\n",
      "Total Time Elapsed: 2.3500000000001364 seconds\n",
      "Epoch 61/150 Done, Total Loss: 4758.992773246765\n",
      "Total Time Elapsed: 2.3899999999998727 seconds\n",
      "Epoch 62/150 Done, Total Loss: 4758.791084861756\n",
      "Total Time Elapsed: 2.230000000000018 seconds\n",
      "Epoch 63/150 Done, Total Loss: 4758.774726295471\n",
      "Total Time Elapsed: 2.089999999999918 seconds\n",
      "Epoch 64/150 Done, Total Loss: 4758.926288032531\n",
      "Total Time Elapsed: 2.4500000000000455 seconds\n",
      "Epoch 65/150 Done, Total Loss: 4758.544465255737\n",
      "Total Time Elapsed: 2.150000000000091 seconds\n",
      "Epoch 66/150 Done, Total Loss: 4758.437092590332\n",
      "Total Time Elapsed: 2.3799999999998818 seconds\n",
      "Epoch 67/150 Done, Total Loss: 4758.458959579468\n",
      "Total Time Elapsed: 3.230000000000018 seconds\n",
      "Epoch 68/150 Done, Total Loss: 4758.76151676178\n",
      "Total Time Elapsed: 2.5 seconds\n",
      "Epoch 69/150 Done, Total Loss: 4758.587173461914\n",
      "Total Time Elapsed: 2.0399999999999636 seconds\n",
      "Epoch 70/150 Done, Total Loss: 4758.664657783508\n",
      "Total Time Elapsed: 2.3200000000001637 seconds\n",
      "Epoch 71/150 Done, Total Loss: 4758.657814407348\n",
      "Total Time Elapsed: 2.5299999999999727 seconds\n",
      "Epoch 72/150 Done, Total Loss: 4758.778505897522\n",
      "Total Time Elapsed: 2.75 seconds\n",
      "Epoch 73/150 Done, Total Loss: 4759.279509735107\n",
      "Total Time Elapsed: 2.7799999999999727 seconds\n",
      "Epoch 74/150 Done, Total Loss: 4759.162967681885\n",
      "Total Time Elapsed: 2.4200000000000728 seconds\n",
      "Epoch 75/150 Done, Total Loss: 4758.471364974976\n",
      "Total Time Elapsed: 2.3199999999999363 seconds\n",
      "Epoch 76/150 Done, Total Loss: 4759.159432792663\n",
      "Total Time Elapsed: 2.5499999999999545 seconds\n",
      "Epoch 77/150 Done, Total Loss: 4758.947354888916\n",
      "Total Time Elapsed: 2.7799999999999727 seconds\n",
      "Epoch 78/150 Done, Total Loss: 4758.592322349548\n",
      "Total Time Elapsed: 2.6100000000001273 seconds\n",
      "Epoch 79/150 Done, Total Loss: 4758.378841781616\n",
      "Total Time Elapsed: 2.8799999999998818 seconds\n",
      "Epoch 80/150 Done, Total Loss: 4758.697384262085\n",
      "Total Time Elapsed: 2.5699999999999363 seconds\n",
      "Epoch 81/150 Done, Total Loss: 4758.837594223022\n",
      "Total Time Elapsed: 2.5700000000001637 seconds\n",
      "Epoch 82/150 Done, Total Loss: 4758.5172662734985\n",
      "Total Time Elapsed: 2.6499999999998636 seconds\n",
      "Epoch 83/150 Done, Total Loss: 4758.5515808105465\n",
      "Total Time Elapsed: 2.5 seconds\n",
      "Epoch 84/150 Done, Total Loss: 4758.333835601807\n",
      "Total Time Elapsed: 2.5900000000001455 seconds\n",
      "Epoch 85/150 Done, Total Loss: 4758.468379211426\n",
      "Total Time Elapsed: 2.5699999999999363 seconds\n",
      "Epoch 86/150 Done, Total Loss: 4758.4807754516605\n",
      "Total Time Elapsed: 2.6900000000000546 seconds\n",
      "Epoch 87/150 Done, Total Loss: 4758.280567169189\n",
      "Total Time Elapsed: 2.2000000000000455 seconds\n",
      "Epoch 88/150 Done, Total Loss: 4758.436813354492\n",
      "Total Time Elapsed: 2.6499999999998636 seconds\n",
      "Epoch 89/150 Done, Total Loss: 4759.421168518066\n",
      "Total Time Elapsed: 2.759999999999991 seconds\n",
      "Epoch 90/150 Done, Total Loss: 4758.440191459656\n",
      "Total Time Elapsed: 2.509999999999991 seconds\n",
      "Epoch 91/150 Done, Total Loss: 4758.485503005982\n",
      "Total Time Elapsed: 2.410000000000082 seconds\n",
      "Epoch 92/150 Done, Total Loss: 4758.869323730469\n",
      "Total Time Elapsed: 2.769999999999982 seconds\n",
      "Epoch 93/150 Done, Total Loss: 4758.762619018555\n",
      "Total Time Elapsed: 2.6400000000001 seconds\n",
      "Epoch 94/150 Done, Total Loss: 4759.022953033447\n",
      "Total Time Elapsed: 2.3999999999998636 seconds\n",
      "Epoch 95/150 Done, Total Loss: 4758.5088579177855\n",
      "Total Time Elapsed: 2.5699999999999363 seconds\n",
      "Epoch 96/150 Done, Total Loss: 4759.33978691101\n",
      "Total Time Elapsed: 2.5900000000001455 seconds\n",
      "Epoch 97/150 Done, Total Loss: 4758.41569519043\n",
      "Total Time Elapsed: 2.369999999999891 seconds\n",
      "Epoch 98/150 Done, Total Loss: 4758.781191062927\n",
      "Total Time Elapsed: 2.6799999999998363 seconds\n",
      "Epoch 99/150 Done, Total Loss: 4758.744235420227\n",
      "Total Time Elapsed: 2.180000000000291 seconds\n",
      "Epoch 100/150 Done, Total Loss: 4759.264025878906\n",
      "Total Time Elapsed: 2.650000000000091 seconds\n",
      "Epoch 101/150 Done, Total Loss: 4758.390395927429\n",
      "Total Time Elapsed: 2.4699999999998 seconds\n",
      "Epoch 102/150 Done, Total Loss: 4758.811771583557\n",
      "Total Time Elapsed: 2.7899999999999636 seconds\n",
      "Epoch 103/150 Done, Total Loss: 4758.398738861084\n",
      "Total Time Elapsed: 2.630000000000109 seconds\n",
      "Epoch 104/150 Done, Total Loss: 4758.669714736939\n",
      "Total Time Elapsed: 2.6399999999998727 seconds\n",
      "Epoch 105/150 Done, Total Loss: 4758.383944511414\n",
      "Total Time Elapsed: 2.680000000000291 seconds\n",
      "Epoch 106/150 Done, Total Loss: 4758.641552734375\n",
      "Total Time Elapsed: 2.519999999999982 seconds\n",
      "Epoch 107/150 Done, Total Loss: 4758.095308303833\n",
      "Total Time Elapsed: 2.7399999999997817 seconds\n",
      "Epoch 108/150 Done, Total Loss: 4758.892281532288\n",
      "Total Time Elapsed: 2.5799999999999272 seconds\n",
      "Epoch 109/150 Done, Total Loss: 4758.808242797852\n",
      "Total Time Elapsed: 2.5799999999999272 seconds\n",
      "Epoch 110/150 Done, Total Loss: 4757.835539436341\n",
      "Total Time Elapsed: 2.4200000000000728 seconds\n",
      "Epoch 111/150 Done, Total Loss: 4759.172417449951\n",
      "Total Time Elapsed: 2.6700000000000728 seconds\n",
      "Epoch 112/150 Done, Total Loss: 4758.205185317993\n",
      "Total Time Elapsed: 2.7400000000002365 seconds\n",
      "Epoch 113/150 Done, Total Loss: 4758.715363121033\n",
      "Total Time Elapsed: 2.5599999999999454 seconds\n",
      "Epoch 114/150 Done, Total Loss: 4757.56740989685\n",
      "Total Time Elapsed: 2.7399999999997817 seconds\n",
      "Epoch 115/150 Done, Total Loss: 4758.592804145813\n",
      "Total Time Elapsed: 2.7100000000000364 seconds\n",
      "Epoch 116/150 Done, Total Loss: 4758.748894119262\n",
      "Total Time Elapsed: 2.8299999999999272 seconds\n",
      "Epoch 117/150 Done, Total Loss: 4759.114223098755\n",
      "Total Time Elapsed: 2.3200000000001637 seconds\n",
      "Epoch 118/150 Done, Total Loss: 4758.439080238342\n",
      "Total Time Elapsed: 2.6900000000000546 seconds\n",
      "Epoch 119/150 Done, Total Loss: 4758.4463722229\n",
      "Total Time Elapsed: 2.849999999999909 seconds\n",
      "Epoch 120/150 Done, Total Loss: 4758.9190574646\n",
      "Total Time Elapsed: 2.5099999999997635 seconds\n",
      "Epoch 121/150 Done, Total Loss: 4759.441847038269\n",
      "Total Time Elapsed: 2.630000000000109 seconds\n",
      "Epoch 122/150 Done, Total Loss: 4758.119403266906\n",
      "Total Time Elapsed: 2.380000000000109 seconds\n",
      "Epoch 123/150 Done, Total Loss: 4759.834508705139\n",
      "Total Time Elapsed: 2.7899999999999636 seconds\n",
      "Epoch 124/150 Done, Total Loss: 4758.558163261414\n",
      "Total Time Elapsed: 2.2800000000002 seconds\n",
      "Epoch 125/150 Done, Total Loss: 4758.981733703613\n",
      "Total Time Elapsed: 2.6299999999996544 seconds\n",
      "Epoch 126/150 Done, Total Loss: 4759.034672355652\n",
      "Total Time Elapsed: 2.4700000000002547 seconds\n",
      "Epoch 127/150 Done, Total Loss: 4758.356572914124\n",
      "Total Time Elapsed: 2.519999999999982 seconds\n",
      "Epoch 128/150 Done, Total Loss: 4758.017061424255\n",
      "Total Time Elapsed: 2.7100000000000364 seconds\n",
      "Epoch 129/150 Done, Total Loss: 4758.838074874878\n",
      "Total Time Elapsed: 2.5799999999999272 seconds\n",
      "Epoch 130/150 Done, Total Loss: 4757.90053653717\n",
      "Total Time Elapsed: 2.7100000000000364 seconds\n",
      "Epoch 131/150 Done, Total Loss: 4759.056434249878\n",
      "Total Time Elapsed: 2.7600000000002183 seconds\n",
      "Epoch 132/150 Done, Total Loss: 4758.8123184204105\n",
      "Total Time Elapsed: 2.5799999999999272 seconds\n",
      "Epoch 133/150 Done, Total Loss: 4758.538591766357\n",
      "Total Time Elapsed: 2.5300000000002 seconds\n",
      "Epoch 134/150 Done, Total Loss: 4758.015907287598\n",
      "Total Time Elapsed: 2.4699999999998 seconds\n",
      "Epoch 135/150 Done, Total Loss: 4758.750517654419\n",
      "Total Time Elapsed: 2.6399999999998727 seconds\n",
      "Epoch 136/150 Done, Total Loss: 4757.670306396484\n",
      "Total Time Elapsed: 2.2400000000002365 seconds\n",
      "Epoch 137/150 Done, Total Loss: 4758.880062294006\n",
      "Total Time Elapsed: 2.769999999999982 seconds\n",
      "Epoch 138/150 Done, Total Loss: 4758.472916412354\n",
      "Total Time Elapsed: 2.6799999999998363 seconds\n",
      "Epoch 139/150 Done, Total Loss: 4758.43957824707\n",
      "Total Time Elapsed: 2.519999999999982 seconds\n",
      "Epoch 140/150 Done, Total Loss: 4759.530238723755\n",
      "Total Time Elapsed: 2.2100000000000364 seconds\n",
      "Epoch 141/150 Done, Total Loss: 4757.962047004699\n",
      "Total Time Elapsed: 2.550000000000182 seconds\n",
      "Epoch 142/150 Done, Total Loss: 4757.983994483948\n",
      "Total Time Elapsed: 2.6900000000000546 seconds\n",
      "Epoch 143/150 Done, Total Loss: 4757.933430671692\n",
      "Total Time Elapsed: 2.3599999999996726 seconds\n",
      "Epoch 144/150 Done, Total Loss: 4758.296364593506\n",
      "Total Time Elapsed: 2.680000000000291 seconds\n",
      "Epoch 145/150 Done, Total Loss: 4758.123239135743\n",
      "Total Time Elapsed: 2.6700000000000728 seconds\n",
      "Epoch 146/150 Done, Total Loss: 4759.194696044922\n",
      "Total Time Elapsed: 2.619999999999891 seconds\n",
      "Epoch 147/150 Done, Total Loss: 4757.387393379211\n",
      "Total Time Elapsed: 2.049999999999727 seconds\n",
      "Epoch 148/150 Done, Total Loss: 4758.562977600098\n",
      "Total Time Elapsed: 2.5100000000002183 seconds\n",
      "Epoch 149/150 Done, Total Loss: 4758.517984199524\n",
      "Total Time Elapsed: 2.619999999999891 seconds\n",
      "Epoch 150/150 Done, Total Loss: 4758.8068578720095\n",
      "Total Time Elapsed: 2.519999999999982 seconds\n",
      "Total Training Time: 365.9000000000001 seconds\n"
     ]
    }
   ],
   "source": [
    "batch_size = 15\n",
    "train_data = TensorDataset(torch.from_numpy(input_seq), torch.from_numpy(output_seq))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "lr = 0.0001\n",
    "gru_model = train(train_loader, lr, model_type=\"GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "path = \"../../data/\"\n",
    "train = np.loadtxt(path +\"incomeClassTrainMapped.csv\", delimiter=\",\", skiprows=1, dtype=np.float32)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32561, 15])\n",
      "torch.Size([32561, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x_data = torch.from_numpy(train[:,0:-1])\n",
    "y_data = torch.from_numpy(train[:,[-1]])\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-8f5e1aa94558>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeal_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_dataset shape\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "deal_dataset = TensorDataset(x_data, y_data)\n",
    "\n",
    "train_size = int(0.8 * len(deal_dataset))\n",
    "test_size = len(deal_dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(deal_dataset, [train_size, test_size])\n",
    "print(\"train_dataset shape\", train_dataset.shape)\n",
    "\n",
    "\n",
    "train_dataLoader = DataLoader(dataset = train_dataset,\n",
    "                             batch_size = 32,\n",
    "                             shuffle = True,\n",
    "                             num_workers = 2)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for index, batchData in enumerate(train_dataLoader):\n",
    "        batch_x, batch_y = batchData\n",
    "    print(\"batch_x\", batch_x.shape)\n",
    "    print(\"batch_y\", batch_y.shape)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
