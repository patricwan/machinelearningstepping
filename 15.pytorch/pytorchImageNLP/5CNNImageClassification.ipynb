{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.3.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "# torchvision是独立于pytorch的关于图像操作的一些方便工具库。\n",
    "# torchvision的详细介绍在：https://pypi.org/project/torchvision/0.1.8/\n",
    "# torchvision主要包括一下几个包：\n",
    "# vision.datasets : 几个常用视觉数据集，可以下载和加载\n",
    "# vision.models : 流行的模型，例如 AlexNet, VGG, and ResNet 以及 与训练好的参数。\n",
    "# vision.transforms : 常用的图像操作，例如：随机切割，旋转等。\n",
    "# vision.utils : 用于把形似 (3 x H x W) 的张量保存到硬盘中，给一个mini-batch的图像可以产生一个图像格网。\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们定义一个基于ConvNet的简单神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、加载数据（顺序调整一下）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(53113)  #cpu随机种子\n",
    "\n",
    "#没gpu下面可以忽略\n",
    "use_cuda = torch.cuda.is_available()  \n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")  \n",
    "batch_size = test_batch_size = 32  \n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "\n",
    "#torch.utils.data.DataLoader在训练模型时使用到此函数，用来把训练数据分成多个batch，\n",
    "#此函数每次抛出一个batch数据，直至把所有的数据都抛出，也就是个数据迭代器。\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./mnist_data', \n",
    "                   train=True, #如果true，从training.pt创建数据集\n",
    "                   download=True, #如果ture，从网上自动下载\n",
    "                   \n",
    "#transform 接受一个图像返回变换后的图像的函数，相当于图像先预处理下\n",
    "#常用的操作如 ToTensor, RandomCrop，Normalize等. \n",
    "#他们可以通过transforms.Compose被组合在一起 \n",
    "                   transform=transforms.Compose([\n",
    "                       \n",
    "                       transforms.ToTensor(), \n",
    "#.ToTensor()将shape为(H, W, C)的nump.ndarray或img转为shape为(C, H, W)的tensor，\n",
    "#其将每一个数值归一化到[0,1]，其归一化方法比较简单，直接除以255即可。\n",
    "                       \n",
    "                       transforms.Normalize((0.1307,), (0.3081,)) # 所有图片像素均值和方差\n",
    "#.Normalize作用就是.ToTensor将输入归一化到(0,1)后，再使用公式”(x-mean)/std”，将每个元素分布到(-1,1)  \n",
    "                   ])), # 第一个参数dataset：数据集\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,  #随机打乱数据\n",
    "    **kwargs)##kwargs是上面gpu的设置\n",
    "  \n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./mnist_data', \n",
    "                   train=False, #如果False，从test.pt创建数据集\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, \n",
    "    shuffle=True, \n",
    "    **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、定义CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1) \n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1)\n",
    "        #in_channels：输入图像通道数，手写数字图像为1，彩色图像为3\n",
    "        #out_channels：输出通道数，这个等于卷积核的数量\n",
    "        #kernel_size：卷积核大小\n",
    "        #stride：步长\n",
    "         \n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        #上个卷积网络的out_channels，就是下一个网络的in_channels，所以这里是20\n",
    "        #out_channels：卷积核数量50\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        #全连接层torch.nn.Linear(in_features, out_features)\n",
    "        #in_features:输入特征维度，4*4*50是自己算出来的，跟输入图像维度有关\n",
    "        #out_features；输出特征维度\n",
    "        \n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        #输出维度10，10分类\n",
    "\n",
    "    def forward(self, x):  \n",
    "        #print(x.shape)  #手写数字的输入维度，(N,1,28,28), N为batch_size\n",
    "        x = F.relu(self.conv1(x)) # x = (N,50,24,24)\n",
    "        x = F.max_pool2d(x, 2, 2) # x = (N,50,12,12)\n",
    "        x = F.relu(self.conv2(x)) # x = (N,50,8,8)\n",
    "        x = F.max_pool2d(x, 2, 2) # x = (N,50,4,4)\n",
    "        x = x.view(-1, 4*4*50)    # x = (N,4*4*50)\n",
    "        x = F.relu(self.fc1(x))   # x = (N,4*4*50)*(4*4*50, 500)=(N,500)\n",
    "        x = self.fc2(x)           # x = (N,500)*(500, 10)=(N,10)\n",
    "        return F.log_softmax(x, dim=1)  #带log的softmax分类，每张图片返回10个概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、初始化模型和定义优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "model = Net().to(device) #模型初始化\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum) #定义优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLL loss的定义\n",
    "\n",
    "$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = - w_{y_n} x_{n,y_n}, \\quad\n",
    "        w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、定义训练和测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=100):\n",
    "    model.train() #进入训练模式\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad() #梯度归零\n",
    "        output = model(data)  #输出的维度[N,10] 这里的data是函数的forward参数x\n",
    "        loss = F.nll_loss(output, target) #这里loss求的是平均数，除以了batch\n",
    "#F.nll_loss(F.log_softmax(input), target) ：\n",
    "#单分类交叉熵损失函数，一张图片里只能有一个类别，输入input的需要softmax\n",
    "#还有一种是多分类损失函数，一张图片有多个类别，输入的input需要sigmoid\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, \n",
    "                batch_idx * len(data), #100*32\n",
    "                len(train_loader.dataset), #60000\n",
    "                100. * batch_idx / len(train_loader), #len(train_loader)=60000/32=1875\n",
    "                loss.item()\n",
    "            ))\n",
    "            #print(len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval() #进入测试模式\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data) \n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            #reduction='sum'代表batch的每个元素loss累加求和，默认是mean求平均\n",
    "                       \n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            \n",
    "            #print(target.shape) #torch.Size([32])\n",
    "            #print(pred.shape) #torch.Size([32, 1])\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            #pred和target的维度不一样\n",
    "            #pred.eq()相等返回1，不相等返回0，返回的tensor维度(32，1)。\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、查看运行结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0.000000%)]\tLoss: 2.297938\n",
      "Train Epoch: 1 [3200/60000 (5.333333%)]\tLoss: 0.569112\n",
      "Train Epoch: 1 [6400/60000 (10.666667%)]\tLoss: 0.205089\n",
      "Train Epoch: 1 [9600/60000 (16.000000%)]\tLoss: 0.093479\n",
      "Train Epoch: 1 [12800/60000 (21.333333%)]\tLoss: 0.180564\n",
      "Train Epoch: 1 [16000/60000 (26.666667%)]\tLoss: 0.040548\n",
      "Train Epoch: 1 [19200/60000 (32.000000%)]\tLoss: 0.135512\n",
      "Train Epoch: 1 [22400/60000 (37.333333%)]\tLoss: 0.054247\n",
      "Train Epoch: 1 [25600/60000 (42.666667%)]\tLoss: 0.114213\n",
      "Train Epoch: 1 [28800/60000 (48.000000%)]\tLoss: 0.059354\n",
      "Train Epoch: 1 [32000/60000 (53.333333%)]\tLoss: 0.087107\n",
      "Train Epoch: 1 [35200/60000 (58.666667%)]\tLoss: 0.188495\n",
      "Train Epoch: 1 [38400/60000 (64.000000%)]\tLoss: 0.093872\n",
      "Train Epoch: 1 [41600/60000 (69.333333%)]\tLoss: 0.076750\n",
      "Train Epoch: 1 [44800/60000 (74.666667%)]\tLoss: 0.042151\n",
      "Train Epoch: 1 [48000/60000 (80.000000%)]\tLoss: 0.038157\n",
      "Train Epoch: 1 [51200/60000 (85.333333%)]\tLoss: 0.052445\n",
      "Train Epoch: 1 [54400/60000 (90.666667%)]\tLoss: 0.013618\n",
      "Train Epoch: 1 [57600/60000 (96.000000%)]\tLoss: 0.031970\n",
      "\n",
      "Test set: Average loss: 0.0649, Accuracy: 9800/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0.000000%)]\tLoss: 0.053813\n",
      "Train Epoch: 2 [3200/60000 (5.333333%)]\tLoss: 0.031595\n",
      "Train Epoch: 2 [6400/60000 (10.666667%)]\tLoss: 0.091015\n",
      "Train Epoch: 2 [9600/60000 (16.000000%)]\tLoss: 0.060761\n",
      "Train Epoch: 2 [12800/60000 (21.333333%)]\tLoss: 0.029431\n",
      "Train Epoch: 2 [16000/60000 (26.666667%)]\tLoss: 0.013208\n",
      "Train Epoch: 2 [19200/60000 (32.000000%)]\tLoss: 0.100324\n",
      "Train Epoch: 2 [22400/60000 (37.333333%)]\tLoss: 0.122841\n",
      "Train Epoch: 2 [25600/60000 (42.666667%)]\tLoss: 0.009370\n",
      "Train Epoch: 2 [28800/60000 (48.000000%)]\tLoss: 0.012475\n",
      "Train Epoch: 2 [32000/60000 (53.333333%)]\tLoss: 0.038851\n",
      "Train Epoch: 2 [35200/60000 (58.666667%)]\tLoss: 0.016315\n",
      "Train Epoch: 2 [38400/60000 (64.000000%)]\tLoss: 0.161825\n",
      "Train Epoch: 2 [41600/60000 (69.333333%)]\tLoss: 0.057857\n",
      "Train Epoch: 2 [44800/60000 (74.666667%)]\tLoss: 0.008596\n",
      "Train Epoch: 2 [48000/60000 (80.000000%)]\tLoss: 0.075595\n",
      "Train Epoch: 2 [51200/60000 (85.333333%)]\tLoss: 0.206515\n",
      "Train Epoch: 2 [54400/60000 (90.666667%)]\tLoss: 0.017829\n",
      "Train Epoch: 2 [57600/60000 (96.000000%)]\tLoss: 0.012335\n",
      "\n",
      "Test set: Average loss: 0.0457, Accuracy: 9850/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "\n",
    "save_model = True\n",
    "if (save_model):\n",
    "    torch.save(model.state_dict(),\"mnist_cnn.pt\") \n",
    "    #词典格式，model.state_dict()只保存模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./fashion_mnist_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting ./fashion_mnist_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./fashion_mnist_data/FashionMNIST/raw\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Compressed file ended before the end-of-stream marker was reached",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-20fba385b8ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m                    transform=transforms.Compose([\n\u001b[0;32m     11\u001b[0m                        \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                        \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1307\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0.3081\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                    ])),\n\u001b[0;32m     14\u001b[0m     batch_size=batch_size, shuffle=True, **kwargs)\n",
      "\u001b[1;32m/usr/local/python36/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/python36/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m             \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;31m# process and save as torch files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/python36/lib/python3.6/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[0marchive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Extracting {} to {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_root\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m     \u001b[0mextract_archive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_finished\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/python36/lib/python3.6/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mextract_archive\u001b[1;34m(from_path, to_path, remove_finished)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mto_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mout_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mzip_f\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0mout_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0m_is_zip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/python36/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/python36/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    480\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m                 raise EOFError(\"Compressed file ended before the \"\n\u001b[0m\u001b[0;32m    483\u001b[0m                                \"end-of-stream marker was reached\")\n\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Compressed file ended before the end-of-stream marker was reached"
     ]
    }
   ],
   "source": [
    "#同上\n",
    "torch.manual_seed(53113)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = test_batch_size = 32\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./fashion_mnist_data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,)) \n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('./fashion_mnist_data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "\n",
    "save_model = True\n",
    "if (save_model):\n",
    "    torch.save(model.state_dict(),\"fashion_mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN模型的迁移学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 很多时候当我们需要训练一个新的图像分类任务，我们不会完全从一个随机的模型开始训练，而是利用_预训练_的模型来加速训练的过程。我们经常使用在`ImageNet`上的预训练模型。\n",
    "- 这是一种transfer learning的方法。我们常用以下两种方法做迁移学习。\n",
    "    - fine tuning: 从一个预训练模型开始，我们改变一些模型的架构，然后继续训练整个模型的参数。\n",
    "    - feature extraction: 我们不再改变与训练模型的参数，而是只更新我们改变过的部分模型参数。我们之所以叫它feature extraction是因为我们把预训练的CNN模型当做一个特征提取模型，利用提取出来的特征做来完成我们的训练任务。\n",
    "    \n",
    "以下是构建和训练迁移学习模型的基本步骤：\n",
    "- 初始化预训练模型\n",
    "- 把最后一层的输出层改变成我们想要分的类别总数\n",
    "- 定义一个optimizer来更新参数\n",
    "- 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据\n",
    "------\n",
    "\n",
    "我们会使用*hymenoptera_data*数据集，[下载](https://download.pytorch.org/tutorial/hymenoptera_data.zip).\n",
    "\n",
    "这个数据集包括两类图片, **bees** 和 **ants**, 这些数据都被处理成了可以使用`ImageFolder <https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder>`来读取的格式。我们只需要把``data_dir``设置成数据的根目录，然后把``model_name``设置成我们想要使用的与训练模型：\n",
    "::\n",
    "   [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "\n",
    "其他的参数有：\n",
    "- ``num_classes``表示数据集分类的类别数\n",
    "- ``batch_size``\n",
    "- ``num_epochs``\n",
    "- ``feature_extract``表示我们训练的时候使用fine tuning还是feature extraction方法。如果``feature_extract = False``，整个模型都会被同时更新。如果``feature_extract = True``，只有模型的最后一层被更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、查看数据，只是查看作用\n",
    "---------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms \n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"./hymenoptera_data\"\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "#蜜蜂和蚂蚁数据集不会自动下载，请到群文件下载，并放在当前代码目录下\n",
    "#os.path.join() 连接路径，相当于.../data_dir/train\n",
    "all_imgs = datasets.ImageFolder(os.path.join(data_dir, \"train\"),\n",
    "                                transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size), #把每张图片变成resnet需要输入的维度224\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]))\n",
    "loader = torch.utils.data.DataLoader(all_imgs, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "#训练数据分batch，变成tensor迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = next(iter(loader))[0] #这个img是一个batch的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
    "#transforms：torchvision的子模块，常用的图像操作\n",
    "#.ToPILImage() 把tensor或数组转换成图像\n",
    "#详细转换过程可以看这个：https://blog.csdn.net/qq_37385726/article/details/81811466\n",
    "\n",
    "plt.ion() #交互模式，默认是交互模式，可以不写\n",
    "#详细了解看这个：https://blog.csdn.net/SZuoDao/article/details/52973621\n",
    "#plt.ioff()\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)      # remove the fake batch dimension \n",
    "    #这个.squeeze(0)看不懂，去掉也可以运行\n",
    "    \n",
    "    image = unloader(image) #tensor转换成图像\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(1) # pause a bit so that plots are updated\n",
    "    #可以去掉看看，只是延迟显示作用\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "imshow(img[8], title='Image') \n",
    "imshow(img[9], title='Image')\n",
    "imshow(img[10], title='Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、把训练集和验证集分batch转换成迭代器\n",
    "---------\n",
    "\n",
    "现在我们知道了模型输入的size，我们就可以把数据预处理成相应的格式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "#把迭代器存放到字典里作为value，key是train和val，后面调用key即可。\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs, labels=next(iter(dataloaders_dict[\"train\"])) #一个batch\n",
    "print(inputs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for inputs, labels in dataloaders_dict[\"train\"]:\n",
    "    #print(inputs)\n",
    "    #print(labels)\n",
    "    print(labels.size()) #最后一个batch不足32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、加载resnet模型并修改全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "# Number of epochs to train for \n",
    "num_epochs = 2\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True  #只更新修改的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False #提取的参数梯度不更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    if model_name == \"resnet\":\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained) \n",
    "        #如果True，从imagenet上返回预训练的模型和参数\n",
    "        \n",
    "        set_parameter_requires_grad(model_ft, feature_extract)#提取的参数梯度不更新\n",
    "        #print(model_ft) 可以打印看下\n",
    "        num_ftrs = model_ft.fc.in_features \n",
    "        #model_ft.fc是resnet的最后全连接层\n",
    "        #(fc): Linear(in_features=512, out_features=1000, bias=True)\n",
    "        #in_features 是全连接层的输入特征维度\n",
    "        #print(num_ftrs)\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        #out_features=1000 改为 num_classes=2\n",
    "        input_size = 224 #resnet18网络输入图片维度是224，resnet34，50，101，152也是\n",
    "        \n",
    "    return model_ft, input_size\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、查看需要更新的参数、定义优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next(iter(model_ft.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(next(iter(model_ft.named_parameters()))) #是元组，只有两个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name,param in model_ft.named_parameters():\n",
    "    print(name) #看下都有哪些参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are \n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters() #需要更新的参数\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = [] #需要更新的参数存放在此\n",
    "    for name,param in model_ft.named_parameters(): \n",
    "        #model_ft.named_parameters()有啥看上面cell\n",
    "        if param.requires_grad == True: \n",
    "#这里要知道全连接层之前的层param.requires_grad == Flase\n",
    "#后面加的全连接层param.requires_grad == True\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else: #否则，所有的参数都会更新\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9) #定义优化器\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss() #定义损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、定义训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练测试合一起了\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=5):\n",
    "    since = time.time()\n",
    "    val_acc_history = [] \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())#深拷贝上面resnet模型参数\n",
    "#.copy和.deepcopy区别看这个：https://blog.csdn.net/u011630575/article/details/78604226 \n",
    "    best_acc = 0.\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs-1))\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            running_loss = 0.\n",
    "            running_corrects = 0.\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else: \n",
    "                model.eval()\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                with torch.autograd.set_grad_enabled(phase==\"train\"):\n",
    "                    #torch.autograd.set_grad_enabled梯度管理器，可设置为打开或关闭\n",
    "                    #phase==\"train\"是True和False，双等号要注意\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                #返回每一行最大的数和索引，prds的位置是索引的位置\n",
    "                #也可以preds = outputs.argmax(dim=1)\n",
    "                if phase == \"train\":\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                running_loss += loss.item() * inputs.size(0) #交叉熵损失函数是平均过的\n",
    "                running_corrects += torch.sum(preds.view(-1) == labels.view(-1)).item()\n",
    "                #.view(-1)展开到一维，并自己计算\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "       \n",
    "            print(\"{} Loss: {} Acc: {}\".format(phase, epoch_loss, epoch_acc))\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                #模型变好，就拷贝更新后的模型参数\n",
    "                \n",
    "            if phase == \"val\":\n",
    "                val_acc_history.append(epoch_acc) #记录每个epoch验证集的准确率\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(\"Training compete in {}m {}s\".format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"Best val Acc: {}\".format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts) #把最新的参数复制到model中\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7、运行模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "model_ft, ohist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ohist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the non-pretrained version of the model used for this run\n",
    "scratch_model,_ = initialize_model(model_name, \n",
    "                                   num_classes, \n",
    "                                   feature_extract=False, #所有参数都训练\n",
    "                                   use_pretrained=False)# 不要imagenet的参数\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.SGD(scratch_model.parameters(), \n",
    "                              lr=0.001, momentum=0.9)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "_,scratch_hist = train_model(scratch_model, \n",
    "                             dataloaders_dict, \n",
    "                             scratch_criterion, \n",
    "                             scratch_optimizer, \n",
    "                             num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot the training curves of validation accuracy vs. number \n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "# ohist = []\n",
    "# shist = []\n",
    "\n",
    "# ohist = [h.cpu().numpy() for h in ohist]\n",
    "# shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.plot(range(1,num_epochs+1),scratch_hist,label=\"Scratch\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
