{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第一课"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "什么是PyTorch?\n",
    "================\n",
    "\n",
    "PyTorch是一个基于Python的科学计算库，它有以下特点:\n",
    "\n",
    "- 类似于NumPy，但是它可以使用GPU\n",
    "- 可以用它定义深度学习模型，可以灵活地进行深度学习模型的训练和使用\n",
    "\n",
    "Tensors\n",
    "---------------\n",
    "\n",
    "\n",
    "Tensor类似与NumPy的ndarray，唯一的区别是Tensor可以在GPU上加速运算。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造一个未初始化的5x3矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.7986e+19, 4.5691e-41, 3.4396e-37],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个随机初始化的矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3148, 0.5820, 0.6892],\n",
       "        [0.5551, 0.1520, 0.9265],\n",
       "        [0.3510, 0.5425, 0.8213],\n",
       "        [0.8385, 0.6578, 0.8176],\n",
       "        [0.1967, 0.7105, 0.1195]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个全部为0，类型为long的矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5,3,dtype=torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5,3).long()\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从数据直接直接构建tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([5.5,3])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以从一个已有的tensor构建一个tensor。这些方法会重用原来tensor的特征，例如，数据类型，除非提供新的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(5,3, dtype=torch.double)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8381, -0.0824,  0.0219],\n",
       "        [-1.4915, -2.1107,  0.9779],\n",
       "        [ 1.4244, -0.7605, -0.9730],\n",
       "        [-0.1420, -0.8863, -2.2631],\n",
       "        [-0.6980, -1.0067, -0.0095]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到tensor的形状:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>注意</h4><p>``torch.Size`` 返回的是一个tuple</p></div>\n",
    "\n",
    "Operations\n",
    "\n",
    "\n",
    "有很多种tensor运算。我们先介绍加法运算。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4954, 0.9024, 0.9164],\n",
       "        [0.3689, 0.4199, 0.9162],\n",
       "        [0.9710, 0.8278, 0.7800],\n",
       "        [0.3130, 0.7839, 0.8759],\n",
       "        [0.9094, 0.4873, 0.5652]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3335,  0.8199,  0.9383],\n",
       "        [-1.1226, -1.6908,  1.8941],\n",
       "        [ 2.3954,  0.0672, -0.1930],\n",
       "        [ 0.1710, -0.1023, -1.3872],\n",
       "        [ 0.2114, -0.5194,  0.5557]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种着加法的写法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3335,  0.8199,  0.9383],\n",
       "        [-1.1226, -1.6908,  1.8941],\n",
       "        [ 2.3954,  0.0672, -0.1930],\n",
       "        [ 0.1710, -0.1023, -1.3872],\n",
       "        [ 0.2114, -0.5194,  0.5557]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法：把输出作为一个变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3335,  0.8199,  0.9383],\n",
       "        [-1.1226, -1.6908,  1.8941],\n",
       "        [ 2.3954,  0.0672, -0.1930],\n",
       "        [ 0.1710, -0.1023, -1.3872],\n",
       "        [ 0.2114, -0.5194,  0.5557]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.empty(5,3)\n",
    "torch.add(x, y, out=result)\n",
    "# result = x + y\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in-place加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3335,  0.8199,  0.9383],\n",
       "        [-1.1226, -1.6908,  1.8941],\n",
       "        [ 2.3954,  0.0672, -0.1930],\n",
       "        [ 0.1710, -0.1023, -1.3872],\n",
       "        [ 0.2114, -0.5194,  0.5557]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>注意</h4><p>任何in-place的运算都会以``_``结尾。\n",
    "    举例来说：``x.copy_(y)``, ``x.t_()``, 会改变 ``x``。</p></div>\n",
    "\n",
    "各种类似NumPy的indexing都可以在PyTorch tensor上面使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1107,  0.9779],\n",
       "        [-0.7605, -0.9730],\n",
       "        [-0.8863, -2.2631],\n",
       "        [-1.0067, -0.0095]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing: 如果你希望resize/reshape一个tensor，可以使用``torch.view``："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9573e+00,  1.2257e-01,  9.7921e-01,  1.3878e+00,  3.3277e-01,\n",
       "         -1.4437e-03,  7.8901e-01,  1.3432e+00],\n",
       "        [ 1.6884e+00,  7.9933e-01, -2.0388e+00,  1.9692e+00, -1.8358e-01,\n",
       "          6.7984e-01,  1.0780e-01,  9.6673e-01]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1,8)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有一个只有一个元素的tensor，使用``.item()``方法可以把里面的value变成Python数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7729])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7728920578956604"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9573e+00,  1.6884e+00],\n",
       "        [ 1.2257e-01,  7.9933e-01],\n",
       "        [ 9.7921e-01, -2.0388e+00],\n",
       "        [ 1.3878e+00,  1.9692e+00],\n",
       "        [ 3.3277e-01, -1.8358e-01],\n",
       "        [-1.4437e-03,  6.7984e-01],\n",
       "        [ 7.8901e-01,  1.0780e-01],\n",
       "        [ 1.3432e+00,  9.6673e-01]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.transpose(1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**更多阅读**\n",
    "\n",
    "\n",
    "  各种Tensor operations, 包括transposing, indexing, slicing,\n",
    "  mathematical operations, linear algebra, random numbers在\n",
    "  `<https://pytorch.org/docs/torch>`.\n",
    "\n",
    "Numpy和Tensor之间的转化\n",
    "------------\n",
    "\n",
    "在Torch Tensor和NumPy array之间相互转化非常容易。\n",
    "\n",
    "Torch Tensor和NumPy array会共享内存，所以改变其中一项也会改变另一项。\n",
    "\n",
    "把Torch Tensor转变成NumPy Array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变numpy array里面的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1] = 2\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 1., 1., 1.])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把NumPy ndarray转成Torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有CPU上的Tensor都支持转成numpy或者从numpy转成Tensor。\n",
    "\n",
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "使用``.to``方法，Tensor可以被移动到别的device上。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device=device)\n",
    "    x = x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.95728540e+00,  1.22565255e-01,  9.79205191e-01,  1.38781381e+00,\n",
       "        3.32774490e-01, -1.44365639e-03,  7.89014876e-01,  1.34321415e+00,\n",
       "        1.68836606e+00,  7.99332917e-01, -2.03880811e+00,  1.96919990e+00,\n",
       "       -1.83580667e-01,  6.79840565e-01,  1.07799731e-01,  9.66726363e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.to(\"cpu\").data.numpy()\n",
    "y.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-2f5a27920c37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \"\"\"\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \"\"\"\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/python36/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    190\u001b[0m             raise RuntimeError(\n\u001b[0;32m    191\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/python36/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[1;34m()\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[1;32mfrom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "热身: 用numpy实现两层神经网络\n",
    "--------------\n",
    "\n",
    "一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss。\n",
    "- $h = W_1X$\n",
    "- $a = max(0, h)$\n",
    "- $y_{hat} = W_2a$\n",
    "\n",
    "这一实现完全使用numpy来计算前向神经网络，loss，和反向传播。\n",
    "- forward pass\n",
    "- loss\n",
    "- backward pass\n",
    "\n",
    "numpy ndarray是一个普通的n维array。它不知道任何关于深度学习或者梯度(gradient)的知识，也不知道计算图(computation graph)，只是一种用来计算数学运算的数据结构。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    h = x.dot(w1) # N * H\n",
    "    h_relu = np.maximum(h, 0) # N * H\n",
    "    y_pred = h_relu.dot(w2) # N * D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(it, loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensors\n",
    "----------------\n",
    "\n",
    "这次我们使用PyTorch tensors来创建前向神经网络，计算损失，以及反向传播。\n",
    "\n",
    "一个PyTorch Tensor很像一个numpy的ndarray。但是它和numpy ndarray最大的区别是，PyTorch Tensor可以在CPU或者GPU上运算。如果想要在GPU上运算，就需要把Tensor换成cuda类型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H)\n",
    "w2 = torch.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    h = x.mm(w1) # N * H\n",
    "    h_relu = h.clamp(min=0) # N * H\n",
    "    y_pred = h_relu.mm(w2) # N * D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(it, loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "y = w*x + b # y = 2*1+3\n",
    "\n",
    "y.backward()\n",
    "\n",
    "# dy / dw = x\n",
    "print(w.grad)\n",
    "print(x.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensor和autograd\n",
    "-------------------------------\n",
    "\n",
    "PyTorch的一个重要功能就是autograd，也就是说只要定义了forward pass(前向神经网络)，计算了loss之后，PyTorch可以自动求导计算模型所有参数的梯度。\n",
    "\n",
    "一个PyTorch的Tensor表示计算图中的一个节点。如果``x``是一个Tensor并且``x.requires_grad=True``那么``x.grad``是另一个储存着``x``当前梯度(相对于一个scalar，常常是loss)的向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum() # computation graph\n",
    "    print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: nn\n",
    "-----------\n",
    "\n",
    "\n",
    "这次我们使用PyTorch中nn这个库来构建网络。\n",
    "用PyTorch autograd来构建计算图和计算gradients，\n",
    "然后PyTorch会帮我们自动计算gradient。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25072262.0\n",
      "1 19905086.0\n",
      "2 17571300.0\n",
      "3 15858608.0\n",
      "4 13857529.0\n",
      "5 11411684.0\n",
      "6 8789994.0\n",
      "7 6407556.0\n",
      "8 4491858.5\n",
      "9 3104328.25\n",
      "10 2150641.75\n",
      "11 1519154.125\n",
      "12 1102484.875\n",
      "13 826666.9375\n",
      "14 640141.8125\n",
      "15 510812.40625\n",
      "16 418157.65625\n",
      "17 349592.40625\n",
      "18 297217.375\n",
      "19 256058.125\n",
      "20 222880.921875\n",
      "21 195608.453125\n",
      "22 172814.71875\n",
      "23 153456.234375\n",
      "24 136853.796875\n",
      "25 122502.796875\n",
      "26 110056.0234375\n",
      "27 99151.3515625\n",
      "28 89545.453125\n",
      "29 81047.703125\n",
      "30 73501.484375\n",
      "31 66781.9765625\n",
      "32 60774.3984375\n",
      "33 55392.13671875\n",
      "34 50572.57421875\n",
      "35 46234.8046875\n",
      "36 42320.3671875\n",
      "37 38781.29296875\n",
      "38 35577.09375\n",
      "39 32669.32421875\n",
      "40 30028.361328125\n",
      "41 27629.3828125\n",
      "42 25443.94921875\n",
      "43 23450.453125\n",
      "44 21629.78125\n",
      "45 19964.931640625\n",
      "46 18440.498046875\n",
      "47 17044.193359375\n",
      "48 15763.48046875\n",
      "49 14587.8291015625\n",
      "50 13507.2744140625\n",
      "51 12513.7333984375\n",
      "52 11599.1669921875\n",
      "53 10759.337890625\n",
      "54 9985.216796875\n",
      "55 9271.345703125\n",
      "56 8612.20703125\n",
      "57 8003.66455078125\n",
      "58 7441.3701171875\n",
      "59 6921.11865234375\n",
      "60 6439.79296875\n",
      "61 5994.34716796875\n",
      "62 5581.599609375\n",
      "63 5199.24853515625\n",
      "64 4844.7216796875\n",
      "65 4515.97021484375\n",
      "66 4210.80859375\n",
      "67 3927.46044921875\n",
      "68 3664.364501953125\n",
      "69 3420.06005859375\n",
      "70 3192.896728515625\n",
      "71 2981.653564453125\n",
      "72 2785.130126953125\n",
      "73 2602.1943359375\n",
      "74 2432.019287109375\n",
      "75 2273.46484375\n",
      "76 2125.732177734375\n",
      "77 1988.10546875\n",
      "78 1859.9423828125\n",
      "79 1740.5833740234375\n",
      "80 1629.2587890625\n",
      "81 1525.4129638671875\n",
      "82 1428.513916015625\n",
      "83 1337.9923095703125\n",
      "84 1253.4761962890625\n",
      "85 1174.525146484375\n",
      "86 1100.755126953125\n",
      "87 1031.8277587890625\n",
      "88 967.3790283203125\n",
      "89 907.1402587890625\n",
      "90 850.7254638671875\n",
      "91 797.9354858398438\n",
      "92 748.56884765625\n",
      "93 702.3692626953125\n",
      "94 659.1331787109375\n",
      "95 618.6549072265625\n",
      "96 580.7608642578125\n",
      "97 545.2705688476562\n",
      "98 512.020751953125\n",
      "99 480.8751525878906\n",
      "100 451.6889343261719\n",
      "101 424.3332214355469\n",
      "102 398.6979064941406\n",
      "103 374.6607971191406\n",
      "104 352.1180419921875\n",
      "105 330.974853515625\n",
      "106 311.1408996582031\n",
      "107 292.5423583984375\n",
      "108 275.0810852050781\n",
      "109 258.7002258300781\n",
      "110 243.32080078125\n",
      "111 228.8848419189453\n",
      "112 215.33541870117188\n",
      "113 202.60317993164062\n",
      "114 190.6483917236328\n",
      "115 179.421142578125\n",
      "116 168.87290954589844\n",
      "117 158.96939086914062\n",
      "118 149.72288513183594\n",
      "119 141.03042602539062\n",
      "120 132.86376953125\n",
      "121 125.181396484375\n",
      "122 117.96116638183594\n",
      "123 111.16864013671875\n",
      "124 104.7776870727539\n",
      "125 98.76490783691406\n",
      "126 93.11030578613281\n",
      "127 87.78731536865234\n",
      "128 82.7784423828125\n",
      "129 78.06273651123047\n",
      "130 73.62181091308594\n",
      "131 69.44184875488281\n",
      "132 65.50638580322266\n",
      "133 61.800025939941406\n",
      "134 58.308570861816406\n",
      "135 55.0184211730957\n",
      "136 51.918338775634766\n",
      "137 48.99933624267578\n",
      "138 46.24837875366211\n",
      "139 43.655094146728516\n",
      "140 41.21040725708008\n",
      "141 38.90623092651367\n",
      "142 36.7354621887207\n",
      "143 34.687896728515625\n",
      "144 32.75809097290039\n",
      "145 30.938995361328125\n",
      "146 29.223339080810547\n",
      "147 27.60513687133789\n",
      "148 26.078876495361328\n",
      "149 24.639202117919922\n",
      "150 23.279579162597656\n",
      "151 21.997243881225586\n",
      "152 20.787317276000977\n",
      "153 19.645654678344727\n",
      "154 18.567354202270508\n",
      "155 17.549772262573242\n",
      "156 16.589126586914062\n",
      "157 15.682344436645508\n",
      "158 14.826615333557129\n",
      "159 14.017451286315918\n",
      "160 13.2534761428833\n",
      "161 12.532447814941406\n",
      "162 11.851644515991211\n",
      "163 11.207889556884766\n",
      "164 10.600314140319824\n",
      "165 10.025954246520996\n",
      "166 9.483576774597168\n",
      "167 8.970772743225098\n",
      "168 8.486899375915527\n",
      "169 8.029001235961914\n",
      "170 7.595879554748535\n",
      "171 7.187447547912598\n",
      "172 6.801194190979004\n",
      "173 6.43575382232666\n",
      "174 6.09010124206543\n",
      "175 5.763797283172607\n",
      "176 5.455406665802002\n",
      "177 5.163514137268066\n",
      "178 4.8876471519470215\n",
      "179 4.626447677612305\n",
      "180 4.379616737365723\n",
      "181 4.146251678466797\n",
      "182 3.9255850315093994\n",
      "183 3.7167770862579346\n",
      "184 3.5192017555236816\n",
      "185 3.332322120666504\n",
      "186 3.155430793762207\n",
      "187 2.9883949756622314\n",
      "188 2.830075263977051\n",
      "189 2.6802432537078857\n",
      "190 2.538464069366455\n",
      "191 2.4044077396392822\n",
      "192 2.277440071105957\n",
      "193 2.1573195457458496\n",
      "194 2.0437614917755127\n",
      "195 1.9362623691558838\n",
      "196 1.8342461585998535\n",
      "197 1.7377984523773193\n",
      "198 1.6466147899627686\n",
      "199 1.5601253509521484\n",
      "200 1.4783565998077393\n",
      "201 1.400862216949463\n",
      "202 1.3275346755981445\n",
      "203 1.2582025527954102\n",
      "204 1.1923378705978394\n",
      "205 1.129957914352417\n",
      "206 1.0710281133651733\n",
      "207 1.0151749849319458\n",
      "208 0.9623206257820129\n",
      "209 0.9122145175933838\n",
      "210 0.8646692037582397\n",
      "211 0.8196102976799011\n",
      "212 0.7769709825515747\n",
      "213 0.7366833686828613\n",
      "214 0.6984901428222656\n",
      "215 0.6622159481048584\n",
      "216 0.6278902888298035\n",
      "217 0.5953798294067383\n",
      "218 0.5645536780357361\n",
      "219 0.5353865027427673\n",
      "220 0.5077365636825562\n",
      "221 0.4814586639404297\n",
      "222 0.4565921127796173\n",
      "223 0.43305864930152893\n",
      "224 0.4107246398925781\n",
      "225 0.3895011246204376\n",
      "226 0.3694634735584259\n",
      "227 0.3504437804222107\n",
      "228 0.33242446184158325\n",
      "229 0.31535473465919495\n",
      "230 0.2991410791873932\n",
      "231 0.28381234407424927\n",
      "232 0.26923730969429016\n",
      "233 0.25539037585258484\n",
      "234 0.24230138957500458\n",
      "235 0.22990451753139496\n",
      "236 0.2181510329246521\n",
      "237 0.20695540308952332\n",
      "238 0.1963849663734436\n",
      "239 0.18638040125370026\n",
      "240 0.17682760953903198\n",
      "241 0.1677488386631012\n",
      "242 0.15921412408351898\n",
      "243 0.15112511813640594\n",
      "244 0.14341795444488525\n",
      "245 0.13611657917499542\n",
      "246 0.129165381193161\n",
      "247 0.12255943566560745\n",
      "248 0.11632855236530304\n",
      "249 0.11045359075069427\n",
      "250 0.10481860488653183\n",
      "251 0.09951331466436386\n",
      "252 0.09442716836929321\n",
      "253 0.0896521732211113\n",
      "254 0.08509750664234161\n",
      "255 0.08078224956989288\n",
      "256 0.07671012729406357\n",
      "257 0.07281374931335449\n",
      "258 0.06911724805831909\n",
      "259 0.06563714891672134\n",
      "260 0.062302011996507645\n",
      "261 0.05913257971405983\n",
      "262 0.056163713335990906\n",
      "263 0.05333118140697479\n",
      "264 0.05063822120428085\n",
      "265 0.04809818044304848\n",
      "266 0.0456920750439167\n",
      "267 0.043391525745391846\n",
      "268 0.04119080677628517\n",
      "269 0.03912035748362541\n",
      "270 0.03715686500072479\n",
      "271 0.03528427705168724\n",
      "272 0.0335211455821991\n",
      "273 0.03185926005244255\n",
      "274 0.030245596542954445\n",
      "275 0.028743699193000793\n",
      "276 0.027309350669384003\n",
      "277 0.025940651074051857\n",
      "278 0.02463914081454277\n",
      "279 0.02339647337794304\n",
      "280 0.022243143990635872\n",
      "281 0.02113107591867447\n",
      "282 0.020079689100384712\n",
      "283 0.019081277772784233\n",
      "284 0.018132740631699562\n",
      "285 0.01722627878189087\n",
      "286 0.0163752231746912\n",
      "287 0.015569756738841534\n",
      "288 0.014793247915804386\n",
      "289 0.014060555957257748\n",
      "290 0.013369042426347733\n",
      "291 0.012709380127489567\n",
      "292 0.012087086215615273\n",
      "293 0.011492354795336723\n",
      "294 0.010930153541266918\n",
      "295 0.010394471697509289\n",
      "296 0.009881285950541496\n",
      "297 0.009404433891177177\n",
      "298 0.008943511173129082\n",
      "299 0.008511167950928211\n",
      "300 0.008090155199170113\n",
      "301 0.007700194604694843\n",
      "302 0.007331683300435543\n",
      "303 0.006980821490287781\n",
      "304 0.006647749803960323\n",
      "305 0.006330761127173901\n",
      "306 0.006030175369232893\n",
      "307 0.005745161324739456\n",
      "308 0.005472799763083458\n",
      "309 0.005217894446104765\n",
      "310 0.0049671512097120285\n",
      "311 0.004731346387416124\n",
      "312 0.004514690022915602\n",
      "313 0.00430280389264226\n",
      "314 0.0041010333225131035\n",
      "315 0.003907532896846533\n",
      "316 0.0037310877814888954\n",
      "317 0.003558337688446045\n",
      "318 0.0034008959773927927\n",
      "319 0.0032443017698824406\n",
      "320 0.003096242668107152\n",
      "321 0.002958232769742608\n",
      "322 0.0028287984896451235\n",
      "323 0.002699712524190545\n",
      "324 0.0025801940355449915\n",
      "325 0.0024666611570864916\n",
      "326 0.002360002137720585\n",
      "327 0.002256946172565222\n",
      "328 0.0021611584816128016\n",
      "329 0.002069078618660569\n",
      "330 0.0019788620993494987\n",
      "331 0.0018967225914821029\n",
      "332 0.0018135647987946868\n",
      "333 0.001739006140269339\n",
      "334 0.0016680271364748478\n",
      "335 0.0016003450145944953\n",
      "336 0.0015314145712181926\n",
      "337 0.0014699208550155163\n",
      "338 0.0014119396219030023\n",
      "339 0.001355141052044928\n",
      "340 0.0013012186391279101\n",
      "341 0.0012492899550125003\n",
      "342 0.001201106351800263\n",
      "343 0.0011522250715643167\n",
      "344 0.0011077083181589842\n",
      "345 0.001066703931428492\n",
      "346 0.0010259373812004924\n",
      "347 0.0009871716611087322\n",
      "348 0.0009494146797806025\n",
      "349 0.0009127938537858427\n",
      "350 0.0008781382930465043\n",
      "351 0.000846330018248409\n",
      "352 0.0008161737932823598\n",
      "353 0.0007874220609664917\n",
      "354 0.0007612999761477113\n",
      "355 0.0007338621653616428\n",
      "356 0.0007058968767523766\n",
      "357 0.0006829925114288926\n",
      "358 0.0006588309188373387\n",
      "359 0.0006372263305820525\n",
      "360 0.0006141556659713387\n",
      "361 0.0005939324619248509\n",
      "362 0.0005742069333791733\n",
      "363 0.0005558951525017619\n",
      "364 0.000536641979124397\n",
      "365 0.0005192054668441415\n",
      "366 0.0005020715179853141\n",
      "367 0.0004861594643443823\n",
      "368 0.0004706899053417146\n",
      "369 0.0004554666520562023\n",
      "370 0.00044149078894406557\n",
      "371 0.0004279351851437241\n",
      "372 0.00041560345562174916\n",
      "373 0.0004024053050670773\n",
      "374 0.00038879792555235326\n",
      "375 0.0003778191457968205\n",
      "376 0.0003669163561426103\n",
      "377 0.00035642710281535983\n",
      "378 0.0003462393069639802\n",
      "379 0.0003356119559612125\n",
      "380 0.0003267508582212031\n",
      "381 0.00031692610355094075\n",
      "382 0.00030859815888106823\n",
      "383 0.0002997943665832281\n",
      "384 0.0002911345800384879\n",
      "385 0.00028364607715047896\n",
      "386 0.0002758163900580257\n",
      "387 0.00026809764676727355\n",
      "388 0.0002612113894429058\n",
      "389 0.00025397539138793945\n",
      "390 0.00024812447372823954\n",
      "391 0.000241391098825261\n",
      "392 0.0002356211916776374\n",
      "393 0.00022940922644920647\n",
      "394 0.00022366917983163148\n",
      "395 0.00021738170471508056\n",
      "396 0.00021229343838058412\n",
      "397 0.00020687980577349663\n",
      "398 0.00020171143114566803\n",
      "399 0.00019748076738324016\n",
      "400 0.00019291535136289895\n",
      "401 0.00018768140580505133\n",
      "402 0.0001837802556110546\n",
      "403 0.00017927125736605376\n",
      "404 0.00017514244245830923\n",
      "405 0.00017131041386164725\n",
      "406 0.000166982295922935\n",
      "407 0.0001628953032195568\n",
      "408 0.0001592774933669716\n",
      "409 0.00015604552754666656\n",
      "410 0.00015255273319780827\n",
      "411 0.00014946986630093306\n",
      "412 0.00014661882596556097\n",
      "413 0.00014365366951096803\n",
      "414 0.00014016915520187467\n",
      "415 0.0001372817496303469\n",
      "416 0.00013434917491395026\n",
      "417 0.00013148690050002187\n",
      "418 0.00012911338126286864\n",
      "419 0.0001262384612346068\n",
      "420 0.00012387402239255607\n",
      "421 0.00012171056732768193\n",
      "422 0.0001193232528748922\n",
      "423 0.00011707717931130901\n",
      "424 0.000114662732812576\n",
      "425 0.00011246078065596521\n",
      "426 0.00011035949137294665\n",
      "427 0.00010820204624906182\n",
      "428 0.00010632555495249107\n",
      "429 0.00010425126674817875\n",
      "430 0.00010216497321380302\n",
      "431 0.00010010292317019776\n",
      "432 9.83283098321408e-05\n",
      "433 9.600341581972316e-05\n",
      "434 9.42882543313317e-05\n",
      "435 9.306411084253341e-05\n",
      "436 9.146308730123565e-05\n",
      "437 9.004491585073993e-05\n",
      "438 8.82339445524849e-05\n",
      "439 8.648612129036337e-05\n",
      "440 8.489299943903461e-05\n",
      "441 8.35876926430501e-05\n",
      "442 8.222094038501382e-05\n",
      "443 8.077015809249133e-05\n",
      "444 7.935667963465676e-05\n",
      "445 7.830667163943872e-05\n",
      "446 7.680650014663115e-05\n",
      "447 7.580177771160379e-05\n",
      "448 7.425980584230274e-05\n",
      "449 7.349115185206756e-05\n",
      "450 7.216503581730649e-05\n",
      "451 7.098712376318872e-05\n",
      "452 6.95547932991758e-05\n",
      "453 6.852276419522241e-05\n",
      "454 6.748217856511474e-05\n",
      "455 6.645223766099662e-05\n",
      "456 6.562347698491067e-05\n",
      "457 6.433051748899743e-05\n",
      "458 6.318128725979477e-05\n",
      "459 6.21182844042778e-05\n",
      "460 6.128406675998122e-05\n",
      "461 6.035745536792092e-05\n",
      "462 5.9466696257004514e-05\n",
      "463 5.8762401749845594e-05\n",
      "464 5.785592657048255e-05\n",
      "465 5.6932934967335314e-05\n",
      "466 5.62042769161053e-05\n",
      "467 5.542308645090088e-05\n",
      "468 5.458468513097614e-05\n",
      "469 5.395925109041855e-05\n",
      "470 5.3140702220844105e-05\n",
      "471 5.234773925621994e-05\n",
      "472 5.1698840252356604e-05\n",
      "473 5.104625233798288e-05\n",
      "474 5.026287180953659e-05\n",
      "475 4.9485432100482285e-05\n",
      "476 4.8735815653344616e-05\n",
      "477 4.8366615374106914e-05\n",
      "478 4.762968092109077e-05\n",
      "479 4.722117228084244e-05\n",
      "480 4.6858163841534406e-05\n",
      "481 4.601667751558125e-05\n",
      "482 4.537416316452436e-05\n",
      "483 4.4733686081599444e-05\n",
      "484 4.407275264384225e-05\n",
      "485 4.3741019908338785e-05\n",
      "486 4.316933700465597e-05\n",
      "487 4.2545711039565504e-05\n",
      "488 4.1820137994363904e-05\n",
      "489 4.1166415030602366e-05\n",
      "490 4.0709386667003855e-05\n",
      "491 4.0389742935076356e-05\n",
      "492 3.997062594862655e-05\n",
      "493 3.957814260502346e-05\n",
      "494 3.9149814256234095e-05\n",
      "495 3.8660011341562495e-05\n",
      "496 3.82712714781519e-05\n",
      "497 3.778766040340997e-05\n",
      "498 3.744758214452304e-05\n",
      "499 3.696514249895699e-05\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=False), # w_1 * x + b_1\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias=False),\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters(): # param (tensor, grad)\n",
    "            param -= learning_rate * param.grad\n",
    "            \n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: optim\n",
    "--------------\n",
    "\n",
    "这一次我们不再手动更新模型的weights,而是使用optim这个包来帮助我们更新参数。\n",
    "optim这个package提供了各种不同的模型优化方法，包括SGD+momentum, RMSProp, Adam等等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 29080318.0\n",
      "5 10436923.0\n",
      "10 2154657.0\n",
      "15 532390.0\n",
      "20 230546.546875\n",
      "25 124364.1953125\n",
      "30 73062.9453125\n",
      "35 45142.203125\n",
      "40 28932.646484375\n",
      "45 19078.97265625\n",
      "50 12885.3955078125\n",
      "55 8874.8388671875\n",
      "60 6213.01416015625\n",
      "65 4410.171875\n",
      "70 3168.23388671875\n",
      "75 2300.213134765625\n",
      "80 1685.985107421875\n",
      "85 1246.32373046875\n",
      "90 928.1376342773438\n",
      "95 695.8846435546875\n",
      "100 524.798095703125\n",
      "105 397.8555908203125\n",
      "110 303.046875\n",
      "115 231.83480834960938\n",
      "120 178.02894592285156\n",
      "125 137.17715454101562\n",
      "130 106.04219818115234\n",
      "135 82.20759582519531\n",
      "140 63.885318756103516\n",
      "145 49.7601318359375\n",
      "150 38.83662033081055\n",
      "155 30.367061614990234\n",
      "160 23.784595489501953\n",
      "165 18.656888961791992\n",
      "170 14.656617164611816\n",
      "175 11.527595520019531\n",
      "180 9.076955795288086\n",
      "185 7.154776573181152\n",
      "190 5.6459269523620605\n",
      "195 4.4589433670043945\n",
      "200 3.524202585220337\n",
      "205 2.787775993347168\n",
      "210 2.206611156463623\n",
      "215 1.7478580474853516\n",
      "220 1.3851509094238281\n",
      "225 1.0984537601470947\n",
      "230 0.8714742660522461\n",
      "235 0.6917911171913147\n",
      "240 0.5493975877761841\n",
      "245 0.4364909529685974\n",
      "250 0.34694331884384155\n",
      "255 0.2758748233318329\n",
      "260 0.21943123638629913\n",
      "265 0.1745912879705429\n",
      "270 0.13895516097545624\n",
      "275 0.11063853651285172\n",
      "280 0.0881119817495346\n",
      "285 0.07018764317035675\n",
      "290 0.05595739558339119\n",
      "295 0.04459508880972862\n",
      "300 0.03557386249303818\n",
      "305 0.028411738574504852\n",
      "310 0.022688724100589752\n",
      "315 0.018129901960492134\n",
      "320 0.014494267292320728\n",
      "325 0.011605734936892986\n",
      "330 0.009308764711022377\n",
      "335 0.007491649594157934\n",
      "340 0.006039069499820471\n",
      "345 0.004885213449597359\n",
      "350 0.0039613149128854275\n",
      "355 0.003225538181141019\n",
      "360 0.002638044999912381\n",
      "365 0.0021652081049978733\n",
      "370 0.0017878840444609523\n",
      "375 0.0014816087204962969\n",
      "380 0.0012368294410407543\n",
      "385 0.0010419049067422748\n",
      "390 0.0008804196259006858\n",
      "395 0.0007494149613194168\n",
      "400 0.0006395681411959231\n",
      "405 0.0005512008792720735\n",
      "410 0.000477395486086607\n",
      "415 0.00041594140930101275\n",
      "420 0.0003650787111837417\n",
      "425 0.0003213321615476161\n",
      "430 0.0002838161599356681\n",
      "435 0.00025245314463973045\n",
      "440 0.00022582289238926023\n",
      "445 0.00020292193221393973\n",
      "450 0.00018261811055708677\n",
      "455 0.00016552285524085164\n",
      "460 0.00015032316150609404\n",
      "465 0.00013642088742926717\n",
      "470 0.00012540919124148786\n",
      "475 0.00011505051224958152\n",
      "480 0.00010570554877631366\n",
      "485 9.769693861017004e-05\n",
      "490 8.979438280221075e-05\n",
      "495 8.366555266547948e-05\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=False), # w_1 * x + b_1\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias=False),\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "# learning_rate = 1e-4\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "i = 0 \n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    \n",
    "    if (i % 5 ==0 ):\n",
    "        print(it, loss.item())\n",
    "    i = i +1\n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update model parameters\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: 自定义 nn Modules\n",
    "--------------------------\n",
    "\n",
    "我们可以定义一个模型，这个模型继承自nn.Module类。如果需要定义一个比Sequential模型更加复杂的模型，就需要定义nn.Module模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 696.9291381835938\n",
      "5 615.1718139648438\n",
      "10 544.909423828125\n",
      "15 484.7691650390625\n",
      "20 432.4112854003906\n",
      "25 386.5163879394531\n",
      "30 345.7978210449219\n",
      "35 309.2781982421875\n",
      "40 276.3218688964844\n",
      "45 246.56204223632812\n",
      "50 219.54469299316406\n",
      "55 194.89508056640625\n",
      "60 172.42190551757812\n",
      "65 151.8444061279297\n",
      "70 132.96307373046875\n",
      "75 115.81351470947266\n",
      "80 100.37520599365234\n",
      "85 86.48570251464844\n",
      "90 74.0639419555664\n",
      "95 63.031436920166016\n",
      "100 53.30567932128906\n",
      "105 44.7796516418457\n",
      "110 37.372623443603516\n",
      "115 30.988656997680664\n",
      "120 25.535381317138672\n",
      "125 20.90952491760254\n",
      "130 17.021394729614258\n",
      "135 13.773482322692871\n",
      "140 11.087059020996094\n",
      "145 8.878859519958496\n",
      "150 7.076192378997803\n",
      "155 5.6155686378479\n",
      "160 4.438809394836426\n",
      "165 3.496457576751709\n",
      "170 2.7458159923553467\n",
      "175 2.1499686241149902\n",
      "180 1.6794917583465576\n",
      "185 1.3087910413742065\n",
      "190 1.0175358057022095\n",
      "195 0.7892367839813232\n",
      "200 0.6106471419334412\n",
      "205 0.47118714451789856\n",
      "210 0.3626347482204437\n",
      "215 0.27827179431915283\n",
      "220 0.2128976583480835\n",
      "225 0.1623910516500473\n",
      "230 0.12347717583179474\n",
      "235 0.09359244257211685\n",
      "240 0.07071615755558014\n",
      "245 0.053260162472724915\n",
      "250 0.03998202458024025\n",
      "255 0.029937205836176872\n",
      "260 0.02236931212246418\n",
      "265 0.016669051721692085\n",
      "270 0.012390805408358574\n",
      "275 0.00918794795870781\n",
      "280 0.006796346977353096\n",
      "285 0.005015544127672911\n",
      "290 0.0036931182257831097\n",
      "295 0.0027135314885526896\n",
      "300 0.0019899997860193253\n",
      "305 0.0014568391488865018\n",
      "310 0.0010647715535014868\n",
      "315 0.0007773012621328235\n",
      "320 0.0005668113590218127\n",
      "325 0.00041276332922279835\n",
      "330 0.00030019073165021837\n",
      "335 0.00021803774870932102\n",
      "340 0.00015817262465134263\n",
      "345 0.0001146028662333265\n",
      "350 8.294036524603143e-05\n",
      "355 5.99545928707812e-05\n",
      "360 4.328306022216566e-05\n",
      "365 3.120648761978373e-05\n",
      "370 2.246498661406804e-05\n",
      "375 1.6145972040249035e-05\n",
      "380 1.158566192316357e-05\n",
      "385 8.296585292555392e-06\n",
      "390 5.928250629949616e-06\n",
      "395 4.227552835800452e-06\n",
      "400 3.0075029826548416e-06\n",
      "405 2.1335395103960764e-06\n",
      "410 1.5089140106283594e-06\n",
      "415 1.0640827667884878e-06\n",
      "420 7.476614882762078e-07\n",
      "425 5.245952365839912e-07\n",
      "430 3.6605820241675247e-07\n",
      "435 2.551539637352107e-07\n",
      "440 1.7676950392342405e-07\n",
      "445 1.2231396340212086e-07\n",
      "450 8.426393804938925e-08\n",
      "455 5.78151784225156e-08\n",
      "460 3.966114547893085e-08\n",
      "465 2.7081226150471593e-08\n",
      "470 1.837919683111977e-08\n",
      "475 1.2569378426974254e-08\n",
      "480 8.550739671875363e-09\n",
      "485 5.83137227394559e-09\n",
      "490 3.985495489189361e-09\n",
      "495 2.746052496505058e-09\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # define the model architecture\n",
    "        self.linear1 = torch.nn.Linear(D_in, H, bias=False)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear2(self.linear1(x).clamp(min=0))\n",
    "        return y_pred\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "i = 0\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    if (i % 5 ==0 ):\n",
    "        print(it, loss.item())\n",
    "    i = i +1\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update model parameters\n",
    "    optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
