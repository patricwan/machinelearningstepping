{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHpdbe4NcoNl"
   },
   "source": [
    "# Seq2Seq, Attention\n",
    "\n",
    "褚则伟 zeweichu@gmail.com\n",
    "\n",
    "在这份notebook当中，我们会(尽可能)复现Luong的attention模型\n",
    "\n",
    "由于我们的数据集非常小，只有一万多个句子的训练数据，所以训练出来的模型效果并不好。如果大家想训练一个好一点的模型，可以参考下面的资料。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5RqjiiwJcoNm"
   },
   "source": [
    "## 更多阅读\n",
    "\n",
    "#### 课件\n",
    "- [cs224d](http://cs224d.stanford.edu/lectures/CS224d-Lecture15.pdf)\n",
    "\n",
    "\n",
    "#### 论文\n",
    "- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
    "- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025?context=cs)\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1406.1078)\n",
    "\n",
    "\n",
    "#### PyTorch代码\n",
    "- [seq2seq-tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)\n",
    "- [Tutorial from Ben Trevett](https://github.com/bentrevett/pytorch-seq2seq)\n",
    "- [IBM seq2seq](https://github.com/IBM/pytorch-seq2seq)\n",
    "- [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)\n",
    "\n",
    "\n",
    "#### 更多关于Machine Translation\n",
    "- [Beam Search](https://www.coursera.org/lecture/nlp-sequence-models/beam-search-4EtHZ)\n",
    "- Pointer network 文本摘要\n",
    "- Copy Mechanism 文本摘要\n",
    "- Converage Loss \n",
    "- ConvSeq2Seq\n",
    "- Transformer\n",
    "- Tensor2Tensor\n",
    "\n",
    "#### TODO\n",
    "- 建议同学尝试对中文进行分词\n",
    "\n",
    "#### NER\n",
    "- https://github.com/allenai/allennlp/tree/master/allennlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "QTPmv1ERcoNn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import Counter #计数器\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ON3mZkKWcoNq"
   },
   "source": [
    "读入中英文数据\n",
    "- 英文我们使用nltk的word tokenizer来分词，并且使用小写字母\n",
    "- 中文我们直接使用单个汉字作为基本单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "uMyTGTsPcoNr"
   },
   "outputs": [],
   "source": [
    "def load_data(in_file):\n",
    "    cn = []\n",
    "    en = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r') as f:\n",
    "        for line in f:\n",
    "            #print(line) #Anyone can do that.\t任何人都可以做到。\n",
    "            line = line.strip().split(\"\\t\") #分词后用逗号隔开\n",
    "            #print(line) #['Anyone can do that.', '任何人都可以做到。']\n",
    "            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "            #BOS:beginning of sequence EOS:end of\n",
    "            \n",
    "            # split chinese sentence into characters\n",
    "            cn.append([\"BOS\"] + [c for c in line[1]] + [\"EOS\"])\n",
    "            #中文一个一个字分词，可以尝试用分词器分词\n",
    "    return en, cn\n",
    "\n",
    "train_file = \"../../../data/nmt/en-cn/train.txt\"\n",
    "dev_file = \"../../../data/nmt/en-cn/dev.txt\"\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "0Yd2oM5OgmuQ",
    "outputId": "84466bbd-0c7e-41a6-a056-f706e50833b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'], ['BOS', 'how', 'about', 'another', 'piece', 'of', 'cake', '?', 'EOS'], ['BOS', 'she', 'married', 'him', '.', 'EOS'], ['BOS', 'i', 'do', \"n't\", 'like', 'learning', 'irregular', 'verbs', '.', 'EOS'], ['BOS', 'it', \"'s\", 'a', 'whole', 'new', 'ball', 'game', 'for', 'me', '.', 'EOS'], ['BOS', 'he', \"'s\", 'sleeping', 'like', 'a', 'baby', '.', 'EOS'], ['BOS', 'he', 'can', 'play', 'both', 'tennis', 'and', 'baseball', '.', 'EOS'], ['BOS', 'we', 'should', 'cancel', 'the', 'hike', '.', 'EOS'], ['BOS', 'he', 'is', 'good', 'at', 'dealing', 'with', 'children', '.', 'EOS'], ['BOS', 'she', 'will', 'do', 'her', 'best', 'to', 'be', 'here', 'on', 'time', '.', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "print(train_en[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "pUcyBMV5m-Ep",
    "outputId": "b03f5040-f3ef-4d1e-8768-b7947779e62c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BOS', '任', '何', '人', '都', '可', '以', '做', '到', '。', 'EOS'], ['BOS', '要', '不', '要', '再', '來', '一', '塊', '蛋', '糕', '？', 'EOS'], ['BOS', '她', '嫁', '给', '了', '他', '。', 'EOS'], ['BOS', '我', '不', '喜', '欢', '学', '习', '不', '规', '则', '动', '词', '。', 'EOS'], ['BOS', '這', '對', '我', '來', '說', '是', '個', '全', '新', '的', '球', '類', '遊', '戲', '。', 'EOS'], ['BOS', '他', '正', '睡', '着', '，', '像', '个', '婴', '儿', '一', '样', '。', 'EOS'], ['BOS', '他', '既', '会', '打', '网', '球', '，', '又', '会', '打', '棒', '球', '。', 'EOS'], ['BOS', '我', '們', '應', '該', '取', '消', '這', '次', '遠', '足', '。', 'EOS'], ['BOS', '他', '擅', '長', '應', '付', '小', '孩', '子', '。', 'EOS'], ['BOS', '她', '会', '尽', '量', '按', '时', '赶', '来', '的', '。', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "print(train_cn[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uR0e5peXgen8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ixbHdLqcoNu"
   },
   "source": [
    "构建单词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "AneLf_O8coNv",
    "outputId": "52d5e2ac-d09a-44cd-d8d8-8a7897b17b5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5492\n",
      "3193\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "def build_dict(sentences, max_words=50000):\n",
    "    word_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s] += 1  #word_count这里应该是个字典\n",
    "    ls = word_count.most_common(max_words) \n",
    "    #按每个单词数量排序前50000个,这个数字自己定的，不重复单词数没有50000\n",
    "    print(len(ls)) #train_en：5491\n",
    "    total_words = len(ls) + 2\n",
    "    #加的2是留给\"unk\"和\"pad\"\n",
    "    #ls = [('BOS', 14533), ('EOS', 14533), ('.', 12521), ('i', 4045), .......\n",
    "    word_dict = {w[0]: index+2 for index, w in enumerate(ls)}\n",
    "    #加的2是留给\"unk\"和\"pad\",转换成字典格式。\n",
    "    word_dict[\"UNK\"] = UNK_IDX\n",
    "    word_dict[\"PAD\"] = PAD_IDX\n",
    "    return word_dict, total_words\n",
    "\n",
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
    "#en_dict.items()把字典转换成可迭代对象，取出键值，并调换键值的位置。\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "2GyNIK_skLlS",
    "outputId": "f45126c1-867e-4fbb-b149-431d0f31902b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method items of dict object at 0x7f64113b2510>\n",
      "5494\n"
     ]
    }
   ],
   "source": [
    "print(en_dict.items)\n",
    "print(en_total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "B96_o-4xnBAa",
    "outputId": "f2241a53-8b2a-4cca-f8d9-31104f22d493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method items of dict object at 0x7f640fe83d80>\n",
      "3195\n"
     ]
    }
   ],
   "source": [
    "print(cn_dict.items)\n",
    "print(cn_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "1FfcIeEDlARY",
    "outputId": "1055afca-e8d2-41cf-cf35-4bd287d418e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method items of dict object at 0x7f641178c360>\n"
     ]
    }
   ],
   "source": [
    "print(inv_en_dict.items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "EinIXvxWnDlh",
    "outputId": "a3e7ad71-702a-4edc-9c84-0252c1af896e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method items of dict object at 0x7f641183b798>\n"
     ]
    }
   ],
   "source": [
    "print(inv_cn_dict.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Q8vXj2EcoNx"
   },
   "source": [
    "把单词全部转变成数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SJWu3OAGcoNy"
   },
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "    '''\n",
    "        Encode the sequences. \n",
    "    '''\n",
    "    length = len(en_sentences)\n",
    "    #en_sentences=[['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'],....\n",
    "    \n",
    "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    #out_en_sentences=[[2, 328, 43, 14, 28, 4, 3], ....\n",
    "    #.get(w, 0)，返回w对应的值，没有就为0.因题库比较小，这里所有的单词向量都有非零索引。\n",
    "    \n",
    " \n",
    "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "\n",
    "    # sort sentences by english lengths\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "      #sorted()排序,key参数可以自定义规则，按seq[x]的长度排序，seq[0]为第一句话长度\n",
    "       \n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "    #print(sorted_index)\n",
    "    #sorted_index=[63, 1544, 1917, 2650, 3998, 6240, 6294, 6703, ....\n",
    "     #前面的索引都是最短句子的索引\n",
    "      \n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "     #print(out_en_sentences)\n",
    "     #out_en_sentences=[[2, 475, 4, 3], [2, 1318, 126, 3], [2, 1707, 126, 3], ......\n",
    "     \n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "        \n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "ZBR_goyX12Qw",
    "outputId": "e032b5b3-ddda-4af2-f6ea-c702de189713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS 他 来 这 里 的 目 的 是 什 么 ？ EOS\n",
      "BOS for what purpose did he come here ? EOS\n"
     ]
    }
   ],
   "source": [
    "k=10000\n",
    "print(\" \".join([inv_cn_dict[i] for i in train_cn[k]])) #通过inv字典获取单词\n",
    "print(\" \".join([inv_en_dict[i] for i in train_en[k]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jd4YrqNPcoN5"
   },
   "source": [
    "把全部句子分成batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "2Ugb6nb1Zn8X",
    "outputId": "fced3673-653d-4066-d772-1b15322821ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 15 30 45 60 75 90]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n"
     ]
    }
   ],
   "source": [
    "print(np.arange(0, 100, 15))\n",
    "print(np.arange(0, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xP9XSbJHcoN6"
   },
   "outputs": [],
   "source": [
    "def get_minibatches(n, minibatch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, minibatch_size) # [0, 1, ..., n-1]\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list) #打乱数据\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "        #所有batch放在一个大列表里\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "cLHziG0aZtwr",
    "outputId": "550da2b4-cd6e-4ece-c198-ad525e9f740b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
       " array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]),\n",
       " array([60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]),\n",
       " array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       " array([45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]),\n",
       " array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_minibatches(100,15) #随机打乱的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33721
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "bzMTd27YZqcf",
    "outputId": "b87419a2-6fcd-4144-eab7-edee2523b129"
   },
   "outputs": [],
   "source": [
    "def prepare_data(seqs):\n",
    "#seqs=[[2, 12, 167, 23, 114, 5, 27, 1755, 4, 3], ........\n",
    "    lengths = [len(seq) for seq in seqs]#每个batch里语句的长度统计出来\n",
    "    n_samples = len(seqs) #一个batch有多少语句\n",
    "    max_len = np.max(lengths) #取出最长的的语句长度，后面用这个做padding基准\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    #先初始化全零矩阵，后面依次赋值\n",
    "    #print(x.shape) #64*最大句子长度\n",
    "    \n",
    "    x_lengths = np.array(lengths).astype(\"int32\")\n",
    "    #print(x_lengths) \n",
    "#这里看下面的输入语句发现英文句子长度都一样，中文句子长短不一。\n",
    "#说明英文句子是特征，中文句子是标签。\n",
    "\n",
    "\n",
    "    for idx, seq in enumerate(seqs):\n",
    "      #取出一个batch的每条语句和对应的索引\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "        #每条语句按行赋值给x，x会有一些零值没有被赋值。\n",
    "        \n",
    "    return x, x_lengths #x_mask\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        #按打乱的batch序号分数据，打乱只是batch打乱，一个batach里面的语句还是顺序的。\n",
    "        #print(mb_en_sentences)\n",
    "        \n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
    "        #返回的维度为：mb_x=(64 * 最大句子长度）,mb_x_len=最大句子长度\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        \n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "        #这里把所有batch数据集合到一起。\n",
    "        #依次为英文句子，英文长度，中文句子翻译，中文句子长度，这四个放在一个列表中\n",
    "        #一个列表为一个batch的数据，所有batch组成一个大列表数据\n",
    "    return all_ex\n",
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)\n",
    "random.shuffle(train_data)\n",
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1407
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "wBE0PAtSL_iC",
    "outputId": "544cfcd0-26aa-4939-e76d-72b0feafa636"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   2,   29,   47,   53,    6,  339,  161,  135,    4,    3],\n",
       "        [   2,  389,    5,   97,   46,   34,    6,  760,    4,    3],\n",
       "        [   2,   12,   10,  199,    7,  498,   15,  924,    4,    3],\n",
       "        [   2,    5, 1462,   46,   37,    7,   39,  176,    4,    3],\n",
       "        [   2,    5,   42,  105,  712,   17,   21,  803,    4,    3],\n",
       "        [   2,    5,   81,   12,   47,   14,   35,  228,    4,    3],\n",
       "        [   2,   34,  106,   24,   29,  139,  192,  207,    4,    3],\n",
       "        [   2,   31, 1251,   14,    8,   36,    6,  228,   11,    3],\n",
       "        [   2,   29,  821,  507,   69,    6, 5317,  973,    4,    3],\n",
       "        [   2,   29,   22,    7,   14,  150,   24,   18,    4,    3],\n",
       "        [   2,   58, 2050,    6,  921,  165,  166, 3072,    4,    3],\n",
       "        [   2,   18,  385,  106,  117,  103,   34, 1889,    4,    3],\n",
       "        [   2,   45,  204,  161,   51,   97,    8,   59,    4,    3],\n",
       "        [   2,  321,  489,   10,   66,   26,   32,  477,    4,    3],\n",
       "        [   2,    5,   76, 1885,   28,   12,   10, 1636,    4,    3],\n",
       "        [   2,    5,  547,    7,    6,  432,  158,  295,    4,    3],\n",
       "        [   2,   31,   14,    8,  130,   54,    7,  148,   11,    3],\n",
       "        [   2,   44,   10,    9, 1317, 2158,    6,  978,    4,    3],\n",
       "        [   2,   18,  199,    7,  268,   36,    9,  489,    4,    3],\n",
       "        [   2,   28,  470,   27,   50,  444,    6,  574,    4,    3],\n",
       "        [   2,   18,  163,    9, 1593,  175,   35,   96,    4,    3],\n",
       "        [   2,   12,   10,    6,  100,   17,  134,  157,    4,    3],\n",
       "        [   2,    6, 5324,   10,  128,  535,   26,   23,    4,    3],\n",
       "        [   2,   29,   86,  794,    7,  284,    6,  473,    4,    3],\n",
       "        [   2,    7,   39,  120,  893,   10,   37,  286,    4,    3],\n",
       "        [   2,  472,   18,   49,   75,   43,  139,  207,    4,    3],\n",
       "        [   2,   31,   22,    8,  297,   38,   21,  644,   11,    3],\n",
       "        [   2,   44,   27,   13,  756,   15,    6,  133,    4,    3],\n",
       "        [   2,  681,   10,  190,   24,   10,   13,   16,   11,    3],\n",
       "        [   2,   37,    9, 2156,   27,    7,   39,  277,    4,    3],\n",
       "        [   2,   18, 2939,    6,  710,   33,    6,  378,    4,    3],\n",
       "        [   2,   12,   10,  692,    7,  284,    6,  473,    4,    3],\n",
       "        [   2,  288,  214,   24,   29,   43,   37,  938,    4,    3],\n",
       "        [   2,    6,  223,  187,   99,    7,   39,  556,    4,    3],\n",
       "        [   2,   21,  355,  349,  463,   26,  125,  155,    4,    3],\n",
       "        [   2,   25,  474,  262,   13,  181,  782,  112,    4,    3],\n",
       "        [   2,   43,    8,  141,  312,  959,   15,   77,   11,    3],\n",
       "        [   2,    5,   42,  692,    7,   97,    8,  142,    4,    3],\n",
       "        [   2,    5,   22,    9, 1690,   15,   21, 2013,    4,    3],\n",
       "        [   2,   12,  349,    7,   39, 5339,  155,  145,    4,    3],\n",
       "        [   2,   44,   27,   71,  214,   15,    6,  319,    4,    3],\n",
       "        [   2,   75, 5342,    6, 2114,   30,  510,   40,    4,    3],\n",
       "        [   2,    8,  143,  818, 1577,   94,    8,   36,    4,    3],\n",
       "        [   2,   18,   70,   13,   22,    9, 5345, 3118,    4,    3],\n",
       "        [   2,  568,    9,  970,   24,   12, 1701,   74,    4,    3],\n",
       "        [   2,   18,   93,   98,  591,   33,    6,  623,    4,    3],\n",
       "        [   2,  114,   41,    8,   72,   59,  105,  291,   11,    3],\n",
       "        [   2,   16,   10,  934,    7,  247,   15,  184,    4,    3],\n",
       "        [   2,    8,   14,   13,   22,    7, 2267,  312,    4,    3],\n",
       "        [   2,   19,   49,    5,   30,  191,   49,  235,    4,    3],\n",
       "        [   2,    6,  215, 1995,  572,    7,    6,  432,    4,    3],\n",
       "        [   2,    5,  211,    5,   85,  236,   28,  632,    4,    3],\n",
       "        [   2,    5,  205,   25,   94,    6,  237,  103,    4,    3],\n",
       "        [   2,  453, 2164,   30,  279,   74,   17, 1281,    4,    3],\n",
       "        [   2,    5,  870,   57,    6, 1473,   38,  214,    4,    3],\n",
       "        [   2,   29,   43,  449,   88,   29,   54,    7,    4,    3],\n",
       "        [   2,    5,   63,   71,  529, 1564,   35,  422,    4,    3],\n",
       "        [   2,    5,   67,   13,   22,  187,    8,  151,    4,    3],\n",
       "        [   2,   51,   90,   37,  226,   17,  178,  107,    4,    3],\n",
       "        [   2,   12,   27, 1054,   15,    9,   96,  373,    4,    3],\n",
       "        [   2,   12,   10,    6, 2606,  152,   33, 1018,    4,    3],\n",
       "        [   2,   91,   20,   73,  543,  218,  156,  193,    4,    3],\n",
       "        [   2,   21, 1337,   10,   33,    6, 5361,  697,    4,    3],\n",
       "        [   2,    5,  205,    9,  138,  555,  101,  363,    4,    3]],\n",
       "       dtype=int32),\n",
       " array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], dtype=int32),\n",
       " array([[  2,   5,  24, ...,   0,   0,   0],\n",
       "        [  2,   5, 833, ...,   3,   0,   0],\n",
       "        [  2,   9, 416, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  2, 179,   5, ...,   0,   0,   0],\n",
       "        [  2,   5,   6, ...,   0,   0,   0],\n",
       "        [  2,  30,  32, ...,   3,   0,   0]], dtype=int32),\n",
       " array([12, 14, 13, 12, 15, 12, 13, 11, 14, 15, 11, 13, 15, 12, 11, 11, 11,\n",
       "        11, 12, 12, 10, 12, 14, 14, 13, 16, 13,  8, 15, 10, 12, 13, 13, 12,\n",
       "        14, 13, 14, 10, 13, 10, 10, 14, 15, 10, 12, 15, 13, 11, 10,  9, 11,\n",
       "        13, 12, 10, 11, 14, 15, 13, 12, 10, 13, 11, 10, 14], dtype=int32))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z_ncYOpDcoN-"
   },
   "source": [
    "### 没有Attention的版本\n",
    "下面是一个更简单的没有Attention的encoder decoder模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fBf8n8o8coN_"
   },
   "outputs": [],
   "source": [
    "class PlainEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        #以英文为例，vocab_size=5493, hidden_size=100, dropout=0.2\n",
    "        super(PlainEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        #这里的hidden_size为embedding_dim：一个单词的维度 \n",
    "        #torch.nn.Embedding(num_embeddings, embedding_dim, .....)\n",
    "        #这里的hidden_size = 100\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)      \n",
    "        #第一个参数为input_size ：输入特征数量\n",
    "        #第二个参数为hidden_size ：隐藏层特征数量\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths): \n",
    "        #x是输入的batch的所有单词，lengths：batch里每个句子的长度\n",
    "        #因为需要把最后一个hidden state取出来，需要知道长度，因为句子长度不一样\n",
    "        ##print(x.shape,lengths),x.sahpe = torch.Size([64, 10])\n",
    "        # lengths= =tensor([10, 10, 10, ..... 10, 10, 10])\n",
    "        \n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        #按照长度排序，descending=True长的在前。\n",
    "        #返回两个参数，句子长度和未排序前的索引\n",
    "        # sorted_idx=tensor([41, 40, 46, 45,...... 19, 18, 63])\n",
    "        # sorted_len=tensor([10, 10, 10, ..... 10, 10, 10])\n",
    "        \n",
    "        x_sorted = x[sorted_idx.long()] #句子用新的idx，按长度排好序了\n",
    "        \n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        #print(embedded.shape)=torch.Size([64, 10, 100])\n",
    "        #tensor([[[-0.6312, -0.9863, -0.3123,  ..., -0.7384,  0.9230, -0.4311],....\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        #这个函数就是用来处理不同长度的句子的，https: // www.cnblogs.com / sbj123456789 / p / 9834018. html\n",
    "\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        #hid.shape = torch.Size([1, 64, 100])\n",
    "        \n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        #out.shape = torch.Size([64, 10, 100]),\n",
    "\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        #out.shape = torch.Size([64, 10, 100])\n",
    "        #hid.shape = torch.Size([1, 64, 100])\n",
    "        \n",
    "        return out, hid[[-1]] #有时候num_layers层数多，需要取出最后一层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fBf8n8o8coN_"
   },
   "outputs": [],
   "source": [
    "class PlainDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, y, y_lengths, hid):\n",
    "        #print(y.shape)=torch.Size([64, 12])\n",
    "        #print(hid.shape)=torch.Size([1, 64, 100])\n",
    "        #中文的y和y_lengths\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()] #隐藏层也要排序\n",
    "\n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) \n",
    "        # batch_size, output_length, embed_size\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid) #加上隐藏层\n",
    "        #print(hid.shape)=torch.Size([1, 64, 100])\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        #print(output_seq.shape)=torch.Size([64, 12, 100])\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        #print(hid.shape)=torch.Size([1, 64, 100])\n",
    "        output = F.log_softmax(self.out(output_seq), -1)\n",
    "        #print(output.shape)=torch.Size([64, 12, 3195])\n",
    "        \n",
    "        return output, hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fBf8n8o8coN_"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PlainSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        #encoder是上面PlainEncoder的实例\n",
    "        #decoder是上面PlainDecoder的实例\n",
    "        super(PlainSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "       \n",
    "    #把两个模型串起来 \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        #self.encoder(x, x_lengths)调用PlainEncoder里面forward的方法\n",
    "        #返回forward的out和hid\n",
    "        \n",
    "        output, hid = self.decoder(y=y,y_lengths=y_lengths,hid=hid)\n",
    "        #self.dencoder()调用PlainDecoder里面forward的方法\n",
    "        \n",
    "        return output, None\n",
    "\n",
    "    def translate(self, x, x_lengths, y, max_length=10):\n",
    "        #x是一个句子，用数值表示\n",
    "        #y是句子的长度\n",
    "        #y是“bos”的数值索引=2\n",
    "        \n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid = self.decoder(y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid) \n",
    "            \n",
    "#刚开始循环bos作为模型的首个输入单词，后续更新y，下个预测单词的输入是上个输出单词\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            \n",
    "        return torch.cat(preds, 1), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "vGykJm8ucoOE"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dropout = 0.2\n",
    "hidden_size = 100\n",
    "\n",
    "#传入中文和英文参数\n",
    "encoder = PlainEncoder(vocab_size=en_total_words,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "decoder = PlainDecoder(vocab_size=cn_total_words,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "model = PlainSeq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kF-lEtsVcoOB"
   },
   "outputs": [],
   "source": [
    "# masked cross entropy loss\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        #target=tensor([[5,108,8,4,3,0,0,0,0,0,0,0],....\n",
    "        #  mask=tensor([[1,1 ,1,1,1,0,0,0,0,0,0,0],.....\n",
    "        #print(input.shape,target.shape,mask.shape)\n",
    "        #torch.Size([64, 12, 3195]) torch.Size([64, 12]) torch.Size([64, 12])\n",
    "        \n",
    "        # input: (batch_size * seq_len) * vocab_size\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        \n",
    "        # target: batch_size * 1=768*1\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        #print(-input.gather(1, target))\n",
    "        output = -input.gather(1, target) * mask\n",
    "#这里算得就是交叉熵损失，前面已经算了F.log_softmax\n",
    "#.gather的作用https://blog.csdn.net/edogawachia/article/details/80515038\n",
    "#output.shape=torch.Size([768, 1])\n",
    "#mask作用是把padding为0的地方重置为零，因为input.gather时，为0的地方不是零了\n",
    "        \n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "        #均值损失\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "vGykJm8ucoOE"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():#不需要更新模型，不需要梯度\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "    print(\"Evaluation loss\", total_loss/total_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "bZdgC77scoOI",
    "outputId": "f96af174-87ac-4aaa-8550-81894752af7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 8.059293746948242\n",
      "Epoch 0 iteration 100 loss 5.432771682739258\n",
      "Epoch 0 iteration 200 loss 4.88300085067749\n",
      "Epoch 0 Training loss 5.438505330550981\n",
      "Evaluation loss 4.798498956811426\n",
      "Epoch 1 iteration 0 loss 4.757434844970703\n",
      "Epoch 1 iteration 100 loss 4.873423099517822\n",
      "Epoch 1 iteration 200 loss 4.3850812911987305\n",
      "Epoch 1 Training loss 4.5758597215799455\n"
     ]
    }
   ],
   "source": [
    "def train(model, data, num_epochs=2):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            #（英文batch，英文长度，中文batch，中文长度）\n",
    "            \n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            \n",
    "            #前n-1个单词作为输入，后n-1个单词作为输出，因为输入的前一个单词要预测后一个单词\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            #\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            #输入输出的长度都减一。\n",
    "            \n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            #返回的是类PlainSeq2Seq里forward函数的两个返回值\n",
    "            \n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "#mb_out_mask=tensor([[1, 1, 1,  ..., 0, 0, 0],[1, 1, 1,  ..., 0, 0, 0],\n",
    "#mb_out_mask.shape= (64*19),这句代码咱不懂，这个mask就是padding的位置设置为0，其他设置为1\n",
    "#mb_out_mask就是LanguageModelCriterion的传入参数mask。\n",
    "\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            #一个batch里多少个单词\n",
    "            \n",
    "            total_loss += loss.item() * num_words\n",
    "            #总损失，loss计算的是均值损失，每个单词都是都有损失，所以乘以单词数\n",
    "            \n",
    "            total_num_words += num_words\n",
    "            #总单词数\n",
    "            \n",
    "            # 更新模型\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            #为了防止梯度过大，设置梯度的阈值\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                print(\"Epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
    "\n",
    "                \n",
    "        print(\"Epoch\", epoch, \"Training loss\", total_loss/total_num_words)\n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data) #评估模型\n",
    "train(model, train_data, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Nii8-NKpcoOG"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "dvf_KTw8coOL",
    "outputId": "87d8055d-9e08-41ba-d05e-e95887725457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮 膚 真 好 。 EOS\n",
      "你的你的。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 部 分 正 确 。 EOS\n",
      "你不知道你的。\n",
      "\n",
      "BOS everyone admired his courage . EOS\n",
      "BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS\n",
      "他的房子。\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几 点 了 ？ EOS\n",
      "这个人。\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今 晚 有 空 。 EOS\n",
      "我不是我的。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這 是 你 的 書 。 EOS\n",
      "他的。\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他 们 在 吃 午 饭 。 EOS\n",
      "他們的。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這 把 椅 子 很 UNK 。 EOS\n",
      "我們是我的。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 真 重 。 EOS\n",
      "我的。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS\n",
      "他們在這裡。\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训 练 。 EOS\n",
      "我们的。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有 人 在 看 著 你 。 EOS\n",
      "你的。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我不是我的。\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜 歡 流 行 音 樂 。 EOS\n",
      "我不是我的。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS T o m 沒 有 孩 子 。 EOS\n",
      "汤姆在哪裡。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 門 鎖 上 。 EOS\n",
      "我們了。\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤 姆 冷 静 下 来 了 。 EOS\n",
      "汤姆在這個。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大 聲 一 點 兒 。 EOS\n",
      "我們在這個人。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周 日 空 出 来 。 EOS\n",
      "他的房子。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS 我 犯 了 一 個 錯 。 EOS\n",
      "我不是我的。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#翻译个句子看看结果咋样\n",
    "def translate_dev(i):\n",
    "    #随便取出句子\n",
    "    en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])\n",
    "    print(\"\".join(cn_sent))\n",
    "\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    #把句子升维，并转换成tensor\n",
    "    \n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    #取出句子长度，并转换成tensor\n",
    "    \n",
    "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "    #bos=tensor([[2]])\n",
    "\n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "    #这里传入bos作为首个单词的输入\n",
    "    #translation=tensor([[ 8,  6, 11, 25, 22, 57, 10,  5,  6,  4]])\n",
    "    \n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\": # 把数值变成单词形式\n",
    "            trans.append(word) #\n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))\n",
    "\n",
    "for i in range(100,120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-lT6Cw-coOP"
   },
   "source": [
    "数据全部处理完成，现在我们开始构建seq2seq模型\n",
    "\n",
    "#### Encoder\n",
    "- Encoder模型的任务是把输入文字传入embedding层和GRU层，转换成一些hidden states作为后续的context vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下面的注释我先把原理捋清楚吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5qt-lDm8coOP"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        \n",
    "        hid = torch.cat([hid[-2], hid[-1]], dim=1)\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)\n",
    "\n",
    "        return out, hid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PunwHJfcoOT"
   },
   "source": [
    "#### Luong Attention\n",
    "- 根据context vectors和当前的输出hidden states，计算输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "16G1_LB0coOU"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "\n",
    "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size*2 + dec_hidden_size, dec_hidden_size)\n",
    "        \n",
    "    def forward(self, output, context, mask):\n",
    "        # output: batch_size, output_len, dec_hidden_size\n",
    "        # context: batch_size, context_len, 2*enc_hidden_size\n",
    "    \n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = context.size(1)\n",
    "        \n",
    "        context_in = self.linear_in(context.view(batch_size*input_len, -1)).view(                \n",
    "            batch_size, input_len, -1) # batch_size, context_len, dec_hidden_size\n",
    "        \n",
    "        # context_in.transpose(1,2): batch_size, dec_hidden_size, context_len \n",
    "        # output: batch_size, output_len, dec_hidden_size\n",
    "        attn = torch.bmm(output, context_in.transpose(1,2)) \n",
    "        # batch_size, output_len, context_len\n",
    "\n",
    "        attn.data.masked_fill(mask, -1e6)\n",
    "\n",
    "        attn = F.softmax(attn, dim=2) \n",
    "        # batch_size, output_len, context_len\n",
    "\n",
    "        context = torch.bmm(attn, context) \n",
    "        # batch_size, output_len, enc_hidden_size\n",
    "        \n",
    "        output = torch.cat((context, output), dim=2) # batch_size, output_len, hidden_size*2\n",
    "\n",
    "        output = output.view(batch_size*output_len, -1)\n",
    "        output = torch.tanh(self.linear_out(output))\n",
    "        output = output.view(batch_size, output_len, -1)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LzcOPJf_coOX"
   },
   "source": [
    "#### Decoder\n",
    "- decoder会根据已经翻译的句子内容，和context vectors，来决定下一个输出的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "gX8flwywcoOY"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_mask(self, x_len, y_len):\n",
    "        # a mask of shape x_len * y_len\n",
    "        device = x_len.device\n",
    "        max_x_len = x_len.max()\n",
    "        max_y_len = y_len.max()\n",
    "        x_mask = torch.arange(max_x_len, device=x_len.device)[None, :] < x_len[:, None]\n",
    "        y_mask = torch.arange(max_y_len, device=x_len.device)[None, :] < y_len[:, None]\n",
    "        #print(\"x_mask[:, :, None] * y_mask[:, None, :]\", x_mask[:, :, None] * y_mask[:, None, :])\n",
    "        mask = ((~ (x_mask[:, :, None] * y_mask[:, None, :])))   #.byte()\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, ctx, ctx_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "        \n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "\n",
    "        mask = self.create_mask(y_lengths, ctx_lengths)\n",
    "\n",
    "        output, attn = self.attention(output_seq, ctx, mask)\n",
    "        output = F.log_softmax(self.out(output), -1)\n",
    "        \n",
    "        return output, hid, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1kfN48PjcoOe"
   },
   "source": [
    "#### Seq2Seq\n",
    "- 最后我们构建Seq2Seq模型把encoder, attention, decoder串到一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xmsUCoGGcoOf"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)\n",
    "        return output, attn\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhMzsG85coOk"
   },
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jar62Gi3coOl"
   },
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "embed_size = hidden_size = 100\n",
    "encoder = Encoder(vocab_size=en_total_words,\n",
    "                       embed_size=embed_size,\n",
    "                      enc_hidden_size=hidden_size,\n",
    "                       dec_hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "decoder = Decoder(vocab_size=cn_total_words,\n",
    "                      embed_size=embed_size,\n",
    "                      enc_hidden_size=hidden_size,\n",
    "                       dec_hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "HOxvWAiscoOo",
    "outputId": "bc6853a1-b681-4da7-9e1e-cc82e2361995",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 8.081789016723633\n",
      "Epoch 0 iteration 100 loss 5.539219379425049\n",
      "Epoch 0 iteration 200 loss 5.1249260902404785\n",
      "Epoch 0 Training loss 5.491253565788245\n",
      "Evaluation loss 5.081770719956435\n",
      "Epoch 1 iteration 0 loss 5.090885639190674\n",
      "Epoch 1 iteration 100 loss 5.215909004211426\n",
      "Epoch 1 iteration 200 loss 4.773082733154297\n",
      "Epoch 1 Training loss 4.925395762876349\n",
      "Epoch 2 iteration 0 loss 4.672070026397705\n",
      "Epoch 2 iteration 100 loss 4.832282543182373\n",
      "Epoch 2 iteration 200 loss 4.35097599029541\n",
      "Epoch 2 Training loss 4.514929467918747\n",
      "Epoch 3 iteration 0 loss 4.2532219886779785\n",
      "Epoch 3 iteration 100 loss 4.490896224975586\n",
      "Epoch 3 iteration 200 loss 4.033875465393066\n",
      "Epoch 3 Training loss 4.1428118343943225\n",
      "Epoch 4 iteration 0 loss 3.8752641677856445\n",
      "Epoch 4 iteration 100 loss 4.239563941955566\n",
      "Epoch 4 iteration 200 loss 3.760349988937378\n",
      "Epoch 4 Training loss 3.8646895561183157\n",
      "Epoch 5 iteration 0 loss 3.6249732971191406\n",
      "Epoch 5 iteration 100 loss 4.023530960083008\n",
      "Epoch 5 iteration 200 loss 3.6012301445007324\n",
      "Epoch 5 Training loss 3.6404324540550013\n",
      "Evaluation loss 3.6434319625311873\n",
      "Epoch 6 iteration 0 loss 3.425724506378174\n",
      "Epoch 6 iteration 100 loss 3.8447866439819336\n",
      "Epoch 6 iteration 200 loss 3.4139957427978516\n",
      "Epoch 6 Training loss 3.450604616661144\n",
      "Epoch 7 iteration 0 loss 3.2418251037597656\n",
      "Epoch 7 iteration 100 loss 3.742126226425171\n",
      "Epoch 7 iteration 200 loss 3.280479907989502\n",
      "Epoch 7 Training loss 3.2859292368801514\n",
      "Epoch 8 iteration 0 loss 3.072279453277588\n",
      "Epoch 8 iteration 100 loss 3.586132526397705\n",
      "Epoch 8 iteration 200 loss 3.159052848815918\n",
      "Epoch 8 Training loss 3.142630494437046\n",
      "Epoch 9 iteration 0 loss 2.942410945892334\n",
      "Epoch 9 iteration 100 loss 3.5045480728149414\n",
      "Epoch 9 iteration 200 loss 2.980076789855957\n",
      "Epoch 9 Training loss 3.0134425529697055\n",
      "Epoch 10 iteration 0 loss 2.8081154823303223\n",
      "Epoch 10 iteration 100 loss 3.398869752883911\n",
      "Epoch 10 iteration 200 loss 2.9181597232818604\n",
      "Epoch 10 Training loss 2.898613956175173\n",
      "Evaluation loss 3.1743306433181635\n",
      "Epoch 11 iteration 0 loss 2.671004295349121\n",
      "Epoch 11 iteration 100 loss 3.3191592693328857\n",
      "Epoch 11 iteration 200 loss 2.8188095092773438\n",
      "Epoch 11 Training loss 2.7938946419825355\n",
      "Epoch 12 iteration 0 loss 2.550006151199341\n",
      "Epoch 12 iteration 100 loss 3.232865810394287\n",
      "Epoch 12 iteration 200 loss 2.707446336746216\n",
      "Epoch 12 Training loss 2.703291960081002\n",
      "Epoch 13 iteration 0 loss 2.4467732906341553\n",
      "Epoch 13 iteration 100 loss 3.155334949493408\n",
      "Epoch 13 iteration 200 loss 2.629235029220581\n",
      "Epoch 13 Training loss 2.6152714541308613\n",
      "Epoch 14 iteration 0 loss 2.3739211559295654\n",
      "Epoch 14 iteration 100 loss 3.09279465675354\n",
      "Epoch 14 iteration 200 loss 2.569971799850464\n",
      "Epoch 14 Training loss 2.5361732201089917\n",
      "Epoch 15 iteration 0 loss 2.2947962284088135\n",
      "Epoch 15 iteration 100 loss 3.019110918045044\n",
      "Epoch 15 iteration 200 loss 2.520444869995117\n",
      "Epoch 15 Training loss 2.4634666015863473\n",
      "Evaluation loss 2.979345819592713\n",
      "Epoch 16 iteration 0 loss 2.2439687252044678\n",
      "Epoch 16 iteration 100 loss 3.0242812633514404\n",
      "Epoch 16 iteration 200 loss 2.4449751377105713\n",
      "Epoch 16 Training loss 2.3976736799380074\n",
      "Epoch 17 iteration 0 loss 2.150400161743164\n",
      "Epoch 17 iteration 100 loss 2.9335732460021973\n",
      "Epoch 17 iteration 200 loss 2.3292551040649414\n",
      "Epoch 17 Training loss 2.334660070175777\n",
      "Epoch 18 iteration 0 loss 2.059173822402954\n",
      "Epoch 18 iteration 100 loss 2.8991899490356445\n",
      "Epoch 18 iteration 200 loss 2.3174326419830322\n",
      "Epoch 18 Training loss 2.272283745733361\n",
      "Epoch 19 iteration 0 loss 2.0220999717712402\n",
      "Epoch 19 iteration 100 loss 2.837592601776123\n",
      "Epoch 19 iteration 200 loss 2.3137118816375732\n",
      "Epoch 19 Training loss 2.222704250690952\n",
      "Epoch 20 iteration 0 loss 2.015784978866577\n",
      "Epoch 20 iteration 100 loss 2.780117988586426\n",
      "Epoch 20 iteration 200 loss 2.193133592605591\n",
      "Epoch 20 Training loss 2.165715104051114\n",
      "Evaluation loss 2.882689331596724\n",
      "Epoch 21 iteration 0 loss 1.9079537391662598\n",
      "Epoch 21 iteration 100 loss 2.7783422470092773\n",
      "Epoch 21 iteration 200 loss 2.1648731231689453\n",
      "Epoch 21 Training loss 2.1148204406187365\n",
      "Epoch 22 iteration 0 loss 1.8573297262191772\n",
      "Epoch 22 iteration 100 loss 2.7379467487335205\n",
      "Epoch 22 iteration 200 loss 2.1029839515686035\n",
      "Epoch 22 Training loss 2.0730350350181332\n",
      "Epoch 23 iteration 0 loss 1.8508738279342651\n",
      "Epoch 23 iteration 100 loss 2.686796188354492\n",
      "Epoch 23 iteration 200 loss 2.1001081466674805\n",
      "Epoch 23 Training loss 2.0255594717112584\n",
      "Epoch 24 iteration 0 loss 1.7573014497756958\n",
      "Epoch 24 iteration 100 loss 2.6178905963897705\n",
      "Epoch 24 iteration 200 loss 1.9956778287887573\n",
      "Epoch 24 Training loss 1.9826103644946547\n",
      "Epoch 25 iteration 0 loss 1.7210495471954346\n",
      "Epoch 25 iteration 100 loss 2.535670757293701\n",
      "Epoch 25 iteration 200 loss 1.9728033542633057\n",
      "Epoch 25 Training loss 1.9416777706836803\n",
      "Evaluation loss 2.831404876071352\n",
      "Epoch 26 iteration 0 loss 1.6944167613983154\n",
      "Epoch 26 iteration 100 loss 2.583125591278076\n",
      "Epoch 26 iteration 200 loss 1.9500846862792969\n",
      "Epoch 26 Training loss 1.9058225125182218\n",
      "Epoch 27 iteration 0 loss 1.6968313455581665\n",
      "Epoch 27 iteration 100 loss 2.5269153118133545\n",
      "Epoch 27 iteration 200 loss 1.9462621212005615\n",
      "Epoch 27 Training loss 1.8722721592530482\n",
      "Epoch 28 iteration 0 loss 1.630069375038147\n",
      "Epoch 28 iteration 100 loss 2.5525941848754883\n",
      "Epoch 28 iteration 200 loss 1.9164413213729858\n",
      "Epoch 28 Training loss 1.836531995434525\n",
      "Epoch 29 iteration 0 loss 1.5969229936599731\n",
      "Epoch 29 iteration 100 loss 2.4575111865997314\n",
      "Epoch 29 iteration 200 loss 1.83981192111969\n",
      "Epoch 29 Training loss 1.8049174632549783\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "ZEhDVMR9coOr",
    "outputId": "549942c9-0c3d-4aaf-b639-10585185b642"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮 膚 真 好 。 EOS\n",
      "你的意見不好。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 部 分 正 确 。 EOS\n",
      "你的兄弟有一双。\n",
      "\n",
      "BOS everyone admired his courage . EOS\n",
      "BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS\n",
      "每個人都很快樂。\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几 点 了 ？ EOS\n",
      "这个时候了？\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今 晚 有 空 。 EOS\n",
      "我今晚在家。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這 是 你 的 書 。 EOS\n",
      "這是你的書。\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他 们 在 吃 午 饭 。 EOS\n",
      "他們在吃午餐。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這 把 椅 子 很 UNK 。 EOS\n",
      "这个椅子是三明治。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 真 重 。 EOS\n",
      "它是很容易。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS\n",
      "多少的事情是他的一個小時。\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训 练 。 EOS\n",
      "已经将起来很好。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有 人 在 看 著 你 。 EOS\n",
      "有人是你的。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我相信他有趣。\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜 歡 流 行 音 樂 。 EOS\n",
      "我喜欢音乐。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS T o m 沒 有 孩 子 。 EOS\n",
      "汤姆没有孩子。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 門 鎖 上 。 EOS\n",
      "請把門關門。\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤 姆 冷 静 下 来 了 。 EOS\n",
      "汤姆在眼鏡。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大 聲 一 點 兒 。 EOS\n",
      "請講大聲一點。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周 日 空 出 来 。 EOS\n",
      "继续下來。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS 我 犯 了 一 個 錯 。 EOS\n",
      "我犯了一个錯誤。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq-updated.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
