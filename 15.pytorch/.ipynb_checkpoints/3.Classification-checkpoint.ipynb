{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 2])\n",
      "torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# 假数据\n",
    "n_data = torch.ones(100, 2)         # 数据的基本形态\n",
    "x0 = torch.normal(2*n_data, 1)      # 类型0 x data (tensor), shape=(100, 2)\n",
    "y0 = torch.zeros(100)               # 类型0 y data (tensor), shape=(100, 1)\n",
    "x1 = torch.normal(-2*n_data, 1)     # 类型1 x data (tensor), shape=(100, 1)\n",
    "y1 = torch.ones(100)                # 类型1 y data (tensor), shape=(100, 1)\n",
    " \n",
    "# 注意 x, y 数据的数据形式是一定要像下面一样 (torch.cat 是在合并数据)\n",
    "x = torch.cat((x0, x1), 0).type(torch.FloatTensor)  # FloatTensor = 32-bit floating\n",
    "y = torch.cat((y0, y1), ).type(torch.LongTensor)    # LongTensor = 64-bit integer\n",
    " \n",
    "# torch 只能在 Variable 上训练, 所以把它们变成 Variable\n",
    "x, y = Variable(x), Variable(y)\n",
    " \n",
    "# plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=y.data.numpy(), s=100, lw=0, cmap=\\'RdYlGn\\')\n",
    "# plt.show()\n",
    " \n",
    "print(x.data.size())\n",
    "print(y.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (out): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F     # 激励函数都在这\n",
    " \n",
    "class Net(torch.nn.Module):     # 继承 torch 的 Module\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()     # 继承 __init__ 功能\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # 隐藏层线性输出\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)       # 输出层线性输出\n",
    " \n",
    "    def forward(self, x):\n",
    "        # 正向传播输入值, 神经网络分析出输出值\n",
    "        x = F.relu(self.hidden(x))      # 激励函数(隐藏层的线性值)\n",
    "        x = self.out(x)                 # 输出值, 但是这个不是预测值, 预测值还需要再另外计算\n",
    "        return x\n",
    " \n",
    "net = Net(n_feature=2, n_hidden=10, n_output=2) # 几个类别就几个 output\n",
    " \n",
    "print(net)  # net 的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6471, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6123, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5805, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5515, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5248, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5003, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4777, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4568, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4195, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4028, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3872, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3726, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3590, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3463, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3343, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3230, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3124, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3024, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2929, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2840, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2755, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2675, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2599, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2527, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2458, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2393, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2331, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2271, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2215, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2160, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2109, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2059, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1966, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1922, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1880, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1840, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1802, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1764, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1729, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1694, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1661, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1629, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1598, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1568, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1540, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1512, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1485, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1459, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1434, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1410, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1386, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1363, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1341, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1320, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1299, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1279, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1259, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1240, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1222, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1204, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1186, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1169, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1153, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1137, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1121, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1106, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1091, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1077, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1063, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1049, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1036, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1023, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1010, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0998, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0985, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0974, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0962, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0951, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0940, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0929, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0919, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0908, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0898, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0888, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0879, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0869, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0860, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0851, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0842, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0834, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0825, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0817, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0809, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0801, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0793, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0785, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0778, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0770, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0763, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0756, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0749, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0742, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0735, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0729, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0722, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0716, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0710, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0704, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0698, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0692, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0686, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0674, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0669, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0664, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0658, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0653, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0648, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0638, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0633, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0623, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0618, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0614, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0609, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0605, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0600, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0596, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0591, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0587, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0583, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0579, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0575, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0571, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0567, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0563, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0559, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0556, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0552, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0548, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0545, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0541, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0537, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0531, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0527, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0524, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0508, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0505, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0502, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0499, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0496, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0493, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0490, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0487, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0479, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0476, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0465, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0460, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0458, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0455, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0453, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0451, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0448, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0446, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0444, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0441, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0434, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0432, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0430, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0426, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0420, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0417, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0413, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0406, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0402, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0400, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# optimizer 是训练的工具\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.02)  # 传入 net 的所有参数, 学习率\n",
    "# 算误差的时候, 注意真实值!不是! one-hot 形式的, 而是1D Tensor, (batch,)\n",
    "# 但是预测值是2D tensor (batch, n_classes)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    " \n",
    "for t in range(200):\n",
    "    out = net(x)     # 喂给 net 训练数据 x, 输出分析值\n",
    " \n",
    "    loss = loss_func(out, y)     # 计算两者的误差\n",
    "    print(loss)\n",
    " \n",
    "    optimizer.zero_grad()   # 清空上一步的残余更新参数值\n",
    "    loss.backward()         # 误差反向传播, 计算参数更新值\n",
    "    optimizer.step()        # 将参数更新值施加到 net 的 parameters 上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
